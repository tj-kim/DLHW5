{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Written Exercise 4.\n",
    "\n",
    "What is the relationship between overfitting of the attacked model f (x) and the resulting attack success?\n",
    "Evaluate and report results of both the shadow models attack and the naive baseline attack, one which always\n",
    "guesses that correctly predicted instances were in training, and incorrectly guessed instances were not. As\n",
    "part of this exercise you should analyze the attacks on CIFAR models that do not overfit as much as the one\n",
    "shown in the starter code.\n",
    "\n",
    "\n",
    "Your analysis of the results should include a measure of overfitting, attacker success rate, and a graphic\n",
    "relating the two. You can find better models that do not overfit as much in Homework 2. You can also\n",
    "analyze models that are reasonable unless they are trained for too many epochs at which point they start to\n",
    "overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hw5_part1_utils\n",
    "\n",
    "from typing import Tuple\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "from copy import copy\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from hw5_part1 import synthesize_attack_data, build_attack_models, evaluate_membership\n",
    "\n",
    "data = hw5_part1_utils.CIFARData()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Build Naive Attack \n",
    "\n",
    "The naive attack takes y_pred and y and claims as part of the dataset (z=1) if (y_pred = y) and z=0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_membership(y_pred, y):\n",
    "    \"\"\"Evaluate the attacker about the membership inference with naive method\n",
    "    If y_pred = y, z = 1, z = 0 otherwise\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "        y_pred {np.ndarray} -- an N x C numpy array with the predictions of the\n",
    "          model on the N instances we are performing the inference attack on.\n",
    "\n",
    "        y {np.ndarray} -- the true labels for each of the instances given as a\n",
    "          numpy array of N integers.\n",
    "\n",
    "    Returns:\n",
    "\n",
    "        {np.ndarray} -- an array of N integer in the range [0,1] representing\n",
    "          the estimated probability that each of the N given instances is a\n",
    "          training set member.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # To return\n",
    "    preds: np.ndarray = None\n",
    "\n",
    "    # SOLUTION\n",
    "    \n",
    "    # 1. Get maximum axes for each of the y_pred\n",
    "    y_pred_int = np.reshape(np.argmax(y_pred,axis=1),y.shape)\n",
    "    \n",
    "    # 2. Find where prediction and labels are equal\n",
    "    preds = np.equal(y_pred_int, y) * 1\n",
    "    \n",
    "    # END OF SOLUTION\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Define mechanism for measuring \"overfitting\" of CIFAR model to be attacked\n",
    "\n",
    "The metric I will choose is (train_acc - test_acc)^2. I will build a function that measures that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_overfit(target_model, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Measure overfitting by taking square difference of difference in accuracy in \n",
    "    test data and train data.\n",
    "    \"\"\"\n",
    "    \n",
    "    train_acc = target_model.model.evaluate(X_train, to_categorical(y_train),verbose=0)[1]\n",
    "    test_acc = target_model.model.evaluate(X_test, to_categorical(y_test),verbose=0)[1]\n",
    "    overfit_measure = (train_acc - test_acc)**2\n",
    "    \n",
    "    return overfit_measure\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Find Many Different CIFAR Models and compare overfit measures\n",
    "\n",
    "1. Original Model trained under 10, 20, 30, 40, 50 epochs\n",
    "2. Obtain overfit metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/tkim/miniconda3/envs/fsdl/lib/python3.7/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/tkim/miniconda3/envs/fsdl/lib/python3.7/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/tkim/miniconda3/envs/fsdl/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "epoch_vals = [10, 20, 30, 40, 50]\n",
    "models = []\n",
    "\n",
    "for ep in epoch_vals:\n",
    "    models += [hw5_part1_utils.CIFARModel(epochs = ep, batch_size = 2048)]\n",
    "    models[-1].init(data.train, data.labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "overfits = []\n",
    "for i in range(len(epoch_vals)):\n",
    "    overfits += [measure_overfit(models[i], data.train[0:1000], data.labels_train[0:1000],\n",
    "                              data.test, data.labels_test)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs: 10 overfit: 0.01444803479572343\n",
      "epochs: 20 overfit: 0.07524049494410523\n",
      "epochs: 30 overfit: 0.14356518098382143\n",
      "epochs: 40 overfit: 0.25532807525411627\n",
      "epochs: 50 overfit: 0.2864390397958374\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(epoch_vals)):\n",
    "    print(\"epochs:\", epoch_vals[i], \"overfit:\", overfits[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Measure performance of Naive and Shadow Attack Model\n",
    "\n",
    "Accuracy of Z overall:\n",
    "1000 points in train data\n",
    "1000 points in test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/tkim/Fair_DL/hw5/hw5_part1_utils.py:134: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
      "\n",
      "attack model no. 0\n",
      "WARNING:tensorflow:From /home/tkim/miniconda3/envs/fsdl/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 7912 samples\n",
      "Epoch 1/30\n",
      "7912/7912 [==============================] - 0s 50us/sample - loss: 0.6940 - acc: 0.5095\n",
      "Epoch 2/30\n",
      "7912/7912 [==============================] - 0s 6us/sample - loss: 0.6925 - acc: 0.5148\n",
      "Epoch 3/30\n",
      "7912/7912 [==============================] - 0s 6us/sample - loss: 0.6902 - acc: 0.5316\n",
      "Epoch 4/30\n",
      "7912/7912 [==============================] - 0s 5us/sample - loss: 0.6898 - acc: 0.5334\n",
      "Epoch 5/30\n",
      "7912/7912 [==============================] - 0s 5us/sample - loss: 0.6894 - acc: 0.5340\n",
      "Epoch 6/30\n",
      "7912/7912 [==============================] - 0s 6us/sample - loss: 0.6894 - acc: 0.5355\n",
      "Epoch 7/30\n",
      "7912/7912 [==============================] - 0s 7us/sample - loss: 0.6887 - acc: 0.5423\n",
      "Epoch 8/30\n",
      "7912/7912 [==============================] - 0s 6us/sample - loss: 0.6893 - acc: 0.5345\n",
      "Epoch 9/30\n",
      "7912/7912 [==============================] - 0s 6us/sample - loss: 0.6891 - acc: 0.5387\n",
      "Epoch 10/30\n",
      "7912/7912 [==============================] - 0s 5us/sample - loss: 0.6902 - acc: 0.5311\n",
      "Epoch 11/30\n",
      "7912/7912 [==============================] - 0s 6us/sample - loss: 0.6883 - acc: 0.5365\n",
      "Epoch 12/30\n",
      "7912/7912 [==============================] - 0s 6us/sample - loss: 0.6880 - acc: 0.5383\n",
      "Epoch 13/30\n",
      "7912/7912 [==============================] - 0s 5us/sample - loss: 0.6893 - acc: 0.5334\n",
      "Epoch 14/30\n",
      "7912/7912 [==============================] - 0s 6us/sample - loss: 0.6877 - acc: 0.5476\n",
      "Epoch 15/30\n",
      "7912/7912 [==============================] - 0s 5us/sample - loss: 0.6878 - acc: 0.5439\n",
      "Epoch 16/30\n",
      "7912/7912 [==============================] - 0s 5us/sample - loss: 0.6871 - acc: 0.5406\n",
      "Epoch 17/30\n",
      "7912/7912 [==============================] - 0s 6us/sample - loss: 0.6875 - acc: 0.5404\n",
      "Epoch 18/30\n",
      "7912/7912 [==============================] - 0s 5us/sample - loss: 0.6875 - acc: 0.5468\n",
      "Epoch 19/30\n",
      "7912/7912 [==============================] - 0s 5us/sample - loss: 0.6854 - acc: 0.5494\n",
      "Epoch 20/30\n",
      "7912/7912 [==============================] - 0s 6us/sample - loss: 0.6865 - acc: 0.5416\n",
      "Epoch 21/30\n",
      "7912/7912 [==============================] - 0s 5us/sample - loss: 0.6880 - acc: 0.5380\n",
      "Epoch 22/30\n",
      "7912/7912 [==============================] - 0s 6us/sample - loss: 0.6858 - acc: 0.5474\n",
      "Epoch 23/30\n",
      "7912/7912 [==============================] - 0s 5us/sample - loss: 0.6877 - acc: 0.5398\n",
      "Epoch 24/30\n",
      "7912/7912 [==============================] - 0s 5us/sample - loss: 0.6878 - acc: 0.5354\n",
      "Epoch 25/30\n",
      "7912/7912 [==============================] - 0s 6us/sample - loss: 0.6860 - acc: 0.5482\n",
      "Epoch 26/30\n",
      "7912/7912 [==============================] - 0s 6us/sample - loss: 0.6875 - acc: 0.5398\n",
      "Epoch 27/30\n",
      "7912/7912 [==============================] - 0s 6us/sample - loss: 0.6865 - acc: 0.5480\n",
      "Epoch 28/30\n",
      "7912/7912 [==============================] - 0s 5us/sample - loss: 0.6862 - acc: 0.5495\n",
      "Epoch 29/30\n",
      "7912/7912 [==============================] - 0s 6us/sample - loss: 0.6880 - acc: 0.5403\n",
      "Epoch 30/30\n",
      "7912/7912 [==============================] - 0s 5us/sample - loss: 0.6869 - acc: 0.5412\n",
      "attack model no. 1\n",
      "Train on 8196 samples\n",
      "Epoch 1/30\n",
      "8196/8196 [==============================] - 0s 49us/sample - loss: 0.6942 - acc: 0.4976\n",
      "Epoch 2/30\n",
      "8196/8196 [==============================] - 0s 6us/sample - loss: 0.6915 - acc: 0.5265\n",
      "Epoch 3/30\n",
      "8196/8196 [==============================] - 0s 5us/sample - loss: 0.6877 - acc: 0.5387\n",
      "Epoch 4/30\n",
      "8196/8196 [==============================] - 0s 5us/sample - loss: 0.6870 - acc: 0.5437\n",
      "Epoch 5/30\n",
      "8196/8196 [==============================] - 0s 6us/sample - loss: 0.6866 - acc: 0.5490\n",
      "Epoch 6/30\n",
      "8196/8196 [==============================] - 0s 6us/sample - loss: 0.6848 - acc: 0.5549\n",
      "Epoch 7/30\n",
      "8196/8196 [==============================] - 0s 6us/sample - loss: 0.6846 - acc: 0.5528\n",
      "Epoch 8/30\n",
      "8196/8196 [==============================] - 0s 6us/sample - loss: 0.6856 - acc: 0.5539\n",
      "Epoch 9/30\n",
      "8196/8196 [==============================] - 0s 6us/sample - loss: 0.6845 - acc: 0.5584\n",
      "Epoch 10/30\n",
      "8196/8196 [==============================] - 0s 6us/sample - loss: 0.6838 - acc: 0.5595\n",
      "Epoch 11/30\n",
      "8196/8196 [==============================] - 0s 6us/sample - loss: 0.6843 - acc: 0.5493\n",
      "Epoch 12/30\n",
      "8196/8196 [==============================] - 0s 6us/sample - loss: 0.6839 - acc: 0.5517\n",
      "Epoch 13/30\n",
      "8196/8196 [==============================] - 0s 6us/sample - loss: 0.6845 - acc: 0.5550\n",
      "Epoch 14/30\n",
      "8196/8196 [==============================] - 0s 5us/sample - loss: 0.6840 - acc: 0.5525\n",
      "Epoch 15/30\n",
      "8196/8196 [==============================] - 0s 6us/sample - loss: 0.6815 - acc: 0.5567\n",
      "Epoch 16/30\n",
      "8196/8196 [==============================] - 0s 6us/sample - loss: 0.6826 - acc: 0.5567\n",
      "Epoch 17/30\n",
      "8196/8196 [==============================] - 0s 9us/sample - loss: 0.6825 - acc: 0.5571\n",
      "Epoch 18/30\n",
      "8196/8196 [==============================] - 0s 7us/sample - loss: 0.6831 - acc: 0.5522\n",
      "Epoch 19/30\n",
      "8196/8196 [==============================] - 0s 6us/sample - loss: 0.6816 - acc: 0.5551\n",
      "Epoch 20/30\n",
      "8196/8196 [==============================] - 0s 5us/sample - loss: 0.6829 - acc: 0.5514\n",
      "Epoch 21/30\n",
      "8196/8196 [==============================] - 0s 6us/sample - loss: 0.6816 - acc: 0.5612\n",
      "Epoch 22/30\n",
      "8196/8196 [==============================] - 0s 7us/sample - loss: 0.6816 - acc: 0.5621\n",
      "Epoch 23/30\n",
      "8196/8196 [==============================] - 0s 5us/sample - loss: 0.6818 - acc: 0.5642\n",
      "Epoch 24/30\n",
      "8196/8196 [==============================] - 0s 6us/sample - loss: 0.6797 - acc: 0.5650\n",
      "Epoch 25/30\n",
      "8196/8196 [==============================] - 0s 6us/sample - loss: 0.6819 - acc: 0.5573\n",
      "Epoch 26/30\n",
      "8196/8196 [==============================] - 0s 5us/sample - loss: 0.6825 - acc: 0.5512\n",
      "Epoch 27/30\n",
      "8196/8196 [==============================] - 0s 6us/sample - loss: 0.6817 - acc: 0.5562\n",
      "Epoch 28/30\n",
      "8196/8196 [==============================] - 0s 5us/sample - loss: 0.6819 - acc: 0.5536\n",
      "Epoch 29/30\n",
      "8196/8196 [==============================] - 0s 6us/sample - loss: 0.6807 - acc: 0.5637\n",
      "Epoch 30/30\n",
      "8196/8196 [==============================] - 0s 6us/sample - loss: 0.6813 - acc: 0.5523\n",
      "attack model no. 2\n",
      "Train on 7900 samples\n",
      "Epoch 1/30\n",
      "7900/7900 [==============================] - 0s 53us/sample - loss: 0.6951 - acc: 0.5057\n",
      "Epoch 2/30\n",
      "7900/7900 [==============================] - 0s 5us/sample - loss: 0.6912 - acc: 0.5246\n",
      "Epoch 3/30\n",
      "7900/7900 [==============================] - 0s 5us/sample - loss: 0.6894 - acc: 0.5353\n",
      "Epoch 4/30\n",
      "7900/7900 [==============================] - 0s 6us/sample - loss: 0.6886 - acc: 0.5372\n",
      "Epoch 5/30\n",
      "7900/7900 [==============================] - 0s 6us/sample - loss: 0.6885 - acc: 0.5358\n",
      "Epoch 6/30\n",
      "7900/7900 [==============================] - 0s 7us/sample - loss: 0.6881 - acc: 0.5386\n",
      "Epoch 7/30\n",
      "7900/7900 [==============================] - 0s 6us/sample - loss: 0.6872 - acc: 0.5489\n",
      "Epoch 8/30\n",
      "7900/7900 [==============================] - 0s 6us/sample - loss: 0.6869 - acc: 0.5528\n",
      "Epoch 9/30\n",
      "7900/7900 [==============================] - 0s 6us/sample - loss: 0.6848 - acc: 0.5556\n",
      "Epoch 10/30\n",
      "7900/7900 [==============================] - 0s 6us/sample - loss: 0.6850 - acc: 0.5548\n",
      "Epoch 11/30\n",
      "7900/7900 [==============================] - 0s 6us/sample - loss: 0.6853 - acc: 0.5528\n",
      "Epoch 12/30\n",
      "7900/7900 [==============================] - 0s 6us/sample - loss: 0.6827 - acc: 0.5641\n",
      "Epoch 13/30\n",
      "7900/7900 [==============================] - 0s 6us/sample - loss: 0.6851 - acc: 0.5462\n",
      "Epoch 14/30\n",
      "7900/7900 [==============================] - 0s 7us/sample - loss: 0.6821 - acc: 0.5635\n",
      "Epoch 15/30\n",
      "7900/7900 [==============================] - 0s 5us/sample - loss: 0.6859 - acc: 0.5504\n",
      "Epoch 16/30\n",
      "7900/7900 [==============================] - 0s 6us/sample - loss: 0.6836 - acc: 0.5563\n",
      "Epoch 17/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7900/7900 [==============================] - 0s 6us/sample - loss: 0.6849 - acc: 0.5520\n",
      "Epoch 18/30\n",
      "7900/7900 [==============================] - 0s 6us/sample - loss: 0.6849 - acc: 0.5558\n",
      "Epoch 19/30\n",
      "7900/7900 [==============================] - 0s 5us/sample - loss: 0.6856 - acc: 0.5532\n",
      "Epoch 20/30\n",
      "7900/7900 [==============================] - 0s 4us/sample - loss: 0.6841 - acc: 0.5597\n",
      "Epoch 21/30\n",
      "7900/7900 [==============================] - 0s 5us/sample - loss: 0.6836 - acc: 0.5565\n",
      "Epoch 22/30\n",
      "7900/7900 [==============================] - 0s 5us/sample - loss: 0.6829 - acc: 0.5568\n",
      "Epoch 23/30\n",
      "7900/7900 [==============================] - 0s 5us/sample - loss: 0.6828 - acc: 0.5615\n",
      "Epoch 24/30\n",
      "7900/7900 [==============================] - 0s 5us/sample - loss: 0.6823 - acc: 0.5648\n",
      "Epoch 25/30\n",
      "7900/7900 [==============================] - 0s 5us/sample - loss: 0.6834 - acc: 0.5589\n",
      "Epoch 26/30\n",
      "7900/7900 [==============================] - 0s 5us/sample - loss: 0.6842 - acc: 0.5576\n",
      "Epoch 27/30\n",
      "7900/7900 [==============================] - 0s 6us/sample - loss: 0.6813 - acc: 0.5662\n",
      "Epoch 28/30\n",
      "7900/7900 [==============================] - 0s 5us/sample - loss: 0.6813 - acc: 0.5600\n",
      "Epoch 29/30\n",
      "7900/7900 [==============================] - 0s 5us/sample - loss: 0.6826 - acc: 0.5618\n",
      "Epoch 30/30\n",
      "7900/7900 [==============================] - 0s 5us/sample - loss: 0.6840 - acc: 0.5556\n",
      "attack model no. 3\n",
      "Train on 7968 samples\n",
      "Epoch 1/30\n",
      "7968/7968 [==============================] - 0s 52us/sample - loss: 0.6957 - acc: 0.4972\n",
      "Epoch 2/30\n",
      "7968/7968 [==============================] - 0s 5us/sample - loss: 0.6907 - acc: 0.5300\n",
      "Epoch 3/30\n",
      "7968/7968 [==============================] - 0s 6us/sample - loss: 0.6886 - acc: 0.5348\n",
      "Epoch 4/30\n",
      "7968/7968 [==============================] - 0s 4us/sample - loss: 0.6863 - acc: 0.5533\n",
      "Epoch 5/30\n",
      "7968/7968 [==============================] - 0s 5us/sample - loss: 0.6837 - acc: 0.5619\n",
      "Epoch 6/30\n",
      "7968/7968 [==============================] - 0s 5us/sample - loss: 0.6834 - acc: 0.5540\n",
      "Epoch 7/30\n",
      "7968/7968 [==============================] - 0s 5us/sample - loss: 0.6838 - acc: 0.5560\n",
      "Epoch 8/30\n",
      "7968/7968 [==============================] - 0s 6us/sample - loss: 0.6798 - acc: 0.5678\n",
      "Epoch 9/30\n",
      "7968/7968 [==============================] - 0s 5us/sample - loss: 0.6805 - acc: 0.5668\n",
      "Epoch 10/30\n",
      "7968/7968 [==============================] - 0s 6us/sample - loss: 0.6817 - acc: 0.5582\n",
      "Epoch 11/30\n",
      "7968/7968 [==============================] - 0s 5us/sample - loss: 0.6804 - acc: 0.5602\n",
      "Epoch 12/30\n",
      "7968/7968 [==============================] - 0s 5us/sample - loss: 0.6811 - acc: 0.5600\n",
      "Epoch 13/30\n",
      "7968/7968 [==============================] - 0s 5us/sample - loss: 0.6798 - acc: 0.5630\n",
      "Epoch 14/30\n",
      "7968/7968 [==============================] - 0s 4us/sample - loss: 0.6794 - acc: 0.5742\n",
      "Epoch 15/30\n",
      "7968/7968 [==============================] - 0s 5us/sample - loss: 0.6783 - acc: 0.5694\n",
      "Epoch 16/30\n",
      "7968/7968 [==============================] - 0s 5us/sample - loss: 0.6814 - acc: 0.5660\n",
      "Epoch 17/30\n",
      "7968/7968 [==============================] - 0s 6us/sample - loss: 0.6767 - acc: 0.5799\n",
      "Epoch 18/30\n",
      "7968/7968 [==============================] - 0s 5us/sample - loss: 0.6773 - acc: 0.5722\n",
      "Epoch 19/30\n",
      "7968/7968 [==============================] - 0s 6us/sample - loss: 0.6783 - acc: 0.5709\n",
      "Epoch 20/30\n",
      "7968/7968 [==============================] - 0s 6us/sample - loss: 0.6755 - acc: 0.5822\n",
      "Epoch 21/30\n",
      "7968/7968 [==============================] - 0s 5us/sample - loss: 0.6788 - acc: 0.5638\n",
      "Epoch 22/30\n",
      "7968/7968 [==============================] - 0s 6us/sample - loss: 0.6783 - acc: 0.5684\n",
      "Epoch 23/30\n",
      "7968/7968 [==============================] - 0s 6us/sample - loss: 0.6786 - acc: 0.5715\n",
      "Epoch 24/30\n",
      "7968/7968 [==============================] - 0s 6us/sample - loss: 0.6762 - acc: 0.5757\n",
      "Epoch 25/30\n",
      "7968/7968 [==============================] - 0s 8us/sample - loss: 0.6773 - acc: 0.5748\n",
      "Epoch 26/30\n",
      "7968/7968 [==============================] - 0s 5us/sample - loss: 0.6771 - acc: 0.5723\n",
      "Epoch 27/30\n",
      "7968/7968 [==============================] - 0s 5us/sample - loss: 0.6779 - acc: 0.5688\n",
      "Epoch 28/30\n",
      "7968/7968 [==============================] - 0s 5us/sample - loss: 0.6752 - acc: 0.5781\n",
      "Epoch 29/30\n",
      "7968/7968 [==============================] - 0s 6us/sample - loss: 0.6766 - acc: 0.5674\n",
      "Epoch 30/30\n",
      "7968/7968 [==============================] - 0s 6us/sample - loss: 0.6766 - acc: 0.5757\n",
      "attack model no. 4\n",
      "Train on 8000 samples\n",
      "Epoch 1/30\n",
      "8000/8000 [==============================] - 0s 53us/sample - loss: 0.6928 - acc: 0.5113\n",
      "Epoch 2/30\n",
      "8000/8000 [==============================] - 0s 5us/sample - loss: 0.6917 - acc: 0.5182\n",
      "Epoch 3/30\n",
      "8000/8000 [==============================] - 0s 5us/sample - loss: 0.6899 - acc: 0.5351\n",
      "Epoch 4/30\n",
      "8000/8000 [==============================] - 0s 6us/sample - loss: 0.6880 - acc: 0.5409\n",
      "Epoch 5/30\n",
      "8000/8000 [==============================] - 0s 6us/sample - loss: 0.6860 - acc: 0.5543\n",
      "Epoch 6/30\n",
      "8000/8000 [==============================] - 0s 6us/sample - loss: 0.6854 - acc: 0.5475\n",
      "Epoch 7/30\n",
      "8000/8000 [==============================] - 0s 5us/sample - loss: 0.6854 - acc: 0.5536\n",
      "Epoch 8/30\n",
      "8000/8000 [==============================] - 0s 5us/sample - loss: 0.6843 - acc: 0.5501\n",
      "Epoch 9/30\n",
      "8000/8000 [==============================] - 0s 5us/sample - loss: 0.6839 - acc: 0.5558\n",
      "Epoch 10/30\n",
      "8000/8000 [==============================] - 0s 6us/sample - loss: 0.6836 - acc: 0.5559\n",
      "Epoch 11/30\n",
      "8000/8000 [==============================] - 0s 5us/sample - loss: 0.6828 - acc: 0.5570\n",
      "Epoch 12/30\n",
      "8000/8000 [==============================] - 0s 5us/sample - loss: 0.6836 - acc: 0.5586\n",
      "Epoch 13/30\n",
      "8000/8000 [==============================] - 0s 6us/sample - loss: 0.6824 - acc: 0.5675\n",
      "Epoch 14/30\n",
      "8000/8000 [==============================] - 0s 5us/sample - loss: 0.6825 - acc: 0.5584\n",
      "Epoch 15/30\n",
      "8000/8000 [==============================] - 0s 5us/sample - loss: 0.6824 - acc: 0.5631\n",
      "Epoch 16/30\n",
      "8000/8000 [==============================] - 0s 5us/sample - loss: 0.6817 - acc: 0.5664\n",
      "Epoch 17/30\n",
      "8000/8000 [==============================] - 0s 5us/sample - loss: 0.6815 - acc: 0.5601\n",
      "Epoch 18/30\n",
      "8000/8000 [==============================] - 0s 6us/sample - loss: 0.6802 - acc: 0.5658\n",
      "Epoch 19/30\n",
      "8000/8000 [==============================] - 0s 5us/sample - loss: 0.6829 - acc: 0.5577\n",
      "Epoch 20/30\n",
      "8000/8000 [==============================] - 0s 6us/sample - loss: 0.6806 - acc: 0.5716\n",
      "Epoch 21/30\n",
      "8000/8000 [==============================] - 0s 5us/sample - loss: 0.6794 - acc: 0.5736\n",
      "Epoch 22/30\n",
      "8000/8000 [==============================] - 0s 5us/sample - loss: 0.6813 - acc: 0.5652\n",
      "Epoch 23/30\n",
      "8000/8000 [==============================] - 0s 5us/sample - loss: 0.6823 - acc: 0.5575\n",
      "Epoch 24/30\n",
      "8000/8000 [==============================] - 0s 5us/sample - loss: 0.6841 - acc: 0.5567\n",
      "Epoch 25/30\n",
      "8000/8000 [==============================] - 0s 6us/sample - loss: 0.6829 - acc: 0.5556\n",
      "Epoch 26/30\n",
      "8000/8000 [==============================] - 0s 5us/sample - loss: 0.6807 - acc: 0.5656\n",
      "Epoch 27/30\n",
      "8000/8000 [==============================] - 0s 6us/sample - loss: 0.6818 - acc: 0.5601\n",
      "Epoch 28/30\n",
      "8000/8000 [==============================] - 0s 6us/sample - loss: 0.6777 - acc: 0.5664\n",
      "Epoch 29/30\n",
      "8000/8000 [==============================] - 0s 5us/sample - loss: 0.6817 - acc: 0.5555\n",
      "Epoch 30/30\n",
      "8000/8000 [==============================] - 0s 5us/sample - loss: 0.6794 - acc: 0.5641\n",
      "attack model no. 5\n",
      "Train on 8068 samples\n",
      "Epoch 1/30\n",
      "8068/8068 [==============================] - 0s 54us/sample - loss: 0.6922 - acc: 0.5178\n",
      "Epoch 2/30\n",
      "8068/8068 [==============================] - 0s 6us/sample - loss: 0.6912 - acc: 0.5273\n",
      "Epoch 3/30\n",
      "8068/8068 [==============================] - 0s 5us/sample - loss: 0.6896 - acc: 0.5410\n",
      "Epoch 4/30\n",
      "8068/8068 [==============================] - 0s 5us/sample - loss: 0.6876 - acc: 0.5506\n",
      "Epoch 5/30\n",
      "8068/8068 [==============================] - 0s 7us/sample - loss: 0.6880 - acc: 0.5442\n",
      "Epoch 6/30\n",
      "8068/8068 [==============================] - 0s 6us/sample - loss: 0.6879 - acc: 0.5491\n",
      "Epoch 7/30\n",
      "8068/8068 [==============================] - 0s 6us/sample - loss: 0.6870 - acc: 0.5436\n",
      "Epoch 8/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8068/8068 [==============================] - 0s 5us/sample - loss: 0.6870 - acc: 0.5464\n",
      "Epoch 9/30\n",
      "8068/8068 [==============================] - 0s 5us/sample - loss: 0.6870 - acc: 0.5514\n",
      "Epoch 10/30\n",
      "8068/8068 [==============================] - 0s 6us/sample - loss: 0.6852 - acc: 0.5519\n",
      "Epoch 11/30\n",
      "8068/8068 [==============================] - 0s 6us/sample - loss: 0.6849 - acc: 0.5522\n",
      "Epoch 12/30\n",
      "8068/8068 [==============================] - 0s 6us/sample - loss: 0.6863 - acc: 0.5431\n",
      "Epoch 13/30\n",
      "8068/8068 [==============================] - 0s 6us/sample - loss: 0.6858 - acc: 0.5504\n",
      "Epoch 14/30\n",
      "8068/8068 [==============================] - 0s 6us/sample - loss: 0.6838 - acc: 0.5509\n",
      "Epoch 15/30\n",
      "8068/8068 [==============================] - 0s 6us/sample - loss: 0.6866 - acc: 0.5423\n",
      "Epoch 16/30\n",
      "8068/8068 [==============================] - 0s 6us/sample - loss: 0.6865 - acc: 0.5480\n",
      "Epoch 17/30\n",
      "8068/8068 [==============================] - 0s 6us/sample - loss: 0.6851 - acc: 0.5444\n",
      "Epoch 18/30\n",
      "8068/8068 [==============================] - 0s 6us/sample - loss: 0.6856 - acc: 0.5433\n",
      "Epoch 19/30\n",
      "8068/8068 [==============================] - 0s 8us/sample - loss: 0.6849 - acc: 0.5521\n",
      "Epoch 20/30\n",
      "8068/8068 [==============================] - 0s 6us/sample - loss: 0.6855 - acc: 0.5473\n",
      "Epoch 21/30\n",
      "8068/8068 [==============================] - 0s 6us/sample - loss: 0.6844 - acc: 0.5496\n",
      "Epoch 22/30\n",
      "8068/8068 [==============================] - 0s 6us/sample - loss: 0.6846 - acc: 0.5516\n",
      "Epoch 23/30\n",
      "8068/8068 [==============================] - 0s 5us/sample - loss: 0.6845 - acc: 0.5579\n",
      "Epoch 24/30\n",
      "8068/8068 [==============================] - 0s 6us/sample - loss: 0.6832 - acc: 0.5566\n",
      "Epoch 25/30\n",
      "8068/8068 [==============================] - 0s 8us/sample - loss: 0.6854 - acc: 0.5456\n",
      "Epoch 26/30\n",
      "8068/8068 [==============================] - 0s 6us/sample - loss: 0.6845 - acc: 0.5444\n",
      "Epoch 27/30\n",
      "8068/8068 [==============================] - 0s 6us/sample - loss: 0.6839 - acc: 0.5511\n",
      "Epoch 28/30\n",
      "8068/8068 [==============================] - 0s 6us/sample - loss: 0.6839 - acc: 0.5538\n",
      "Epoch 29/30\n",
      "8068/8068 [==============================] - 0s 6us/sample - loss: 0.6837 - acc: 0.5527\n",
      "Epoch 30/30\n",
      "8068/8068 [==============================] - 0s 6us/sample - loss: 0.6852 - acc: 0.5445\n",
      "attack model no. 6\n",
      "Train on 7944 samples\n",
      "Epoch 1/30\n",
      "7944/7944 [==============================] - 0s 55us/sample - loss: 0.6957 - acc: 0.5000\n",
      "Epoch 2/30\n",
      "7944/7944 [==============================] - 0s 5us/sample - loss: 0.6941 - acc: 0.5049\n",
      "Epoch 3/30\n",
      "7944/7944 [==============================] - 0s 6us/sample - loss: 0.6906 - acc: 0.5292\n",
      "Epoch 4/30\n",
      "7944/7944 [==============================] - 0s 5us/sample - loss: 0.6888 - acc: 0.5326\n",
      "Epoch 5/30\n",
      "7944/7944 [==============================] - 0s 5us/sample - loss: 0.6876 - acc: 0.5394\n",
      "Epoch 6/30\n",
      "7944/7944 [==============================] - 0s 7us/sample - loss: 0.6876 - acc: 0.5400\n",
      "Epoch 7/30\n",
      "7944/7944 [==============================] - 0s 5us/sample - loss: 0.6871 - acc: 0.5444\n",
      "Epoch 8/30\n",
      "7944/7944 [==============================] - 0s 5us/sample - loss: 0.6880 - acc: 0.5378\n",
      "Epoch 9/30\n",
      "7944/7944 [==============================] - 0s 5us/sample - loss: 0.6866 - acc: 0.5389\n",
      "Epoch 10/30\n",
      "7944/7944 [==============================] - 0s 6us/sample - loss: 0.6869 - acc: 0.5422\n",
      "Epoch 11/30\n",
      "7944/7944 [==============================] - 0s 5us/sample - loss: 0.6864 - acc: 0.5438\n",
      "Epoch 12/30\n",
      "7944/7944 [==============================] - 0s 5us/sample - loss: 0.6883 - acc: 0.5358\n",
      "Epoch 13/30\n",
      "7944/7944 [==============================] - 0s 5us/sample - loss: 0.6876 - acc: 0.5369\n",
      "Epoch 14/30\n",
      "7944/7944 [==============================] - 0s 5us/sample - loss: 0.6859 - acc: 0.5443\n",
      "Epoch 15/30\n",
      "7944/7944 [==============================] - 0s 6us/sample - loss: 0.6875 - acc: 0.5449\n",
      "Epoch 16/30\n",
      "7944/7944 [==============================] - 0s 5us/sample - loss: 0.6855 - acc: 0.5466\n",
      "Epoch 17/30\n",
      "7944/7944 [==============================] - 0s 6us/sample - loss: 0.6856 - acc: 0.5451\n",
      "Epoch 18/30\n",
      "7944/7944 [==============================] - 0s 5us/sample - loss: 0.6856 - acc: 0.5410\n",
      "Epoch 19/30\n",
      "7944/7944 [==============================] - 0s 5us/sample - loss: 0.6848 - acc: 0.5491\n",
      "Epoch 20/30\n",
      "7944/7944 [==============================] - 0s 6us/sample - loss: 0.6850 - acc: 0.5471\n",
      "Epoch 21/30\n",
      "7944/7944 [==============================] - 0s 5us/sample - loss: 0.6846 - acc: 0.5476\n",
      "Epoch 22/30\n",
      "7944/7944 [==============================] - 0s 5us/sample - loss: 0.6848 - acc: 0.5470\n",
      "Epoch 23/30\n",
      "7944/7944 [==============================] - 0s 5us/sample - loss: 0.6864 - acc: 0.5466\n",
      "Epoch 24/30\n",
      "7944/7944 [==============================] - 0s 6us/sample - loss: 0.6844 - acc: 0.5473\n",
      "Epoch 25/30\n",
      "7944/7944 [==============================] - 0s 5us/sample - loss: 0.6870 - acc: 0.5378\n",
      "Epoch 26/30\n",
      "7944/7944 [==============================] - 0s 6us/sample - loss: 0.6848 - acc: 0.5497\n",
      "Epoch 27/30\n",
      "7944/7944 [==============================] - 0s 5us/sample - loss: 0.6845 - acc: 0.5500\n",
      "Epoch 28/30\n",
      "7944/7944 [==============================] - 0s 5us/sample - loss: 0.6863 - acc: 0.5419\n",
      "Epoch 29/30\n",
      "7944/7944 [==============================] - 0s 6us/sample - loss: 0.6852 - acc: 0.5485\n",
      "Epoch 30/30\n",
      "7944/7944 [==============================] - 0s 5us/sample - loss: 0.6844 - acc: 0.5461\n",
      "attack model no. 7\n",
      "Train on 8164 samples\n",
      "Epoch 1/30\n",
      "8164/8164 [==============================] - 0s 55us/sample - loss: 0.6924 - acc: 0.5146\n",
      "Epoch 2/30\n",
      "8164/8164 [==============================] - 0s 5us/sample - loss: 0.6895 - acc: 0.5383\n",
      "Epoch 3/30\n",
      "8164/8164 [==============================] - 0s 5us/sample - loss: 0.6890 - acc: 0.5356\n",
      "Epoch 4/30\n",
      "8164/8164 [==============================] - 0s 6us/sample - loss: 0.6864 - acc: 0.5521\n",
      "Epoch 5/30\n",
      "8164/8164 [==============================] - 0s 6us/sample - loss: 0.6873 - acc: 0.5489\n",
      "Epoch 6/30\n",
      "8164/8164 [==============================] - 0s 6us/sample - loss: 0.6856 - acc: 0.5625\n",
      "Epoch 7/30\n",
      "8164/8164 [==============================] - 0s 6us/sample - loss: 0.6864 - acc: 0.5534\n",
      "Epoch 8/30\n",
      "8164/8164 [==============================] - 0s 6us/sample - loss: 0.6851 - acc: 0.5556\n",
      "Epoch 9/30\n",
      "8164/8164 [==============================] - 0s 6us/sample - loss: 0.6855 - acc: 0.5480\n",
      "Epoch 10/30\n",
      "8164/8164 [==============================] - 0s 6us/sample - loss: 0.6869 - acc: 0.5472\n",
      "Epoch 11/30\n",
      "8164/8164 [==============================] - 0s 6us/sample - loss: 0.6841 - acc: 0.5574\n",
      "Epoch 12/30\n",
      "8164/8164 [==============================] - 0s 6us/sample - loss: 0.6850 - acc: 0.5534\n",
      "Epoch 13/30\n",
      "8164/8164 [==============================] - 0s 6us/sample - loss: 0.6827 - acc: 0.5622\n",
      "Epoch 14/30\n",
      "8164/8164 [==============================] - 0s 6us/sample - loss: 0.6835 - acc: 0.5527\n",
      "Epoch 15/30\n",
      "8164/8164 [==============================] - 0s 6us/sample - loss: 0.6848 - acc: 0.5522\n",
      "Epoch 16/30\n",
      "8164/8164 [==============================] - 0s 6us/sample - loss: 0.6856 - acc: 0.5513\n",
      "Epoch 17/30\n",
      "8164/8164 [==============================] - 0s 6us/sample - loss: 0.6842 - acc: 0.5545\n",
      "Epoch 18/30\n",
      "8164/8164 [==============================] - 0s 6us/sample - loss: 0.6834 - acc: 0.5590\n",
      "Epoch 19/30\n",
      "8164/8164 [==============================] - 0s 7us/sample - loss: 0.6830 - acc: 0.5612\n",
      "Epoch 20/30\n",
      "8164/8164 [==============================] - 0s 5us/sample - loss: 0.6832 - acc: 0.5603\n",
      "Epoch 21/30\n",
      "8164/8164 [==============================] - 0s 6us/sample - loss: 0.6821 - acc: 0.5674\n",
      "Epoch 22/30\n",
      "8164/8164 [==============================] - 0s 6us/sample - loss: 0.6818 - acc: 0.5665\n",
      "Epoch 23/30\n",
      "8164/8164 [==============================] - 0s 5us/sample - loss: 0.6837 - acc: 0.5562\n",
      "Epoch 24/30\n",
      "8164/8164 [==============================] - 0s 5us/sample - loss: 0.6816 - acc: 0.5646\n",
      "Epoch 25/30\n",
      "8164/8164 [==============================] - 0s 6us/sample - loss: 0.6837 - acc: 0.5576\n",
      "Epoch 26/30\n",
      "8164/8164 [==============================] - 0s 6us/sample - loss: 0.6826 - acc: 0.5616\n",
      "Epoch 27/30\n",
      "8164/8164 [==============================] - 0s 5us/sample - loss: 0.6820 - acc: 0.5600\n",
      "Epoch 28/30\n",
      "8164/8164 [==============================] - 0s 5us/sample - loss: 0.6820 - acc: 0.5566\n",
      "Epoch 29/30\n",
      "8164/8164 [==============================] - 0s 7us/sample - loss: 0.6823 - acc: 0.5593\n",
      "Epoch 30/30\n",
      "8164/8164 [==============================] - 0s 6us/sample - loss: 0.6809 - acc: 0.5676\n",
      "attack model no. 8\n",
      "Train on 7792 samples\n",
      "Epoch 1/30\n",
      "7792/7792 [==============================] - 0s 59us/sample - loss: 0.6951 - acc: 0.5072\n",
      "Epoch 2/30\n",
      "7792/7792 [==============================] - 0s 5us/sample - loss: 0.6923 - acc: 0.5203\n",
      "Epoch 3/30\n",
      "7792/7792 [==============================] - 0s 6us/sample - loss: 0.6916 - acc: 0.5239\n",
      "Epoch 4/30\n",
      "7792/7792 [==============================] - 0s 6us/sample - loss: 0.6904 - acc: 0.5377\n",
      "Epoch 5/30\n",
      "7792/7792 [==============================] - 0s 6us/sample - loss: 0.6893 - acc: 0.5468\n",
      "Epoch 6/30\n",
      "7792/7792 [==============================] - 0s 5us/sample - loss: 0.6894 - acc: 0.5304\n",
      "Epoch 7/30\n",
      "7792/7792 [==============================] - 0s 6us/sample - loss: 0.6898 - acc: 0.5347\n",
      "Epoch 8/30\n",
      "7792/7792 [==============================] - 0s 7us/sample - loss: 0.6896 - acc: 0.5411\n",
      "Epoch 9/30\n",
      "7792/7792 [==============================] - 0s 6us/sample - loss: 0.6880 - acc: 0.5409\n",
      "Epoch 10/30\n",
      "7792/7792 [==============================] - 0s 6us/sample - loss: 0.6907 - acc: 0.5254\n",
      "Epoch 11/30\n",
      "7792/7792 [==============================] - 0s 6us/sample - loss: 0.6874 - acc: 0.5461\n",
      "Epoch 12/30\n",
      "7792/7792 [==============================] - 0s 6us/sample - loss: 0.6885 - acc: 0.5394\n",
      "Epoch 13/30\n",
      "7792/7792 [==============================] - 0s 6us/sample - loss: 0.6893 - acc: 0.5318\n",
      "Epoch 14/30\n",
      "7792/7792 [==============================] - 0s 6us/sample - loss: 0.6876 - acc: 0.5420\n",
      "Epoch 15/30\n",
      "7792/7792 [==============================] - 0s 6us/sample - loss: 0.6885 - acc: 0.5326\n",
      "Epoch 16/30\n",
      "7792/7792 [==============================] - 0s 6us/sample - loss: 0.6885 - acc: 0.5373\n",
      "Epoch 17/30\n",
      "7792/7792 [==============================] - 0s 6us/sample - loss: 0.6883 - acc: 0.5343\n",
      "Epoch 18/30\n",
      "7792/7792 [==============================] - 0s 6us/sample - loss: 0.6868 - acc: 0.5445\n",
      "Epoch 19/30\n",
      "7792/7792 [==============================] - 0s 6us/sample - loss: 0.6864 - acc: 0.5454\n",
      "Epoch 20/30\n",
      "7792/7792 [==============================] - 0s 7us/sample - loss: 0.6873 - acc: 0.5434\n",
      "Epoch 21/30\n",
      "7792/7792 [==============================] - 0s 6us/sample - loss: 0.6868 - acc: 0.5415\n",
      "Epoch 22/30\n",
      "7792/7792 [==============================] - 0s 6us/sample - loss: 0.6870 - acc: 0.5388\n",
      "Epoch 23/30\n",
      "7792/7792 [==============================] - 0s 5us/sample - loss: 0.6866 - acc: 0.5389\n",
      "Epoch 24/30\n",
      "7792/7792 [==============================] - 0s 6us/sample - loss: 0.6879 - acc: 0.5370\n",
      "Epoch 25/30\n",
      "7792/7792 [==============================] - 0s 6us/sample - loss: 0.6860 - acc: 0.5416\n",
      "Epoch 26/30\n",
      "7792/7792 [==============================] - 0s 6us/sample - loss: 0.6869 - acc: 0.5409\n",
      "Epoch 27/30\n",
      "7792/7792 [==============================] - 0s 5us/sample - loss: 0.6860 - acc: 0.5494\n",
      "Epoch 28/30\n",
      "7792/7792 [==============================] - 0s 6us/sample - loss: 0.6849 - acc: 0.5498\n",
      "Epoch 29/30\n",
      "7792/7792 [==============================] - 0s 6us/sample - loss: 0.6866 - acc: 0.5397\n",
      "Epoch 30/30\n",
      "7792/7792 [==============================] - 0s 6us/sample - loss: 0.6849 - acc: 0.5526\n",
      "attack model no. 9\n",
      "Train on 8056 samples\n",
      "Epoch 1/30\n",
      "8056/8056 [==============================] - 0s 59us/sample - loss: 0.6934 - acc: 0.5053\n",
      "Epoch 2/30\n",
      "8056/8056 [==============================] - 0s 6us/sample - loss: 0.6899 - acc: 0.5376\n",
      "Epoch 3/30\n",
      "8056/8056 [==============================] - 0s 6us/sample - loss: 0.6874 - acc: 0.5514\n",
      "Epoch 4/30\n",
      "8056/8056 [==============================] - 0s 6us/sample - loss: 0.6853 - acc: 0.5560\n",
      "Epoch 5/30\n",
      "8056/8056 [==============================] - 0s 6us/sample - loss: 0.6863 - acc: 0.5516\n",
      "Epoch 6/30\n",
      "8056/8056 [==============================] - 0s 5us/sample - loss: 0.6814 - acc: 0.5655\n",
      "Epoch 7/30\n",
      "8056/8056 [==============================] - 0s 5us/sample - loss: 0.6835 - acc: 0.5564\n",
      "Epoch 8/30\n",
      "8056/8056 [==============================] - 0s 6us/sample - loss: 0.6853 - acc: 0.5559\n",
      "Epoch 9/30\n",
      "8056/8056 [==============================] - 0s 6us/sample - loss: 0.6847 - acc: 0.5565\n",
      "Epoch 10/30\n",
      "8056/8056 [==============================] - 0s 6us/sample - loss: 0.6822 - acc: 0.5572\n",
      "Epoch 11/30\n",
      "8056/8056 [==============================] - 0s 6us/sample - loss: 0.6830 - acc: 0.5567\n",
      "Epoch 12/30\n",
      "8056/8056 [==============================] - 0s 6us/sample - loss: 0.6834 - acc: 0.5567\n",
      "Epoch 13/30\n",
      "8056/8056 [==============================] - 0s 6us/sample - loss: 0.6820 - acc: 0.5570\n",
      "Epoch 14/30\n",
      "8056/8056 [==============================] - 0s 6us/sample - loss: 0.6837 - acc: 0.5514\n",
      "Epoch 15/30\n",
      "8056/8056 [==============================] - 0s 6us/sample - loss: 0.6808 - acc: 0.5637\n",
      "Epoch 16/30\n",
      "8056/8056 [==============================] - 0s 6us/sample - loss: 0.6801 - acc: 0.5660\n",
      "Epoch 17/30\n",
      "8056/8056 [==============================] - 0s 5us/sample - loss: 0.6818 - acc: 0.5576\n",
      "Epoch 18/30\n",
      "8056/8056 [==============================] - 0s 6us/sample - loss: 0.6815 - acc: 0.5544\n",
      "Epoch 19/30\n",
      "8056/8056 [==============================] - 0s 6us/sample - loss: 0.6825 - acc: 0.5510\n",
      "Epoch 20/30\n",
      "8056/8056 [==============================] - 0s 6us/sample - loss: 0.6816 - acc: 0.5554\n",
      "Epoch 21/30\n",
      "8056/8056 [==============================] - 0s 6us/sample - loss: 0.6808 - acc: 0.5595\n",
      "Epoch 22/30\n",
      "8056/8056 [==============================] - 0s 6us/sample - loss: 0.6788 - acc: 0.5685\n",
      "Epoch 23/30\n",
      "8056/8056 [==============================] - 0s 5us/sample - loss: 0.6813 - acc: 0.5588\n",
      "Epoch 24/30\n",
      "8056/8056 [==============================] - 0s 6us/sample - loss: 0.6803 - acc: 0.5659\n",
      "Epoch 25/30\n",
      "8056/8056 [==============================] - 0s 6us/sample - loss: 0.6800 - acc: 0.5632\n",
      "Epoch 26/30\n",
      "8056/8056 [==============================] - 0s 6us/sample - loss: 0.6810 - acc: 0.5564\n",
      "Epoch 27/30\n",
      "8056/8056 [==============================] - 0s 6us/sample - loss: 0.6798 - acc: 0.5611\n",
      "Epoch 28/30\n",
      "8056/8056 [==============================] - 0s 6us/sample - loss: 0.6816 - acc: 0.5583\n",
      "Epoch 29/30\n",
      "8056/8056 [==============================] - 0s 6us/sample - loss: 0.6783 - acc: 0.5663\n",
      "Epoch 30/30\n",
      "8056/8056 [==============================] - 0s 6us/sample - loss: 0.6781 - acc: 0.5699\n",
      "attack model no. 0\n",
      "Train on 7912 samples\n",
      "Epoch 1/30\n",
      "7912/7912 [==============================] - 0s 62us/sample - loss: 0.6899 - acc: 0.5292\n",
      "Epoch 2/30\n",
      "7912/7912 [==============================] - 0s 4us/sample - loss: 0.6830 - acc: 0.5674\n",
      "Epoch 3/30\n",
      "7912/7912 [==============================] - 0s 4us/sample - loss: 0.6792 - acc: 0.5718\n",
      "Epoch 4/30\n",
      "7912/7912 [==============================] - 0s 4us/sample - loss: 0.6757 - acc: 0.5825\n",
      "Epoch 5/30\n",
      "7912/7912 [==============================] - 0s 4us/sample - loss: 0.6759 - acc: 0.5838\n",
      "Epoch 6/30\n",
      "7912/7912 [==============================] - 0s 4us/sample - loss: 0.6722 - acc: 0.5843\n",
      "Epoch 7/30\n",
      "7912/7912 [==============================] - 0s 5us/sample - loss: 0.6713 - acc: 0.5877\n",
      "Epoch 8/30\n",
      "7912/7912 [==============================] - 0s 5us/sample - loss: 0.6698 - acc: 0.5891\n",
      "Epoch 9/30\n",
      "7912/7912 [==============================] - 0s 5us/sample - loss: 0.6698 - acc: 0.5866\n",
      "Epoch 10/30\n",
      "7912/7912 [==============================] - 0s 4us/sample - loss: 0.6660 - acc: 0.5873\n",
      "Epoch 11/30\n",
      "7912/7912 [==============================] - 0s 4us/sample - loss: 0.6667 - acc: 0.5925\n",
      "Epoch 12/30\n",
      "7912/7912 [==============================] - 0s 4us/sample - loss: 0.6668 - acc: 0.5870\n",
      "Epoch 13/30\n",
      "7912/7912 [==============================] - 0s 5us/sample - loss: 0.6634 - acc: 0.5992\n",
      "Epoch 14/30\n",
      "7912/7912 [==============================] - 0s 4us/sample - loss: 0.6657 - acc: 0.5829\n",
      "Epoch 15/30\n",
      "7912/7912 [==============================] - 0s 4us/sample - loss: 0.6646 - acc: 0.5911\n",
      "Epoch 16/30\n",
      "7912/7912 [==============================] - 0s 5us/sample - loss: 0.6619 - acc: 0.5925\n",
      "Epoch 17/30\n",
      "7912/7912 [==============================] - 0s 4us/sample - loss: 0.6627 - acc: 0.5934\n",
      "Epoch 18/30\n",
      "7912/7912 [==============================] - 0s 4us/sample - loss: 0.6631 - acc: 0.5987\n",
      "Epoch 19/30\n",
      "7912/7912 [==============================] - 0s 5us/sample - loss: 0.6632 - acc: 0.5958\n",
      "Epoch 20/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7912/7912 [==============================] - 0s 4us/sample - loss: 0.6598 - acc: 0.6004\n",
      "Epoch 21/30\n",
      "7912/7912 [==============================] - 0s 5us/sample - loss: 0.6613 - acc: 0.5952\n",
      "Epoch 22/30\n",
      "7912/7912 [==============================] - 0s 4us/sample - loss: 0.6609 - acc: 0.5967\n",
      "Epoch 23/30\n",
      "7912/7912 [==============================] - 0s 5us/sample - loss: 0.6586 - acc: 0.5993\n",
      "Epoch 24/30\n",
      "7912/7912 [==============================] - 0s 5us/sample - loss: 0.6623 - acc: 0.5945\n",
      "Epoch 25/30\n",
      "7912/7912 [==============================] - 0s 4us/sample - loss: 0.6621 - acc: 0.5952\n",
      "Epoch 26/30\n",
      "7912/7912 [==============================] - 0s 5us/sample - loss: 0.6581 - acc: 0.5974\n",
      "Epoch 27/30\n",
      "7912/7912 [==============================] - 0s 5us/sample - loss: 0.6600 - acc: 0.6009\n",
      "Epoch 28/30\n",
      "7912/7912 [==============================] - 0s 4us/sample - loss: 0.6610 - acc: 0.5943\n",
      "Epoch 29/30\n",
      "7912/7912 [==============================] - 0s 5us/sample - loss: 0.6603 - acc: 0.5925\n",
      "Epoch 30/30\n",
      "7912/7912 [==============================] - 0s 4us/sample - loss: 0.6596 - acc: 0.5962\n",
      "attack model no. 1\n",
      "Train on 8196 samples\n",
      "Epoch 1/30\n",
      "8196/8196 [==============================] - 0s 59us/sample - loss: 0.6862 - acc: 0.5549\n",
      "Epoch 2/30\n",
      "8196/8196 [==============================] - 0s 5us/sample - loss: 0.6775 - acc: 0.5885\n",
      "Epoch 3/30\n",
      "8196/8196 [==============================] - 0s 5us/sample - loss: 0.6710 - acc: 0.6052\n",
      "Epoch 4/30\n",
      "8196/8196 [==============================] - 0s 5us/sample - loss: 0.6656 - acc: 0.6081\n",
      "Epoch 5/30\n",
      "8196/8196 [==============================] - 0s 5us/sample - loss: 0.6610 - acc: 0.6138\n",
      "Epoch 6/30\n",
      "8196/8196 [==============================] - 0s 5us/sample - loss: 0.6579 - acc: 0.6122\n",
      "Epoch 7/30\n",
      "8196/8196 [==============================] - 0s 5us/sample - loss: 0.6600 - acc: 0.6055\n",
      "Epoch 8/30\n",
      "8196/8196 [==============================] - 0s 5us/sample - loss: 0.6526 - acc: 0.6163\n",
      "Epoch 9/30\n",
      "8196/8196 [==============================] - 0s 5us/sample - loss: 0.6511 - acc: 0.6184\n",
      "Epoch 10/30\n",
      "8196/8196 [==============================] - 0s 5us/sample - loss: 0.6514 - acc: 0.6135\n",
      "Epoch 11/30\n",
      "8196/8196 [==============================] - 0s 5us/sample - loss: 0.6544 - acc: 0.6043\n",
      "Epoch 12/30\n",
      "8196/8196 [==============================] - 0s 5us/sample - loss: 0.6456 - acc: 0.6229\n",
      "Epoch 13/30\n",
      "8196/8196 [==============================] - 0s 5us/sample - loss: 0.6487 - acc: 0.6142\n",
      "Epoch 14/30\n",
      "8196/8196 [==============================] - 0s 5us/sample - loss: 0.6488 - acc: 0.6149\n",
      "Epoch 15/30\n",
      "8196/8196 [==============================] - 0s 5us/sample - loss: 0.6475 - acc: 0.6177\n",
      "Epoch 16/30\n",
      "8196/8196 [==============================] - 0s 5us/sample - loss: 0.6445 - acc: 0.6169\n",
      "Epoch 17/30\n",
      "8196/8196 [==============================] - 0s 5us/sample - loss: 0.6424 - acc: 0.6279\n",
      "Epoch 18/30\n",
      "8196/8196 [==============================] - 0s 5us/sample - loss: 0.6466 - acc: 0.6180\n",
      "Epoch 19/30\n",
      "8196/8196 [==============================] - 0s 6us/sample - loss: 0.6458 - acc: 0.6180\n",
      "Epoch 20/30\n",
      "8196/8196 [==============================] - 0s 5us/sample - loss: 0.6475 - acc: 0.6187\n",
      "Epoch 21/30\n",
      "8196/8196 [==============================] - 0s 5us/sample - loss: 0.6426 - acc: 0.6199\n",
      "Epoch 22/30\n",
      "8196/8196 [==============================] - 0s 5us/sample - loss: 0.6411 - acc: 0.6264\n",
      "Epoch 23/30\n",
      "8196/8196 [==============================] - 0s 5us/sample - loss: 0.6407 - acc: 0.6219\n",
      "Epoch 24/30\n",
      "8196/8196 [==============================] - 0s 5us/sample - loss: 0.6429 - acc: 0.6245\n",
      "Epoch 25/30\n",
      "8196/8196 [==============================] - 0s 5us/sample - loss: 0.6432 - acc: 0.6269\n",
      "Epoch 26/30\n",
      "8196/8196 [==============================] - 0s 5us/sample - loss: 0.6445 - acc: 0.6199\n",
      "Epoch 27/30\n",
      "8196/8196 [==============================] - 0s 5us/sample - loss: 0.6445 - acc: 0.6259\n",
      "Epoch 28/30\n",
      "8196/8196 [==============================] - 0s 5us/sample - loss: 0.6418 - acc: 0.6227\n",
      "Epoch 29/30\n",
      "8196/8196 [==============================] - 0s 5us/sample - loss: 0.6383 - acc: 0.6264\n",
      "Epoch 30/30\n",
      "8196/8196 [==============================] - 0s 5us/sample - loss: 0.6424 - acc: 0.6243\n",
      "attack model no. 2\n",
      "Train on 7900 samples\n",
      "Epoch 1/30\n",
      "7900/7900 [==============================] - 1s 64us/sample - loss: 0.6947 - acc: 0.5222\n",
      "Epoch 2/30\n",
      "7900/7900 [==============================] - 0s 7us/sample - loss: 0.6851 - acc: 0.5571\n",
      "Epoch 3/30\n",
      "7900/7900 [==============================] - 0s 8us/sample - loss: 0.6771 - acc: 0.5880\n",
      "Epoch 4/30\n",
      "7900/7900 [==============================] - 0s 5us/sample - loss: 0.6721 - acc: 0.5905\n",
      "Epoch 5/30\n",
      "7900/7900 [==============================] - 0s 5us/sample - loss: 0.6665 - acc: 0.6009\n",
      "Epoch 6/30\n",
      "7900/7900 [==============================] - 0s 5us/sample - loss: 0.6675 - acc: 0.5997\n",
      "Epoch 7/30\n",
      "7900/7900 [==============================] - 0s 5us/sample - loss: 0.6611 - acc: 0.6113\n",
      "Epoch 8/30\n",
      "7900/7900 [==============================] - 0s 5us/sample - loss: 0.6631 - acc: 0.6076\n",
      "Epoch 9/30\n",
      "7900/7900 [==============================] - 0s 5us/sample - loss: 0.6649 - acc: 0.5968\n",
      "Epoch 10/30\n",
      "7900/7900 [==============================] - 0s 5us/sample - loss: 0.6612 - acc: 0.6015\n",
      "Epoch 11/30\n",
      "7900/7900 [==============================] - 0s 5us/sample - loss: 0.6598 - acc: 0.6044\n",
      "Epoch 12/30\n",
      "7900/7900 [==============================] - 0s 5us/sample - loss: 0.6613 - acc: 0.5951\n",
      "Epoch 13/30\n",
      "7900/7900 [==============================] - 0s 5us/sample - loss: 0.6590 - acc: 0.6022\n",
      "Epoch 14/30\n",
      "7900/7900 [==============================] - 0s 5us/sample - loss: 0.6565 - acc: 0.6065\n",
      "Epoch 15/30\n",
      "7900/7900 [==============================] - 0s 5us/sample - loss: 0.6591 - acc: 0.6051\n",
      "Epoch 16/30\n",
      "7900/7900 [==============================] - 0s 5us/sample - loss: 0.6568 - acc: 0.6027\n",
      "Epoch 17/30\n",
      "7900/7900 [==============================] - 0s 5us/sample - loss: 0.6580 - acc: 0.6009\n",
      "Epoch 18/30\n",
      "7900/7900 [==============================] - 0s 5us/sample - loss: 0.6557 - acc: 0.6020\n",
      "Epoch 19/30\n",
      "7900/7900 [==============================] - 0s 5us/sample - loss: 0.6556 - acc: 0.6100\n",
      "Epoch 20/30\n",
      "7900/7900 [==============================] - 0s 5us/sample - loss: 0.6549 - acc: 0.6027\n",
      "Epoch 21/30\n",
      "7900/7900 [==============================] - 0s 5us/sample - loss: 0.6555 - acc: 0.6029\n",
      "Epoch 22/30\n",
      "7900/7900 [==============================] - 0s 5us/sample - loss: 0.6556 - acc: 0.6053\n",
      "Epoch 23/30\n",
      "7900/7900 [==============================] - 0s 5us/sample - loss: 0.6541 - acc: 0.6078\n",
      "Epoch 24/30\n",
      "7900/7900 [==============================] - 0s 5us/sample - loss: 0.6552 - acc: 0.6038\n",
      "Epoch 25/30\n",
      "7900/7900 [==============================] - 0s 5us/sample - loss: 0.6563 - acc: 0.6024\n",
      "Epoch 26/30\n",
      "7900/7900 [==============================] - 0s 5us/sample - loss: 0.6552 - acc: 0.6003\n",
      "Epoch 27/30\n",
      "7900/7900 [==============================] - 0s 5us/sample - loss: 0.6556 - acc: 0.6006\n",
      "Epoch 28/30\n",
      "7900/7900 [==============================] - 0s 5us/sample - loss: 0.6533 - acc: 0.6086\n",
      "Epoch 29/30\n",
      "7900/7900 [==============================] - 0s 5us/sample - loss: 0.6543 - acc: 0.6034\n",
      "Epoch 30/30\n",
      "7900/7900 [==============================] - 0s 5us/sample - loss: 0.6525 - acc: 0.6090\n",
      "attack model no. 3\n",
      "Train on 7968 samples\n",
      "Epoch 1/30\n",
      "7968/7968 [==============================] - 1s 73us/sample - loss: 0.7032 - acc: 0.4971\n",
      "Epoch 2/30\n",
      "7968/7968 [==============================] - 0s 5us/sample - loss: 0.6906 - acc: 0.5290\n",
      "Epoch 3/30\n",
      "7968/7968 [==============================] - 0s 5us/sample - loss: 0.6822 - acc: 0.5678\n",
      "Epoch 4/30\n",
      "7968/7968 [==============================] - 0s 5us/sample - loss: 0.6765 - acc: 0.5870\n",
      "Epoch 5/30\n",
      "7968/7968 [==============================] - 0s 5us/sample - loss: 0.6691 - acc: 0.6088\n",
      "Epoch 6/30\n",
      "7968/7968 [==============================] - 0s 6us/sample - loss: 0.6620 - acc: 0.6171\n",
      "Epoch 7/30\n",
      "7968/7968 [==============================] - 0s 5us/sample - loss: 0.6516 - acc: 0.6319\n",
      "Epoch 8/30\n",
      "7968/7968 [==============================] - 0s 5us/sample - loss: 0.6484 - acc: 0.6222\n",
      "Epoch 9/30\n",
      "7968/7968 [==============================] - 0s 5us/sample - loss: 0.6388 - acc: 0.6407\n",
      "Epoch 10/30\n",
      "7968/7968 [==============================] - 0s 5us/sample - loss: 0.6387 - acc: 0.6348\n",
      "Epoch 11/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7968/7968 [==============================] - 0s 5us/sample - loss: 0.6392 - acc: 0.6317\n",
      "Epoch 12/30\n",
      "7968/7968 [==============================] - 0s 5us/sample - loss: 0.6373 - acc: 0.6383\n",
      "Epoch 13/30\n",
      "7968/7968 [==============================] - 0s 5us/sample - loss: 0.6316 - acc: 0.6443\n",
      "Epoch 14/30\n",
      "7968/7968 [==============================] - 0s 5us/sample - loss: 0.6354 - acc: 0.6386\n",
      "Epoch 15/30\n",
      "7968/7968 [==============================] - 0s 5us/sample - loss: 0.6328 - acc: 0.6360\n",
      "Epoch 16/30\n",
      "7968/7968 [==============================] - 0s 5us/sample - loss: 0.6355 - acc: 0.6364\n",
      "Epoch 17/30\n",
      "7968/7968 [==============================] - 0s 5us/sample - loss: 0.6364 - acc: 0.6298\n",
      "Epoch 18/30\n",
      "7968/7968 [==============================] - 0s 5us/sample - loss: 0.6351 - acc: 0.6338\n",
      "Epoch 19/30\n",
      "7968/7968 [==============================] - 0s 5us/sample - loss: 0.6319 - acc: 0.6396\n",
      "Epoch 20/30\n",
      "7968/7968 [==============================] - 0s 5us/sample - loss: 0.6298 - acc: 0.6392\n",
      "Epoch 21/30\n",
      "7968/7968 [==============================] - 0s 5us/sample - loss: 0.6282 - acc: 0.6401\n",
      "Epoch 22/30\n",
      "7968/7968 [==============================] - 0s 5us/sample - loss: 0.6330 - acc: 0.6360\n",
      "Epoch 23/30\n",
      "7968/7968 [==============================] - 0s 5us/sample - loss: 0.6311 - acc: 0.6357\n",
      "Epoch 24/30\n",
      "7968/7968 [==============================] - 0s 5us/sample - loss: 0.6315 - acc: 0.6350\n",
      "Epoch 25/30\n",
      "7968/7968 [==============================] - 0s 5us/sample - loss: 0.6276 - acc: 0.6406\n",
      "Epoch 26/30\n",
      "7968/7968 [==============================] - 0s 5us/sample - loss: 0.6294 - acc: 0.6376\n",
      "Epoch 27/30\n",
      "7968/7968 [==============================] - 0s 5us/sample - loss: 0.6244 - acc: 0.6462\n",
      "Epoch 28/30\n",
      "7968/7968 [==============================] - 0s 5us/sample - loss: 0.6292 - acc: 0.6382\n",
      "Epoch 29/30\n",
      "7968/7968 [==============================] - 0s 5us/sample - loss: 0.6299 - acc: 0.6362\n",
      "Epoch 30/30\n",
      "7968/7968 [==============================] - 0s 5us/sample - loss: 0.6286 - acc: 0.6373\n",
      "attack model no. 4\n",
      "Train on 8000 samples\n",
      "Epoch 1/30\n",
      "8000/8000 [==============================] - 1s 69us/sample - loss: 0.6876 - acc: 0.5290\n",
      "Epoch 2/30\n",
      "8000/8000 [==============================] - 0s 5us/sample - loss: 0.6838 - acc: 0.5558\n",
      "Epoch 3/30\n",
      "8000/8000 [==============================] - 0s 5us/sample - loss: 0.6793 - acc: 0.5811\n",
      "Epoch 4/30\n",
      "8000/8000 [==============================] - 0s 6us/sample - loss: 0.6766 - acc: 0.5794\n",
      "Epoch 5/30\n",
      "8000/8000 [==============================] - 0s 6us/sample - loss: 0.6766 - acc: 0.5781\n",
      "Epoch 6/30\n",
      "8000/8000 [==============================] - 0s 6us/sample - loss: 0.6729 - acc: 0.5882\n",
      "Epoch 7/30\n",
      "8000/8000 [==============================] - 0s 5us/sample - loss: 0.6748 - acc: 0.5804\n",
      "Epoch 8/30\n",
      "8000/8000 [==============================] - 0s 5us/sample - loss: 0.6730 - acc: 0.5741\n",
      "Epoch 9/30\n",
      "8000/8000 [==============================] - 0s 6us/sample - loss: 0.6704 - acc: 0.5867\n",
      "Epoch 10/30\n",
      "8000/8000 [==============================] - 0s 6us/sample - loss: 0.6719 - acc: 0.5842\n",
      "Epoch 11/30\n",
      "8000/8000 [==============================] - 0s 6us/sample - loss: 0.6686 - acc: 0.5856\n",
      "Epoch 12/30\n",
      "8000/8000 [==============================] - 0s 7us/sample - loss: 0.6704 - acc: 0.5835\n",
      "Epoch 13/30\n",
      "8000/8000 [==============================] - 0s 5us/sample - loss: 0.6718 - acc: 0.5790\n",
      "Epoch 14/30\n",
      "8000/8000 [==============================] - 0s 5us/sample - loss: 0.6701 - acc: 0.5845\n",
      "Epoch 15/30\n",
      "8000/8000 [==============================] - 0s 5us/sample - loss: 0.6684 - acc: 0.5861\n",
      "Epoch 16/30\n",
      "8000/8000 [==============================] - 0s 5us/sample - loss: 0.6666 - acc: 0.5944\n",
      "Epoch 17/30\n",
      "8000/8000 [==============================] - 0s 5us/sample - loss: 0.6657 - acc: 0.5944\n",
      "Epoch 18/30\n",
      "8000/8000 [==============================] - 0s 6us/sample - loss: 0.6661 - acc: 0.5915\n",
      "Epoch 19/30\n",
      "8000/8000 [==============================] - 0s 9us/sample - loss: 0.6696 - acc: 0.5803\n",
      "Epoch 20/30\n",
      "8000/8000 [==============================] - 0s 7us/sample - loss: 0.6664 - acc: 0.5924\n",
      "Epoch 21/30\n",
      "8000/8000 [==============================] - 0s 6us/sample - loss: 0.6668 - acc: 0.5855\n",
      "Epoch 22/30\n",
      "8000/8000 [==============================] - 0s 6us/sample - loss: 0.6682 - acc: 0.5921\n",
      "Epoch 23/30\n",
      "8000/8000 [==============================] - 0s 6us/sample - loss: 0.6666 - acc: 0.5857\n",
      "Epoch 24/30\n",
      "8000/8000 [==============================] - 0s 6us/sample - loss: 0.6676 - acc: 0.5896\n",
      "Epoch 25/30\n",
      "8000/8000 [==============================] - 0s 7us/sample - loss: 0.6650 - acc: 0.5903\n",
      "Epoch 26/30\n",
      "8000/8000 [==============================] - 0s 6us/sample - loss: 0.6630 - acc: 0.5969\n",
      "Epoch 27/30\n",
      "8000/8000 [==============================] - 0s 7us/sample - loss: 0.6655 - acc: 0.5909\n",
      "Epoch 28/30\n",
      "8000/8000 [==============================] - 0s 7us/sample - loss: 0.6660 - acc: 0.5792\n",
      "Epoch 29/30\n",
      "8000/8000 [==============================] - 0s 6us/sample - loss: 0.6677 - acc: 0.5876\n",
      "Epoch 30/30\n",
      "8000/8000 [==============================] - 0s 6us/sample - loss: 0.6663 - acc: 0.5876\n",
      "attack model no. 5\n",
      "Train on 8068 samples\n",
      "Epoch 1/30\n",
      "8068/8068 [==============================] - 1s 71us/sample - loss: 0.6922 - acc: 0.5107\n",
      "Epoch 2/30\n",
      "8068/8068 [==============================] - 0s 6us/sample - loss: 0.6799 - acc: 0.5852\n",
      "Epoch 3/30\n",
      "8068/8068 [==============================] - 0s 6us/sample - loss: 0.6735 - acc: 0.6020\n",
      "Epoch 4/30\n",
      "8068/8068 [==============================] - 0s 6us/sample - loss: 0.6664 - acc: 0.6076\n",
      "Epoch 5/30\n",
      "8068/8068 [==============================] - 0s 6us/sample - loss: 0.6617 - acc: 0.6173\n",
      "Epoch 6/30\n",
      "8068/8068 [==============================] - 0s 7us/sample - loss: 0.6566 - acc: 0.6181\n",
      "Epoch 7/30\n",
      "8068/8068 [==============================] - 0s 7us/sample - loss: 0.6543 - acc: 0.6217\n",
      "Epoch 8/30\n",
      "8068/8068 [==============================] - 0s 6us/sample - loss: 0.6531 - acc: 0.6200\n",
      "Epoch 9/30\n",
      "8068/8068 [==============================] - 0s 6us/sample - loss: 0.6562 - acc: 0.6175\n",
      "Epoch 10/30\n",
      "8068/8068 [==============================] - 0s 7us/sample - loss: 0.6461 - acc: 0.6277\n",
      "Epoch 11/30\n",
      "8068/8068 [==============================] - 0s 8us/sample - loss: 0.6476 - acc: 0.6268\n",
      "Epoch 12/30\n",
      "8068/8068 [==============================] - 0s 7us/sample - loss: 0.6497 - acc: 0.6197\n",
      "Epoch 13/30\n",
      "8068/8068 [==============================] - 0s 7us/sample - loss: 0.6469 - acc: 0.6239\n",
      "Epoch 14/30\n",
      "8068/8068 [==============================] - 0s 6us/sample - loss: 0.6455 - acc: 0.6272\n",
      "Epoch 15/30\n",
      "8068/8068 [==============================] - 0s 6us/sample - loss: 0.6441 - acc: 0.6306\n",
      "Epoch 16/30\n",
      "8068/8068 [==============================] - 0s 6us/sample - loss: 0.6447 - acc: 0.6265\n",
      "Epoch 17/30\n",
      "8068/8068 [==============================] - 0s 6us/sample - loss: 0.6419 - acc: 0.6301\n",
      "Epoch 18/30\n",
      "8068/8068 [==============================] - 0s 6us/sample - loss: 0.6418 - acc: 0.6296\n",
      "Epoch 19/30\n",
      "8068/8068 [==============================] - 0s 7us/sample - loss: 0.6402 - acc: 0.6331\n",
      "Epoch 20/30\n",
      "8068/8068 [==============================] - 0s 7us/sample - loss: 0.6424 - acc: 0.6226\n",
      "Epoch 21/30\n",
      "8068/8068 [==============================] - 0s 6us/sample - loss: 0.6414 - acc: 0.6301\n",
      "Epoch 22/30\n",
      "8068/8068 [==============================] - 0s 6us/sample - loss: 0.6416 - acc: 0.6294\n",
      "Epoch 23/30\n",
      "8068/8068 [==============================] - 0s 6us/sample - loss: 0.6408 - acc: 0.6309\n",
      "Epoch 24/30\n",
      "8068/8068 [==============================] - 0s 6us/sample - loss: 0.6411 - acc: 0.6259\n",
      "Epoch 25/30\n",
      "8068/8068 [==============================] - 0s 6us/sample - loss: 0.6424 - acc: 0.6289\n",
      "Epoch 26/30\n",
      "8068/8068 [==============================] - 0s 8us/sample - loss: 0.6368 - acc: 0.6371\n",
      "Epoch 27/30\n",
      "8068/8068 [==============================] - 0s 8us/sample - loss: 0.6420 - acc: 0.6295\n",
      "Epoch 28/30\n",
      "8068/8068 [==============================] - 0s 6us/sample - loss: 0.6417 - acc: 0.6267\n",
      "Epoch 29/30\n",
      "8068/8068 [==============================] - 0s 6us/sample - loss: 0.6376 - acc: 0.6342\n",
      "Epoch 30/30\n",
      "8068/8068 [==============================] - 0s 6us/sample - loss: 0.6391 - acc: 0.6274\n",
      "attack model no. 6\n",
      "Train on 7944 samples\n",
      "Epoch 1/30\n",
      "7944/7944 [==============================] - 1s 72us/sample - loss: 0.6922 - acc: 0.5157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30\n",
      "7944/7944 [==============================] - 0s 6us/sample - loss: 0.6843 - acc: 0.5587\n",
      "Epoch 3/30\n",
      "7944/7944 [==============================] - 0s 6us/sample - loss: 0.6783 - acc: 0.5860\n",
      "Epoch 4/30\n",
      "7944/7944 [==============================] - 0s 6us/sample - loss: 0.6744 - acc: 0.5913\n",
      "Epoch 5/30\n",
      "7944/7944 [==============================] - 0s 6us/sample - loss: 0.6714 - acc: 0.5892\n",
      "Epoch 6/30\n",
      "7944/7944 [==============================] - 0s 6us/sample - loss: 0.6665 - acc: 0.5932\n",
      "Epoch 7/30\n",
      "7944/7944 [==============================] - 0s 6us/sample - loss: 0.6680 - acc: 0.5926\n",
      "Epoch 8/30\n",
      "7944/7944 [==============================] - 0s 7us/sample - loss: 0.6625 - acc: 0.6051\n",
      "Epoch 9/30\n",
      "7944/7944 [==============================] - 0s 6us/sample - loss: 0.6645 - acc: 0.5959\n",
      "Epoch 10/30\n",
      "7944/7944 [==============================] - 0s 6us/sample - loss: 0.6623 - acc: 0.5966\n",
      "Epoch 11/30\n",
      "7944/7944 [==============================] - 0s 6us/sample - loss: 0.6635 - acc: 0.5957\n",
      "Epoch 12/30\n",
      "7944/7944 [==============================] - 0s 6us/sample - loss: 0.6611 - acc: 0.5984\n",
      "Epoch 13/30\n",
      "7944/7944 [==============================] - 0s 6us/sample - loss: 0.6622 - acc: 0.6011\n",
      "Epoch 14/30\n",
      "7944/7944 [==============================] - 0s 6us/sample - loss: 0.6604 - acc: 0.5976\n",
      "Epoch 15/30\n",
      "7944/7944 [==============================] - 0s 6us/sample - loss: 0.6599 - acc: 0.5993\n",
      "Epoch 16/30\n",
      "7944/7944 [==============================] - 0s 6us/sample - loss: 0.6609 - acc: 0.5994\n",
      "Epoch 17/30\n",
      "7944/7944 [==============================] - 0s 6us/sample - loss: 0.6607 - acc: 0.5978\n",
      "Epoch 18/30\n",
      "7944/7944 [==============================] - 0s 6us/sample - loss: 0.6613 - acc: 0.6025\n",
      "Epoch 19/30\n",
      "7944/7944 [==============================] - 0s 7us/sample - loss: 0.6623 - acc: 0.5925\n",
      "Epoch 20/30\n",
      "7944/7944 [==============================] - 0s 6us/sample - loss: 0.6597 - acc: 0.6016\n",
      "Epoch 21/30\n",
      "7944/7944 [==============================] - 0s 6us/sample - loss: 0.6597 - acc: 0.6007\n",
      "Epoch 22/30\n",
      "7944/7944 [==============================] - 0s 6us/sample - loss: 0.6584 - acc: 0.6057\n",
      "Epoch 23/30\n",
      "7944/7944 [==============================] - 0s 6us/sample - loss: 0.6554 - acc: 0.6069\n",
      "Epoch 24/30\n",
      "7944/7944 [==============================] - 0s 7us/sample - loss: 0.6589 - acc: 0.6022\n",
      "Epoch 25/30\n",
      "7944/7944 [==============================] - 0s 6us/sample - loss: 0.6601 - acc: 0.6036\n",
      "Epoch 26/30\n",
      "7944/7944 [==============================] - 0s 6us/sample - loss: 0.6563 - acc: 0.6055\n",
      "Epoch 27/30\n",
      "7944/7944 [==============================] - 0s 6us/sample - loss: 0.6567 - acc: 0.6070\n",
      "Epoch 28/30\n",
      "7944/7944 [==============================] - 0s 6us/sample - loss: 0.6563 - acc: 0.6067\n",
      "Epoch 29/30\n",
      "7944/7944 [==============================] - 0s 7us/sample - loss: 0.6549 - acc: 0.6061\n",
      "Epoch 30/30\n",
      "7944/7944 [==============================] - 0s 6us/sample - loss: 0.6570 - acc: 0.6117\n",
      "attack model no. 7\n",
      "Train on 8164 samples\n",
      "Epoch 1/30\n",
      "8164/8164 [==============================] - 1s 72us/sample - loss: 0.6917 - acc: 0.5223\n",
      "Epoch 2/30\n",
      "8164/8164 [==============================] - 0s 6us/sample - loss: 0.6846 - acc: 0.5652\n",
      "Epoch 3/30\n",
      "8164/8164 [==============================] - 0s 6us/sample - loss: 0.6787 - acc: 0.5890\n",
      "Epoch 4/30\n",
      "8164/8164 [==============================] - 0s 7us/sample - loss: 0.6746 - acc: 0.5955\n",
      "Epoch 5/30\n",
      "8164/8164 [==============================] - 0s 7us/sample - loss: 0.6716 - acc: 0.5933\n",
      "Epoch 6/30\n",
      "8164/8164 [==============================] - 0s 7us/sample - loss: 0.6667 - acc: 0.6037\n",
      "Epoch 7/30\n",
      "8164/8164 [==============================] - 0s 6us/sample - loss: 0.6659 - acc: 0.6035\n",
      "Epoch 8/30\n",
      "8164/8164 [==============================] - 0s 7us/sample - loss: 0.6657 - acc: 0.5987\n",
      "Epoch 9/30\n",
      "8164/8164 [==============================] - 0s 6us/sample - loss: 0.6649 - acc: 0.5985\n",
      "Epoch 10/30\n",
      "8164/8164 [==============================] - 0s 7us/sample - loss: 0.6636 - acc: 0.6031\n",
      "Epoch 11/30\n",
      "8164/8164 [==============================] - 0s 8us/sample - loss: 0.6615 - acc: 0.6077\n",
      "Epoch 12/30\n",
      "8164/8164 [==============================] - 0s 8us/sample - loss: 0.6604 - acc: 0.6091\n",
      "Epoch 13/30\n",
      "8164/8164 [==============================] - 0s 7us/sample - loss: 0.6569 - acc: 0.6077\n",
      "Epoch 14/30\n",
      "8164/8164 [==============================] - 0s 7us/sample - loss: 0.6601 - acc: 0.6041\n",
      "Epoch 15/30\n",
      "8164/8164 [==============================] - 0s 7us/sample - loss: 0.6563 - acc: 0.6067\n",
      "Epoch 16/30\n",
      "8164/8164 [==============================] - 0s 6us/sample - loss: 0.6560 - acc: 0.6100\n",
      "Epoch 17/30\n",
      "8164/8164 [==============================] - 0s 6us/sample - loss: 0.6569 - acc: 0.6085\n",
      "Epoch 18/30\n",
      "8164/8164 [==============================] - 0s 6us/sample - loss: 0.6538 - acc: 0.6127\n",
      "Epoch 19/30\n",
      "8164/8164 [==============================] - 0s 7us/sample - loss: 0.6547 - acc: 0.6111\n",
      "Epoch 20/30\n",
      "8164/8164 [==============================] - 0s 7us/sample - loss: 0.6523 - acc: 0.6139\n",
      "Epoch 21/30\n",
      "8164/8164 [==============================] - 0s 7us/sample - loss: 0.6541 - acc: 0.6140\n",
      "Epoch 22/30\n",
      "8164/8164 [==============================] - 0s 7us/sample - loss: 0.6550 - acc: 0.6106\n",
      "Epoch 23/30\n",
      "8164/8164 [==============================] - 0s 6us/sample - loss: 0.6575 - acc: 0.6056\n",
      "Epoch 24/30\n",
      "8164/8164 [==============================] - 0s 6us/sample - loss: 0.6557 - acc: 0.6176\n",
      "Epoch 25/30\n",
      "8164/8164 [==============================] - 0s 7us/sample - loss: 0.6540 - acc: 0.6161\n",
      "Epoch 26/30\n",
      "8164/8164 [==============================] - 0s 8us/sample - loss: 0.6537 - acc: 0.6104\n",
      "Epoch 27/30\n",
      "8164/8164 [==============================] - 0s 6us/sample - loss: 0.6545 - acc: 0.6145\n",
      "Epoch 28/30\n",
      "8164/8164 [==============================] - 0s 6us/sample - loss: 0.6537 - acc: 0.6135\n",
      "Epoch 29/30\n",
      "8164/8164 [==============================] - 0s 7us/sample - loss: 0.6540 - acc: 0.6159\n",
      "Epoch 30/30\n",
      "8164/8164 [==============================] - 0s 7us/sample - loss: 0.6507 - acc: 0.6195\n",
      "attack model no. 8\n",
      "Train on 7792 samples\n",
      "Epoch 1/30\n",
      "7792/7792 [==============================] - 1s 73us/sample - loss: 0.6905 - acc: 0.5313\n",
      "Epoch 2/30\n",
      "7792/7792 [==============================] - 0s 5us/sample - loss: 0.6846 - acc: 0.5616\n",
      "Epoch 3/30\n",
      "7792/7792 [==============================] - 0s 6us/sample - loss: 0.6795 - acc: 0.5842\n",
      "Epoch 4/30\n",
      "7792/7792 [==============================] - 0s 6us/sample - loss: 0.6746 - acc: 0.5873\n",
      "Epoch 5/30\n",
      "7792/7792 [==============================] - 0s 7us/sample - loss: 0.6744 - acc: 0.5806\n",
      "Epoch 6/30\n",
      "7792/7792 [==============================] - 0s 6us/sample - loss: 0.6731 - acc: 0.5833\n",
      "Epoch 7/30\n",
      "7792/7792 [==============================] - 0s 6us/sample - loss: 0.6711 - acc: 0.5878\n",
      "Epoch 8/30\n",
      "7792/7792 [==============================] - 0s 6us/sample - loss: 0.6699 - acc: 0.5877\n",
      "Epoch 9/30\n",
      "7792/7792 [==============================] - 0s 6us/sample - loss: 0.6684 - acc: 0.5892\n",
      "Epoch 10/30\n",
      "7792/7792 [==============================] - 0s 6us/sample - loss: 0.6658 - acc: 0.5918\n",
      "Epoch 11/30\n",
      "7792/7792 [==============================] - 0s 6us/sample - loss: 0.6703 - acc: 0.5861\n",
      "Epoch 12/30\n",
      "7792/7792 [==============================] - 0s 6us/sample - loss: 0.6682 - acc: 0.5871\n",
      "Epoch 13/30\n",
      "7792/7792 [==============================] - 0s 6us/sample - loss: 0.6646 - acc: 0.5834\n",
      "Epoch 14/30\n",
      "7792/7792 [==============================] - 0s 7us/sample - loss: 0.6659 - acc: 0.5893\n",
      "Epoch 15/30\n",
      "7792/7792 [==============================] - 0s 6us/sample - loss: 0.6643 - acc: 0.5939\n",
      "Epoch 16/30\n",
      "7792/7792 [==============================] - 0s 6us/sample - loss: 0.6639 - acc: 0.5871\n",
      "Epoch 17/30\n",
      "7792/7792 [==============================] - 0s 7us/sample - loss: 0.6618 - acc: 0.5954\n",
      "Epoch 18/30\n",
      "7792/7792 [==============================] - 0s 7us/sample - loss: 0.6638 - acc: 0.5898\n",
      "Epoch 19/30\n",
      "7792/7792 [==============================] - 0s 7us/sample - loss: 0.6613 - acc: 0.5948\n",
      "Epoch 20/30\n",
      "7792/7792 [==============================] - 0s 7us/sample - loss: 0.6632 - acc: 0.5968\n",
      "Epoch 21/30\n",
      "7792/7792 [==============================] - 0s 7us/sample - loss: 0.6623 - acc: 0.5968\n",
      "Epoch 22/30\n",
      "7792/7792 [==============================] - 0s 7us/sample - loss: 0.6597 - acc: 0.5988\n",
      "Epoch 23/30\n",
      "7792/7792 [==============================] - 0s 7us/sample - loss: 0.6599 - acc: 0.5963\n",
      "Epoch 24/30\n",
      "7792/7792 [==============================] - 0s 7us/sample - loss: 0.6641 - acc: 0.5898\n",
      "Epoch 25/30\n",
      "7792/7792 [==============================] - 0s 7us/sample - loss: 0.6613 - acc: 0.5989\n",
      "Epoch 26/30\n",
      "7792/7792 [==============================] - 0s 6us/sample - loss: 0.6620 - acc: 0.6009\n",
      "Epoch 27/30\n",
      "7792/7792 [==============================] - 0s 6us/sample - loss: 0.6608 - acc: 0.6007\n",
      "Epoch 28/30\n",
      "7792/7792 [==============================] - 0s 6us/sample - loss: 0.6602 - acc: 0.5943\n",
      "Epoch 29/30\n",
      "7792/7792 [==============================] - 0s 7us/sample - loss: 0.6587 - acc: 0.6024\n",
      "Epoch 30/30\n",
      "7792/7792 [==============================] - 0s 7us/sample - loss: 0.6611 - acc: 0.5996\n",
      "attack model no. 9\n",
      "Train on 8056 samples\n",
      "Epoch 1/30\n",
      "8056/8056 [==============================] - 1s 82us/sample - loss: 0.6815 - acc: 0.5653\n",
      "Epoch 2/30\n",
      "8056/8056 [==============================] - 0s 6us/sample - loss: 0.6696 - acc: 0.6012\n",
      "Epoch 3/30\n",
      "8056/8056 [==============================] - 0s 6us/sample - loss: 0.6578 - acc: 0.6159\n",
      "Epoch 4/30\n",
      "8056/8056 [==============================] - 0s 6us/sample - loss: 0.6526 - acc: 0.6212\n",
      "Epoch 5/30\n",
      "8056/8056 [==============================] - 0s 6us/sample - loss: 0.6494 - acc: 0.6240\n",
      "Epoch 6/30\n",
      "8056/8056 [==============================] - 0s 7us/sample - loss: 0.6481 - acc: 0.6195\n",
      "Epoch 7/30\n",
      "8056/8056 [==============================] - 0s 7us/sample - loss: 0.6452 - acc: 0.6220\n",
      "Epoch 8/30\n",
      "8056/8056 [==============================] - 0s 7us/sample - loss: 0.6388 - acc: 0.6346\n",
      "Epoch 9/30\n",
      "8056/8056 [==============================] - 0s 7us/sample - loss: 0.6405 - acc: 0.6246\n",
      "Epoch 10/30\n",
      "8056/8056 [==============================] - 0s 7us/sample - loss: 0.6394 - acc: 0.6260\n",
      "Epoch 11/30\n",
      "8056/8056 [==============================] - 0s 6us/sample - loss: 0.6371 - acc: 0.6271\n",
      "Epoch 12/30\n",
      "8056/8056 [==============================] - 0s 7us/sample - loss: 0.6345 - acc: 0.6316\n",
      "Epoch 13/30\n",
      "8056/8056 [==============================] - 0s 6us/sample - loss: 0.6354 - acc: 0.6346\n",
      "Epoch 14/30\n",
      "8056/8056 [==============================] - 0s 6us/sample - loss: 0.6344 - acc: 0.6300\n",
      "Epoch 15/30\n",
      "8056/8056 [==============================] - 0s 7us/sample - loss: 0.6352 - acc: 0.6317\n",
      "Epoch 16/30\n",
      "8056/8056 [==============================] - 0s 7us/sample - loss: 0.6363 - acc: 0.6286\n",
      "Epoch 17/30\n",
      "8056/8056 [==============================] - 0s 7us/sample - loss: 0.6302 - acc: 0.6341\n",
      "Epoch 18/30\n",
      "8056/8056 [==============================] - 0s 8us/sample - loss: 0.6339 - acc: 0.6328\n",
      "Epoch 19/30\n",
      "8056/8056 [==============================] - 0s 7us/sample - loss: 0.6343 - acc: 0.6363\n",
      "Epoch 20/30\n",
      "8056/8056 [==============================] - 0s 7us/sample - loss: 0.6330 - acc: 0.6301\n",
      "Epoch 21/30\n",
      "8056/8056 [==============================] - 0s 7us/sample - loss: 0.6334 - acc: 0.6327\n",
      "Epoch 22/30\n",
      "8056/8056 [==============================] - 0s 8us/sample - loss: 0.6326 - acc: 0.6344\n",
      "Epoch 23/30\n",
      "8056/8056 [==============================] - 0s 7us/sample - loss: 0.6304 - acc: 0.6349\n",
      "Epoch 24/30\n",
      "8056/8056 [==============================] - 0s 7us/sample - loss: 0.6301 - acc: 0.6344\n",
      "Epoch 25/30\n",
      "8056/8056 [==============================] - 0s 6us/sample - loss: 0.6290 - acc: 0.6380\n",
      "Epoch 26/30\n",
      "8056/8056 [==============================] - 0s 6us/sample - loss: 0.6332 - acc: 0.6305\n",
      "Epoch 27/30\n",
      "8056/8056 [==============================] - 0s 8us/sample - loss: 0.6326 - acc: 0.6320\n",
      "Epoch 28/30\n",
      "8056/8056 [==============================] - 0s 6us/sample - loss: 0.6297 - acc: 0.6332\n",
      "Epoch 29/30\n",
      "8056/8056 [==============================] - 0s 7us/sample - loss: 0.6351 - acc: 0.6274\n",
      "Epoch 30/30\n",
      "8056/8056 [==============================] - 0s 7us/sample - loss: 0.6264 - acc: 0.6395\n",
      "attack model no. 0\n",
      "Train on 7912 samples\n",
      "Epoch 1/30\n",
      "7912/7912 [==============================] - 1s 104us/sample - loss: 0.6882 - acc: 0.5440\n",
      "Epoch 2/30\n",
      "7912/7912 [==============================] - 0s 6us/sample - loss: 0.6803 - acc: 0.5823\n",
      "Epoch 3/30\n",
      "7912/7912 [==============================] - 0s 6us/sample - loss: 0.6707 - acc: 0.6121\n",
      "Epoch 4/30\n",
      "7912/7912 [==============================] - 0s 6us/sample - loss: 0.6680 - acc: 0.6125\n",
      "Epoch 5/30\n",
      "7912/7912 [==============================] - 0s 6us/sample - loss: 0.6646 - acc: 0.6076\n",
      "Epoch 6/30\n",
      "7912/7912 [==============================] - 0s 8us/sample - loss: 0.6614 - acc: 0.6103\n",
      "Epoch 7/30\n",
      "7912/7912 [==============================] - 0s 6us/sample - loss: 0.6544 - acc: 0.6231\n",
      "Epoch 8/30\n",
      "7912/7912 [==============================] - 0s 7us/sample - loss: 0.6531 - acc: 0.6251\n",
      "Epoch 9/30\n",
      "7912/7912 [==============================] - 0s 7us/sample - loss: 0.6488 - acc: 0.6263\n",
      "Epoch 10/30\n",
      "7912/7912 [==============================] - 0s 7us/sample - loss: 0.6484 - acc: 0.6211\n",
      "Epoch 11/30\n",
      "7912/7912 [==============================] - 0s 6us/sample - loss: 0.6469 - acc: 0.6179\n",
      "Epoch 12/30\n",
      "7912/7912 [==============================] - 0s 7us/sample - loss: 0.6477 - acc: 0.6122\n",
      "Epoch 13/30\n",
      "7912/7912 [==============================] - 0s 7us/sample - loss: 0.6424 - acc: 0.6232\n",
      "Epoch 14/30\n",
      "7912/7912 [==============================] - 0s 7us/sample - loss: 0.6397 - acc: 0.6253\n",
      "Epoch 15/30\n",
      "7912/7912 [==============================] - 0s 6us/sample - loss: 0.6384 - acc: 0.6234\n",
      "Epoch 16/30\n",
      "7912/7912 [==============================] - 0s 9us/sample - loss: 0.6387 - acc: 0.6217\n",
      "Epoch 17/30\n",
      "7912/7912 [==============================] - 0s 7us/sample - loss: 0.6387 - acc: 0.6196\n",
      "Epoch 18/30\n",
      "7912/7912 [==============================] - 0s 6us/sample - loss: 0.6353 - acc: 0.6194\n",
      "Epoch 19/30\n",
      "7912/7912 [==============================] - 0s 6us/sample - loss: 0.6375 - acc: 0.6230\n",
      "Epoch 20/30\n",
      "7912/7912 [==============================] - 0s 7us/sample - loss: 0.6350 - acc: 0.6275\n",
      "Epoch 21/30\n",
      "7912/7912 [==============================] - 0s 7us/sample - loss: 0.6368 - acc: 0.6204\n",
      "Epoch 22/30\n",
      "7912/7912 [==============================] - 0s 7us/sample - loss: 0.6283 - acc: 0.6312\n",
      "Epoch 23/30\n",
      "7912/7912 [==============================] - 0s 7us/sample - loss: 0.6360 - acc: 0.6298\n",
      "Epoch 24/30\n",
      "7912/7912 [==============================] - 0s 7us/sample - loss: 0.6330 - acc: 0.6307\n",
      "Epoch 25/30\n",
      "7912/7912 [==============================] - 0s 7us/sample - loss: 0.6310 - acc: 0.6312\n",
      "Epoch 26/30\n",
      "7912/7912 [==============================] - 0s 7us/sample - loss: 0.6315 - acc: 0.6317\n",
      "Epoch 27/30\n",
      "7912/7912 [==============================] - 0s 7us/sample - loss: 0.6335 - acc: 0.6289\n",
      "Epoch 28/30\n",
      "7912/7912 [==============================] - 0s 7us/sample - loss: 0.6316 - acc: 0.6271\n",
      "Epoch 29/30\n",
      "7912/7912 [==============================] - 0s 7us/sample - loss: 0.6299 - acc: 0.6312\n",
      "Epoch 30/30\n",
      "7912/7912 [==============================] - 0s 7us/sample - loss: 0.6334 - acc: 0.6299\n",
      "attack model no. 1\n",
      "Train on 8196 samples\n",
      "Epoch 1/30\n",
      "8196/8196 [==============================] - 1s 102us/sample - loss: 0.6922 - acc: 0.5228\n",
      "Epoch 2/30\n",
      "8196/8196 [==============================] - 0s 6us/sample - loss: 0.6698 - acc: 0.6153\n",
      "Epoch 3/30\n",
      "8196/8196 [==============================] - 0s 6us/sample - loss: 0.6548 - acc: 0.6490\n",
      "Epoch 4/30\n",
      "8196/8196 [==============================] - 0s 7us/sample - loss: 0.6427 - acc: 0.6615\n",
      "Epoch 5/30\n",
      "8196/8196 [==============================] - 0s 7us/sample - loss: 0.6307 - acc: 0.6625\n",
      "Epoch 6/30\n",
      "8196/8196 [==============================] - 0s 7us/sample - loss: 0.6202 - acc: 0.6646\n",
      "Epoch 7/30\n",
      "8196/8196 [==============================] - 0s 8us/sample - loss: 0.6209 - acc: 0.6600\n",
      "Epoch 8/30\n",
      "8196/8196 [==============================] - 0s 8us/sample - loss: 0.6177 - acc: 0.6584\n",
      "Epoch 9/30\n",
      "8196/8196 [==============================] - 0s 8us/sample - loss: 0.6062 - acc: 0.6704\n",
      "Epoch 10/30\n",
      "8196/8196 [==============================] - 0s 7us/sample - loss: 0.6066 - acc: 0.6635\n",
      "Epoch 11/30\n",
      "8196/8196 [==============================] - 0s 7us/sample - loss: 0.5993 - acc: 0.6673\n",
      "Epoch 12/30\n",
      "8196/8196 [==============================] - 0s 6us/sample - loss: 0.5966 - acc: 0.6723\n",
      "Epoch 13/30\n",
      "8196/8196 [==============================] - 0s 8us/sample - loss: 0.5940 - acc: 0.6762\n",
      "Epoch 14/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8196/8196 [==============================] - 0s 7us/sample - loss: 0.5963 - acc: 0.6761\n",
      "Epoch 15/30\n",
      "8196/8196 [==============================] - 0s 6us/sample - loss: 0.5930 - acc: 0.6711\n",
      "Epoch 16/30\n",
      "8196/8196 [==============================] - 0s 7us/sample - loss: 0.5892 - acc: 0.6836\n",
      "Epoch 17/30\n",
      "8196/8196 [==============================] - 0s 12us/sample - loss: 0.5921 - acc: 0.6748\n",
      "Epoch 18/30\n",
      "8196/8196 [==============================] - 0s 10us/sample - loss: 0.5897 - acc: 0.6809\n",
      "Epoch 19/30\n",
      "8196/8196 [==============================] - 0s 10us/sample - loss: 0.5940 - acc: 0.6731\n",
      "Epoch 20/30\n",
      "8196/8196 [==============================] - 0s 11us/sample - loss: 0.5940 - acc: 0.6789\n",
      "Epoch 21/30\n",
      "8196/8196 [==============================] - 0s 9us/sample - loss: 0.5912 - acc: 0.6773\n",
      "Epoch 22/30\n",
      "8196/8196 [==============================] - 0s 8us/sample - loss: 0.5898 - acc: 0.6813\n",
      "Epoch 23/30\n",
      "8196/8196 [==============================] - 0s 12us/sample - loss: 0.5858 - acc: 0.6813\n",
      "Epoch 24/30\n",
      "8196/8196 [==============================] - 0s 9us/sample - loss: 0.5893 - acc: 0.6786\n",
      "Epoch 25/30\n",
      "8196/8196 [==============================] - 0s 8us/sample - loss: 0.5885 - acc: 0.6807\n",
      "Epoch 26/30\n",
      "8196/8196 [==============================] - 0s 7us/sample - loss: 0.5879 - acc: 0.6809\n",
      "Epoch 27/30\n",
      "8196/8196 [==============================] - 0s 6us/sample - loss: 0.5870 - acc: 0.6790\n",
      "Epoch 28/30\n",
      "8196/8196 [==============================] - 0s 7us/sample - loss: 0.5868 - acc: 0.6813\n",
      "Epoch 29/30\n",
      "8196/8196 [==============================] - 0s 7us/sample - loss: 0.5855 - acc: 0.6823\n",
      "Epoch 30/30\n",
      "8196/8196 [==============================] - 0s 8us/sample - loss: 0.5873 - acc: 0.6839\n",
      "attack model no. 2\n",
      "Train on 7900 samples\n",
      "Epoch 1/30\n",
      "7900/7900 [==============================] - 1s 89us/sample - loss: 0.6892 - acc: 0.5268\n",
      "Epoch 2/30\n",
      "7900/7900 [==============================] - 0s 7us/sample - loss: 0.6779 - acc: 0.5899\n",
      "Epoch 3/30\n",
      "7900/7900 [==============================] - 0s 9us/sample - loss: 0.6655 - acc: 0.6282\n",
      "Epoch 4/30\n",
      "7900/7900 [==============================] - 0s 8us/sample - loss: 0.6529 - acc: 0.6466\n",
      "Epoch 5/30\n",
      "7900/7900 [==============================] - 0s 8us/sample - loss: 0.6430 - acc: 0.6565\n",
      "Epoch 6/30\n",
      "7900/7900 [==============================] - 0s 8us/sample - loss: 0.6350 - acc: 0.6534\n",
      "Epoch 7/30\n",
      "7900/7900 [==============================] - 0s 7us/sample - loss: 0.6346 - acc: 0.6506\n",
      "Epoch 8/30\n",
      "7900/7900 [==============================] - 0s 9us/sample - loss: 0.6273 - acc: 0.6597\n",
      "Epoch 9/30\n",
      "7900/7900 [==============================] - 0s 8us/sample - loss: 0.6244 - acc: 0.6581\n",
      "Epoch 10/30\n",
      "7900/7900 [==============================] - 0s 8us/sample - loss: 0.6267 - acc: 0.6514\n",
      "Epoch 11/30\n",
      "7900/7900 [==============================] - 0s 7us/sample - loss: 0.6203 - acc: 0.6633\n",
      "Epoch 12/30\n",
      "7900/7900 [==============================] - 0s 7us/sample - loss: 0.6194 - acc: 0.6529\n",
      "Epoch 13/30\n",
      "7900/7900 [==============================] - 0s 6us/sample - loss: 0.6182 - acc: 0.6580\n",
      "Epoch 14/30\n",
      "7900/7900 [==============================] - 0s 8us/sample - loss: 0.6132 - acc: 0.6623\n",
      "Epoch 15/30\n",
      "7900/7900 [==============================] - 0s 9us/sample - loss: 0.6165 - acc: 0.6562\n",
      "Epoch 16/30\n",
      "7900/7900 [==============================] - 0s 7us/sample - loss: 0.6164 - acc: 0.6606\n",
      "Epoch 17/30\n",
      "7900/7900 [==============================] - 0s 7us/sample - loss: 0.6138 - acc: 0.6584\n",
      "Epoch 18/30\n",
      "7900/7900 [==============================] - 0s 7us/sample - loss: 0.6168 - acc: 0.6539\n",
      "Epoch 19/30\n",
      "7900/7900 [==============================] - 0s 8us/sample - loss: 0.6086 - acc: 0.6657\n",
      "Epoch 20/30\n",
      "7900/7900 [==============================] - 0s 7us/sample - loss: 0.6076 - acc: 0.6619\n",
      "Epoch 21/30\n",
      "7900/7900 [==============================] - 0s 7us/sample - loss: 0.6114 - acc: 0.6587\n",
      "Epoch 22/30\n",
      "7900/7900 [==============================] - 0s 8us/sample - loss: 0.6125 - acc: 0.6580\n",
      "Epoch 23/30\n",
      "7900/7900 [==============================] - 0s 10us/sample - loss: 0.6082 - acc: 0.6615\n",
      "Epoch 24/30\n",
      "7900/7900 [==============================] - 0s 6us/sample - loss: 0.6076 - acc: 0.6625\n",
      "Epoch 25/30\n",
      "7900/7900 [==============================] - 0s 7us/sample - loss: 0.6083 - acc: 0.6616\n",
      "Epoch 26/30\n",
      "7900/7900 [==============================] - 0s 7us/sample - loss: 0.6062 - acc: 0.6643\n",
      "Epoch 27/30\n",
      "7900/7900 [==============================] - 0s 11us/sample - loss: 0.6105 - acc: 0.6642\n",
      "Epoch 28/30\n",
      "7900/7900 [==============================] - 0s 7us/sample - loss: 0.6082 - acc: 0.6584\n",
      "Epoch 29/30\n",
      "7900/7900 [==============================] - 0s 7us/sample - loss: 0.6068 - acc: 0.6624\n",
      "Epoch 30/30\n",
      "7900/7900 [==============================] - 0s 6us/sample - loss: 0.6060 - acc: 0.6587\n",
      "attack model no. 3\n",
      "Train on 7968 samples\n",
      "Epoch 1/30\n",
      "7968/7968 [==============================] - 1s 83us/sample - loss: 0.6842 - acc: 0.5566\n",
      "Epoch 2/30\n",
      "7968/7968 [==============================] - 0s 7us/sample - loss: 0.6676 - acc: 0.6236\n",
      "Epoch 3/30\n",
      "7968/7968 [==============================] - 0s 8us/sample - loss: 0.6515 - acc: 0.6598\n",
      "Epoch 4/30\n",
      "7968/7968 [==============================] - 0s 8us/sample - loss: 0.6335 - acc: 0.6775\n",
      "Epoch 5/30\n",
      "7968/7968 [==============================] - 0s 7us/sample - loss: 0.6197 - acc: 0.6811\n",
      "Epoch 6/30\n",
      "7968/7968 [==============================] - 0s 8us/sample - loss: 0.6106 - acc: 0.6830\n",
      "Epoch 7/30\n",
      "7968/7968 [==============================] - 0s 7us/sample - loss: 0.6046 - acc: 0.6798\n",
      "Epoch 8/30\n",
      "7968/7968 [==============================] - 0s 7us/sample - loss: 0.5939 - acc: 0.6894\n",
      "Epoch 9/30\n",
      "7968/7968 [==============================] - 0s 8us/sample - loss: 0.5910 - acc: 0.6879\n",
      "Epoch 10/30\n",
      "7968/7968 [==============================] - 0s 7us/sample - loss: 0.5912 - acc: 0.6881\n",
      "Epoch 11/30\n",
      "7968/7968 [==============================] - 0s 8us/sample - loss: 0.5907 - acc: 0.6869\n",
      "Epoch 12/30\n",
      "7968/7968 [==============================] - 0s 8us/sample - loss: 0.5864 - acc: 0.6862\n",
      "Epoch 13/30\n",
      "7968/7968 [==============================] - 0s 8us/sample - loss: 0.5882 - acc: 0.6871\n",
      "Epoch 14/30\n",
      "7968/7968 [==============================] - 0s 7us/sample - loss: 0.5856 - acc: 0.6840\n",
      "Epoch 15/30\n",
      "7968/7968 [==============================] - 0s 8us/sample - loss: 0.5782 - acc: 0.6952\n",
      "Epoch 16/30\n",
      "7968/7968 [==============================] - 0s 8us/sample - loss: 0.5789 - acc: 0.6906\n",
      "Epoch 17/30\n",
      "7968/7968 [==============================] - 0s 7us/sample - loss: 0.5815 - acc: 0.6889\n",
      "Epoch 18/30\n",
      "7968/7968 [==============================] - 0s 7us/sample - loss: 0.5823 - acc: 0.6866\n",
      "Epoch 19/30\n",
      "7968/7968 [==============================] - 0s 8us/sample - loss: 0.5777 - acc: 0.6889\n",
      "Epoch 20/30\n",
      "7968/7968 [==============================] - 0s 7us/sample - loss: 0.5787 - acc: 0.6926\n",
      "Epoch 21/30\n",
      "7968/7968 [==============================] - 0s 7us/sample - loss: 0.5753 - acc: 0.6931\n",
      "Epoch 22/30\n",
      "7968/7968 [==============================] - 0s 9us/sample - loss: 0.5736 - acc: 0.6928\n",
      "Epoch 23/30\n",
      "7968/7968 [==============================] - 0s 8us/sample - loss: 0.5740 - acc: 0.6905\n",
      "Epoch 24/30\n",
      "7968/7968 [==============================] - 0s 8us/sample - loss: 0.5763 - acc: 0.6857\n",
      "Epoch 25/30\n",
      "7968/7968 [==============================] - ETA: 0s - loss: 0.5743 - acc: 0.689 - 0s 8us/sample - loss: 0.5737 - acc: 0.6895\n",
      "Epoch 26/30\n",
      "7968/7968 [==============================] - 0s 10us/sample - loss: 0.5728 - acc: 0.6916\n",
      "Epoch 27/30\n",
      "7968/7968 [==============================] - 0s 8us/sample - loss: 0.5741 - acc: 0.6896\n",
      "Epoch 28/30\n",
      "7968/7968 [==============================] - 0s 7us/sample - loss: 0.5702 - acc: 0.6916\n",
      "Epoch 29/30\n",
      "7968/7968 [==============================] - 0s 7us/sample - loss: 0.5714 - acc: 0.6888\n",
      "Epoch 30/30\n",
      "7968/7968 [==============================] - 0s 8us/sample - loss: 0.5710 - acc: 0.6910\n",
      "attack model no. 4\n",
      "Train on 8000 samples\n",
      "Epoch 1/30\n",
      "8000/8000 [==============================] - 1s 113us/sample - loss: 0.6841 - acc: 0.5328\n",
      "Epoch 2/30\n",
      "8000/8000 [==============================] - 0s 7us/sample - loss: 0.6706 - acc: 0.6044\n",
      "Epoch 3/30\n",
      "8000/8000 [==============================] - 0s 6us/sample - loss: 0.6573 - acc: 0.6344\n",
      "Epoch 4/30\n",
      "8000/8000 [==============================] - 0s 7us/sample - loss: 0.6464 - acc: 0.6340\n",
      "Epoch 5/30\n",
      "8000/8000 [==============================] - 0s 9us/sample - loss: 0.6357 - acc: 0.6459\n",
      "Epoch 6/30\n",
      "8000/8000 [==============================] - 0s 9us/sample - loss: 0.6269 - acc: 0.6515\n",
      "Epoch 7/30\n",
      "8000/8000 [==============================] - 0s 7us/sample - loss: 0.6241 - acc: 0.6522\n",
      "Epoch 8/30\n",
      "8000/8000 [==============================] - 0s 6us/sample - loss: 0.6235 - acc: 0.6485\n",
      "Epoch 9/30\n",
      "8000/8000 [==============================] - 0s 9us/sample - loss: 0.6199 - acc: 0.6510\n",
      "Epoch 10/30\n",
      "8000/8000 [==============================] - 0s 10us/sample - loss: 0.6203 - acc: 0.6515\n",
      "Epoch 11/30\n",
      "8000/8000 [==============================] - 0s 7us/sample - loss: 0.6236 - acc: 0.6423\n",
      "Epoch 12/30\n",
      "8000/8000 [==============================] - 0s 6us/sample - loss: 0.6178 - acc: 0.6530\n",
      "Epoch 13/30\n",
      "8000/8000 [==============================] - 0s 6us/sample - loss: 0.6123 - acc: 0.6612\n",
      "Epoch 14/30\n",
      "8000/8000 [==============================] - 0s 10us/sample - loss: 0.6166 - acc: 0.6555\n",
      "Epoch 15/30\n",
      "8000/8000 [==============================] - 0s 8us/sample - loss: 0.6148 - acc: 0.6526\n",
      "Epoch 16/30\n",
      "8000/8000 [==============================] - 0s 8us/sample - loss: 0.6092 - acc: 0.6644\n",
      "Epoch 17/30\n",
      "8000/8000 [==============================] - 0s 7us/sample - loss: 0.6162 - acc: 0.6521\n",
      "Epoch 18/30\n",
      "8000/8000 [==============================] - 0s 9us/sample - loss: 0.6172 - acc: 0.6488\n",
      "Epoch 19/30\n",
      "8000/8000 [==============================] - 0s 8us/sample - loss: 0.6140 - acc: 0.6550\n",
      "Epoch 20/30\n",
      "8000/8000 [==============================] - 0s 7us/sample - loss: 0.6099 - acc: 0.6562\n",
      "Epoch 21/30\n",
      "8000/8000 [==============================] - 0s 7us/sample - loss: 0.6102 - acc: 0.6584\n",
      "Epoch 22/30\n",
      "8000/8000 [==============================] - 0s 9us/sample - loss: 0.6072 - acc: 0.6676\n",
      "Epoch 23/30\n",
      "8000/8000 [==============================] - 0s 9us/sample - loss: 0.6122 - acc: 0.6551\n",
      "Epoch 24/30\n",
      "8000/8000 [==============================] - 0s 7us/sample - loss: 0.6129 - acc: 0.6576\n",
      "Epoch 25/30\n",
      "8000/8000 [==============================] - 0s 6us/sample - loss: 0.6091 - acc: 0.6607\n",
      "Epoch 26/30\n",
      "8000/8000 [==============================] - 0s 7us/sample - loss: 0.6115 - acc: 0.6559\n",
      "Epoch 27/30\n",
      "8000/8000 [==============================] - 0s 10us/sample - loss: 0.6083 - acc: 0.6619\n",
      "Epoch 28/30\n",
      "8000/8000 [==============================] - 0s 7us/sample - loss: 0.6098 - acc: 0.6553\n",
      "Epoch 29/30\n",
      "8000/8000 [==============================] - 0s 6us/sample - loss: 0.6128 - acc: 0.6530\n",
      "Epoch 30/30\n",
      "8000/8000 [==============================] - 0s 6us/sample - loss: 0.6103 - acc: 0.6607\n",
      "attack model no. 5\n",
      "Train on 8068 samples\n",
      "Epoch 1/30\n",
      "8068/8068 [==============================] - 1s 118us/sample - loss: 0.6987 - acc: 0.5093\n",
      "Epoch 2/30\n",
      "8068/8068 [==============================] - 0s 8us/sample - loss: 0.6747 - acc: 0.6065\n",
      "Epoch 3/30\n",
      "8068/8068 [==============================] - 0s 11us/sample - loss: 0.6575 - acc: 0.6415\n",
      "Epoch 4/30\n",
      "8068/8068 [==============================] - 0s 8us/sample - loss: 0.6389 - acc: 0.6679\n",
      "Epoch 5/30\n",
      "8068/8068 [==============================] - 0s 9us/sample - loss: 0.6287 - acc: 0.6522\n",
      "Epoch 6/30\n",
      "8068/8068 [==============================] - 0s 10us/sample - loss: 0.6174 - acc: 0.6675\n",
      "Epoch 7/30\n",
      "8068/8068 [==============================] - 0s 8us/sample - loss: 0.6116 - acc: 0.6687\n",
      "Epoch 8/30\n",
      "8068/8068 [==============================] - 0s 7us/sample - loss: 0.6093 - acc: 0.6660\n",
      "Epoch 9/30\n",
      "8068/8068 [==============================] - 0s 7us/sample - loss: 0.6041 - acc: 0.6713\n",
      "Epoch 10/30\n",
      "8068/8068 [==============================] - 0s 7us/sample - loss: 0.6035 - acc: 0.6729\n",
      "Epoch 11/30\n",
      "8068/8068 [==============================] - 0s 8us/sample - loss: 0.5995 - acc: 0.6756\n",
      "Epoch 12/30\n",
      "8068/8068 [==============================] - 0s 7us/sample - loss: 0.6021 - acc: 0.6715\n",
      "Epoch 13/30\n",
      "8068/8068 [==============================] - 0s 7us/sample - loss: 0.5974 - acc: 0.6774\n",
      "Epoch 14/30\n",
      "8068/8068 [==============================] - 0s 7us/sample - loss: 0.6006 - acc: 0.6735\n",
      "Epoch 15/30\n",
      "8068/8068 [==============================] - 0s 9us/sample - loss: 0.5996 - acc: 0.6744\n",
      "Epoch 16/30\n",
      "8068/8068 [==============================] - 0s 11us/sample - loss: 0.5962 - acc: 0.6766\n",
      "Epoch 17/30\n",
      "8068/8068 [==============================] - 0s 11us/sample - loss: 0.5970 - acc: 0.6732\n",
      "Epoch 18/30\n",
      "8068/8068 [==============================] - 0s 8us/sample - loss: 0.5922 - acc: 0.6805\n",
      "Epoch 19/30\n",
      "8068/8068 [==============================] - 0s 8us/sample - loss: 0.5945 - acc: 0.6785\n",
      "Epoch 20/30\n",
      "8068/8068 [==============================] - 0s 7us/sample - loss: 0.5908 - acc: 0.6806\n",
      "Epoch 21/30\n",
      "8068/8068 [==============================] - 0s 7us/sample - loss: 0.5889 - acc: 0.6827\n",
      "Epoch 22/30\n",
      "8068/8068 [==============================] - 0s 8us/sample - loss: 0.5937 - acc: 0.6766\n",
      "Epoch 23/30\n",
      "8068/8068 [==============================] - 0s 7us/sample - loss: 0.5928 - acc: 0.6808\n",
      "Epoch 24/30\n",
      "8068/8068 [==============================] - 0s 8us/sample - loss: 0.5926 - acc: 0.6796\n",
      "Epoch 25/30\n",
      "8068/8068 [==============================] - 0s 7us/sample - loss: 0.5908 - acc: 0.6851\n",
      "Epoch 26/30\n",
      "8068/8068 [==============================] - 0s 8us/sample - loss: 0.5860 - acc: 0.6847\n",
      "Epoch 27/30\n",
      "8068/8068 [==============================] - 0s 8us/sample - loss: 0.5895 - acc: 0.6832\n",
      "Epoch 28/30\n",
      "8068/8068 [==============================] - 0s 8us/sample - loss: 0.5905 - acc: 0.6807\n",
      "Epoch 29/30\n",
      "8068/8068 [==============================] - 0s 7us/sample - loss: 0.5853 - acc: 0.6858\n",
      "Epoch 30/30\n",
      "8068/8068 [==============================] - 0s 7us/sample - loss: 0.5859 - acc: 0.6900\n",
      "attack model no. 6\n",
      "Train on 7944 samples\n",
      "Epoch 1/30\n",
      "7944/7944 [==============================] - 1s 103us/sample - loss: 0.7023 - acc: 0.4889\n",
      "Epoch 2/30\n",
      "7944/7944 [==============================] - 0s 9us/sample - loss: 0.6833 - acc: 0.5585\n",
      "Epoch 3/30\n",
      "7944/7944 [==============================] - 0s 9us/sample - loss: 0.6693 - acc: 0.6173\n",
      "Epoch 4/30\n",
      "7944/7944 [==============================] - 0s 7us/sample - loss: 0.6586 - acc: 0.6313\n",
      "Epoch 5/30\n",
      "7944/7944 [==============================] - 0s 6us/sample - loss: 0.6477 - acc: 0.6392\n",
      "Epoch 6/30\n",
      "7944/7944 [==============================] - 0s 8us/sample - loss: 0.6405 - acc: 0.6453\n",
      "Epoch 7/30\n",
      "7944/7944 [==============================] - 0s 6us/sample - loss: 0.6301 - acc: 0.6550\n",
      "Epoch 8/30\n",
      "7944/7944 [==============================] - 0s 7us/sample - loss: 0.6321 - acc: 0.6470\n",
      "Epoch 9/30\n",
      "7944/7944 [==============================] - 0s 6us/sample - loss: 0.6259 - acc: 0.6522\n",
      "Epoch 10/30\n",
      "7944/7944 [==============================] - 0s 7us/sample - loss: 0.6267 - acc: 0.6485\n",
      "Epoch 11/30\n",
      "7944/7944 [==============================] - 0s 11us/sample - loss: 0.6247 - acc: 0.6499\n",
      "Epoch 12/30\n",
      "7944/7944 [==============================] - 0s 8us/sample - loss: 0.6241 - acc: 0.6487\n",
      "Epoch 13/30\n",
      "7944/7944 [==============================] - 0s 6us/sample - loss: 0.6251 - acc: 0.6372\n",
      "Epoch 14/30\n",
      "7944/7944 [==============================] - 0s 7us/sample - loss: 0.6175 - acc: 0.6478\n",
      "Epoch 15/30\n",
      "7944/7944 [==============================] - 0s 7us/sample - loss: 0.6169 - acc: 0.6547\n",
      "Epoch 16/30\n",
      "7944/7944 [==============================] - 0s 7us/sample - loss: 0.6137 - acc: 0.6577\n",
      "Epoch 17/30\n",
      "7944/7944 [==============================] - 0s 6us/sample - loss: 0.6120 - acc: 0.6574\n",
      "Epoch 18/30\n",
      "7944/7944 [==============================] - 0s 7us/sample - loss: 0.6133 - acc: 0.6575\n",
      "Epoch 19/30\n",
      "7944/7944 [==============================] - 0s 7us/sample - loss: 0.6096 - acc: 0.6619\n",
      "Epoch 20/30\n",
      "7944/7944 [==============================] - 0s 7us/sample - loss: 0.6114 - acc: 0.6565\n",
      "Epoch 21/30\n",
      "7944/7944 [==============================] - 0s 7us/sample - loss: 0.6121 - acc: 0.6567\n",
      "Epoch 22/30\n",
      "7944/7944 [==============================] - 0s 7us/sample - loss: 0.6104 - acc: 0.6590\n",
      "Epoch 23/30\n",
      "7944/7944 [==============================] - 0s 7us/sample - loss: 0.6097 - acc: 0.6608\n",
      "Epoch 24/30\n",
      "7944/7944 [==============================] - 0s 7us/sample - loss: 0.6071 - acc: 0.6672\n",
      "Epoch 25/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7944/7944 [==============================] - 0s 7us/sample - loss: 0.6146 - acc: 0.6542\n",
      "Epoch 26/30\n",
      "7944/7944 [==============================] - 0s 7us/sample - loss: 0.6070 - acc: 0.6610\n",
      "Epoch 27/30\n",
      "7944/7944 [==============================] - 0s 7us/sample - loss: 0.6059 - acc: 0.6594\n",
      "Epoch 28/30\n",
      "7944/7944 [==============================] - 0s 6us/sample - loss: 0.6086 - acc: 0.6591\n",
      "Epoch 29/30\n",
      "7944/7944 [==============================] - 0s 7us/sample - loss: 0.6054 - acc: 0.6640\n",
      "Epoch 30/30\n",
      "7944/7944 [==============================] - 0s 7us/sample - loss: 0.6046 - acc: 0.6623\n",
      "attack model no. 7\n",
      "Train on 8164 samples\n",
      "Epoch 1/30\n",
      "8164/8164 [==============================] - 1s 81us/sample - loss: 0.6829 - acc: 0.5747\n",
      "Epoch 2/30\n",
      "8164/8164 [==============================] - 0s 7us/sample - loss: 0.6650 - acc: 0.6244\n",
      "Epoch 3/30\n",
      "8164/8164 [==============================] - 0s 7us/sample - loss: 0.6493 - acc: 0.6572\n",
      "Epoch 4/30\n",
      "8164/8164 [==============================] - 0s 7us/sample - loss: 0.6366 - acc: 0.6639\n",
      "Epoch 5/30\n",
      "8164/8164 [==============================] - 0s 7us/sample - loss: 0.6308 - acc: 0.6614\n",
      "Epoch 6/30\n",
      "8164/8164 [==============================] - 0s 8us/sample - loss: 0.6251 - acc: 0.6640\n",
      "Epoch 7/30\n",
      "8164/8164 [==============================] - 0s 7us/sample - loss: 0.6156 - acc: 0.6671\n",
      "Epoch 8/30\n",
      "8164/8164 [==============================] - 0s 8us/sample - loss: 0.6132 - acc: 0.6712\n",
      "Epoch 9/30\n",
      "8164/8164 [==============================] - 0s 7us/sample - loss: 0.6065 - acc: 0.6662\n",
      "Epoch 10/30\n",
      "8164/8164 [==============================] - 0s 8us/sample - loss: 0.6064 - acc: 0.6614\n",
      "Epoch 11/30\n",
      "8164/8164 [==============================] - 0s 9us/sample - loss: 0.6050 - acc: 0.6676\n",
      "Epoch 12/30\n",
      "8164/8164 [==============================] - 0s 9us/sample - loss: 0.6033 - acc: 0.6714\n",
      "Epoch 13/30\n",
      "8164/8164 [==============================] - 0s 10us/sample - loss: 0.5992 - acc: 0.6785\n",
      "Epoch 14/30\n",
      "8164/8164 [==============================] - 0s 7us/sample - loss: 0.6007 - acc: 0.6707\n",
      "Epoch 15/30\n",
      "8164/8164 [==============================] - 0s 7us/sample - loss: 0.5944 - acc: 0.6763\n",
      "Epoch 16/30\n",
      "8164/8164 [==============================] - 0s 8us/sample - loss: 0.5931 - acc: 0.6794\n",
      "Epoch 17/30\n",
      "8164/8164 [==============================] - 0s 8us/sample - loss: 0.5968 - acc: 0.6733\n",
      "Epoch 18/30\n",
      "8164/8164 [==============================] - 0s 7us/sample - loss: 0.5943 - acc: 0.6830\n",
      "Epoch 19/30\n",
      "8164/8164 [==============================] - 0s 8us/sample - loss: 0.5935 - acc: 0.6817\n",
      "Epoch 20/30\n",
      "8164/8164 [==============================] - 0s 7us/sample - loss: 0.5988 - acc: 0.6776\n",
      "Epoch 21/30\n",
      "8164/8164 [==============================] - 0s 8us/sample - loss: 0.5948 - acc: 0.6812\n",
      "Epoch 22/30\n",
      "8164/8164 [==============================] - 0s 8us/sample - loss: 0.5936 - acc: 0.6825\n",
      "Epoch 23/30\n",
      "8164/8164 [==============================] - 0s 9us/sample - loss: 0.5935 - acc: 0.6777\n",
      "Epoch 24/30\n",
      "8164/8164 [==============================] - 0s 7us/sample - loss: 0.5920 - acc: 0.6842\n",
      "Epoch 25/30\n",
      "8164/8164 [==============================] - 0s 7us/sample - loss: 0.5920 - acc: 0.6807\n",
      "Epoch 26/30\n",
      "8164/8164 [==============================] - 0s 8us/sample - loss: 0.5909 - acc: 0.6879\n",
      "Epoch 27/30\n",
      "8164/8164 [==============================] - 0s 8us/sample - loss: 0.5930 - acc: 0.6797\n",
      "Epoch 28/30\n",
      "8164/8164 [==============================] - 0s 8us/sample - loss: 0.5951 - acc: 0.6818\n",
      "Epoch 29/30\n",
      "8164/8164 [==============================] - 0s 8us/sample - loss: 0.5929 - acc: 0.6790\n",
      "Epoch 30/30\n",
      "8164/8164 [==============================] - 0s 8us/sample - loss: 0.5923 - acc: 0.6819\n",
      "attack model no. 8\n",
      "Train on 7792 samples\n",
      "Epoch 1/30\n",
      "7792/7792 [==============================] - 1s 104us/sample - loss: 0.7021 - acc: 0.4888\n",
      "Epoch 2/30\n",
      "7792/7792 [==============================] - 0s 7us/sample - loss: 0.6875 - acc: 0.5445\n",
      "Epoch 3/30\n",
      "7792/7792 [==============================] - 0s 7us/sample - loss: 0.6798 - acc: 0.5842\n",
      "Epoch 4/30\n",
      "7792/7792 [==============================] - 0s 7us/sample - loss: 0.6723 - acc: 0.5959\n",
      "Epoch 5/30\n",
      "7792/7792 [==============================] - 0s 7us/sample - loss: 0.6684 - acc: 0.6001\n",
      "Epoch 6/30\n",
      "7792/7792 [==============================] - 0s 6us/sample - loss: 0.6639 - acc: 0.6063\n",
      "Epoch 7/30\n",
      "7792/7792 [==============================] - 0s 7us/sample - loss: 0.6601 - acc: 0.6115\n",
      "Epoch 8/30\n",
      "7792/7792 [==============================] - 0s 10us/sample - loss: 0.6606 - acc: 0.6007\n",
      "Epoch 9/30\n",
      "7792/7792 [==============================] - 0s 10us/sample - loss: 0.6572 - acc: 0.6020\n",
      "Epoch 10/30\n",
      "7792/7792 [==============================] - 0s 8us/sample - loss: 0.6535 - acc: 0.6100\n",
      "Epoch 11/30\n",
      "7792/7792 [==============================] - 0s 7us/sample - loss: 0.6539 - acc: 0.6093\n",
      "Epoch 12/30\n",
      "7792/7792 [==============================] - 0s 8us/sample - loss: 0.6523 - acc: 0.6141\n",
      "Epoch 13/30\n",
      "7792/7792 [==============================] - 0s 7us/sample - loss: 0.6505 - acc: 0.6050\n",
      "Epoch 14/30\n",
      "7792/7792 [==============================] - 0s 7us/sample - loss: 0.6508 - acc: 0.6109\n",
      "Epoch 15/30\n",
      "7792/7792 [==============================] - 0s 7us/sample - loss: 0.6487 - acc: 0.6106\n",
      "Epoch 16/30\n",
      "7792/7792 [==============================] - 0s 7us/sample - loss: 0.6443 - acc: 0.6159\n",
      "Epoch 17/30\n",
      "7792/7792 [==============================] - 0s 7us/sample - loss: 0.6470 - acc: 0.6124\n",
      "Epoch 18/30\n",
      "7792/7792 [==============================] - 0s 7us/sample - loss: 0.6455 - acc: 0.6119\n",
      "Epoch 19/30\n",
      "7792/7792 [==============================] - 0s 9us/sample - loss: 0.6482 - acc: 0.6118\n",
      "Epoch 20/30\n",
      "7792/7792 [==============================] - 0s 12us/sample - loss: 0.6466 - acc: 0.6118\n",
      "Epoch 21/30\n",
      "7792/7792 [==============================] - 0s 9us/sample - loss: 0.6438 - acc: 0.6137\n",
      "Epoch 22/30\n",
      "7792/7792 [==============================] - 0s 9us/sample - loss: 0.6426 - acc: 0.6158\n",
      "Epoch 23/30\n",
      "7792/7792 [==============================] - 0s 8us/sample - loss: 0.6425 - acc: 0.6128\n",
      "Epoch 24/30\n",
      "7792/7792 [==============================] - 0s 8us/sample - loss: 0.6417 - acc: 0.6201\n",
      "Epoch 25/30\n",
      "7792/7792 [==============================] - 0s 10us/sample - loss: 0.6424 - acc: 0.6159\n",
      "Epoch 26/30\n",
      "7792/7792 [==============================] - 0s 10us/sample - loss: 0.6455 - acc: 0.6108\n",
      "Epoch 27/30\n",
      "7792/7792 [==============================] - 0s 10us/sample - loss: 0.6420 - acc: 0.6154\n",
      "Epoch 28/30\n",
      "7792/7792 [==============================] - 0s 10us/sample - loss: 0.6435 - acc: 0.6064\n",
      "Epoch 29/30\n",
      "7792/7792 [==============================] - 0s 11us/sample - loss: 0.6441 - acc: 0.6128\n",
      "Epoch 30/30\n",
      "7792/7792 [==============================] - 0s 10us/sample - loss: 0.6437 - acc: 0.6154\n",
      "attack model no. 9\n",
      "Train on 8056 samples\n",
      "Epoch 1/30\n",
      "8056/8056 [==============================] - 1s 95us/sample - loss: 0.6940 - acc: 0.5078\n",
      "Epoch 2/30\n",
      "8056/8056 [==============================] - 0s 6us/sample - loss: 0.6693 - acc: 0.6146\n",
      "Epoch 3/30\n",
      "8056/8056 [==============================] - 0s 7us/sample - loss: 0.6501 - acc: 0.6594\n",
      "Epoch 4/30\n",
      "8056/8056 [==============================] - 0s 7us/sample - loss: 0.6344 - acc: 0.6684\n",
      "Epoch 5/30\n",
      "8056/8056 [==============================] - 0s 7us/sample - loss: 0.6233 - acc: 0.6732\n",
      "Epoch 6/30\n",
      "8056/8056 [==============================] - 0s 9us/sample - loss: 0.6083 - acc: 0.6779\n",
      "Epoch 7/30\n",
      "8056/8056 [==============================] - 0s 7us/sample - loss: 0.6084 - acc: 0.6766\n",
      "Epoch 8/30\n",
      "8056/8056 [==============================] - 0s 11us/sample - loss: 0.6013 - acc: 0.6753\n",
      "Epoch 9/30\n",
      "8056/8056 [==============================] - 0s 8us/sample - loss: 0.6013 - acc: 0.6755\n",
      "Epoch 10/30\n",
      "8056/8056 [==============================] - 0s 9us/sample - loss: 0.5996 - acc: 0.6703\n",
      "Epoch 11/30\n",
      "8056/8056 [==============================] - 0s 7us/sample - loss: 0.5926 - acc: 0.6795\n",
      "Epoch 12/30\n",
      "8056/8056 [==============================] - 0s 8us/sample - loss: 0.5969 - acc: 0.6701\n",
      "Epoch 13/30\n",
      "8056/8056 [==============================] - 0s 8us/sample - loss: 0.5893 - acc: 0.6773\n",
      "Epoch 14/30\n",
      "8056/8056 [==============================] - 0s 8us/sample - loss: 0.5873 - acc: 0.6807\n",
      "Epoch 15/30\n",
      "8056/8056 [==============================] - 0s 8us/sample - loss: 0.5845 - acc: 0.6795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/30\n",
      "8056/8056 [==============================] - 0s 8us/sample - loss: 0.5904 - acc: 0.6723\n",
      "Epoch 17/30\n",
      "8056/8056 [==============================] - 0s 8us/sample - loss: 0.5867 - acc: 0.6756\n",
      "Epoch 18/30\n",
      "8056/8056 [==============================] - 0s 7us/sample - loss: 0.5821 - acc: 0.6889\n",
      "Epoch 19/30\n",
      "8056/8056 [==============================] - 0s 7us/sample - loss: 0.5854 - acc: 0.6866\n",
      "Epoch 20/30\n",
      "8056/8056 [==============================] - 0s 9us/sample - loss: 0.5882 - acc: 0.6810\n",
      "Epoch 21/30\n",
      "8056/8056 [==============================] - 0s 8us/sample - loss: 0.5845 - acc: 0.6753\n",
      "Epoch 22/30\n",
      "8056/8056 [==============================] - 0s 8us/sample - loss: 0.5815 - acc: 0.6841\n",
      "Epoch 23/30\n",
      "8056/8056 [==============================] - 0s 8us/sample - loss: 0.5840 - acc: 0.6841\n",
      "Epoch 24/30\n",
      "8056/8056 [==============================] - 0s 8us/sample - loss: 0.5818 - acc: 0.6888\n",
      "Epoch 25/30\n",
      "8056/8056 [==============================] - 0s 8us/sample - loss: 0.5808 - acc: 0.6828\n",
      "Epoch 26/30\n",
      "8056/8056 [==============================] - 0s 8us/sample - loss: 0.5839 - acc: 0.6872\n",
      "Epoch 27/30\n",
      "8056/8056 [==============================] - 0s 8us/sample - loss: 0.5847 - acc: 0.6785\n",
      "Epoch 28/30\n",
      "8056/8056 [==============================] - 0s 8us/sample - loss: 0.5788 - acc: 0.6866\n",
      "Epoch 29/30\n",
      "8056/8056 [==============================] - 0s 8us/sample - loss: 0.5799 - acc: 0.6883\n",
      "Epoch 30/30\n",
      "8056/8056 [==============================] - 0s 8us/sample - loss: 0.5822 - acc: 0.6851\n",
      "attack model no. 0\n",
      "Train on 7912 samples\n",
      "Epoch 1/30\n",
      "7912/7912 [==============================] - 1s 111us/sample - loss: 0.6848 - acc: 0.5658\n",
      "Epoch 2/30\n",
      "7912/7912 [==============================] - 0s 7us/sample - loss: 0.6707 - acc: 0.6311\n",
      "Epoch 3/30\n",
      "7912/7912 [==============================] - 0s 7us/sample - loss: 0.6564 - acc: 0.6514\n",
      "Epoch 4/30\n",
      "7912/7912 [==============================] - 0s 8us/sample - loss: 0.6463 - acc: 0.6560\n",
      "Epoch 5/30\n",
      "7912/7912 [==============================] - 0s 8us/sample - loss: 0.6381 - acc: 0.6546\n",
      "Epoch 6/30\n",
      "7912/7912 [==============================] - 0s 8us/sample - loss: 0.6236 - acc: 0.6668\n",
      "Epoch 7/30\n",
      "7912/7912 [==============================] - 0s 7us/sample - loss: 0.6244 - acc: 0.6532\n",
      "Epoch 8/30\n",
      "7912/7912 [==============================] - 0s 8us/sample - loss: 0.6208 - acc: 0.6550\n",
      "Epoch 9/30\n",
      "7912/7912 [==============================] - 0s 7us/sample - loss: 0.6162 - acc: 0.6587\n",
      "Epoch 10/30\n",
      "7912/7912 [==============================] - 0s 8us/sample - loss: 0.6120 - acc: 0.6550\n",
      "Epoch 11/30\n",
      "7912/7912 [==============================] - 0s 11us/sample - loss: 0.6064 - acc: 0.6639\n",
      "Epoch 12/30\n",
      "7912/7912 [==============================] - 0s 10us/sample - loss: 0.6024 - acc: 0.6580\n",
      "Epoch 13/30\n",
      "7912/7912 [==============================] - 0s 8us/sample - loss: 0.6013 - acc: 0.6665\n",
      "Epoch 14/30\n",
      "7912/7912 [==============================] - 0s 8us/sample - loss: 0.5993 - acc: 0.6589\n",
      "Epoch 15/30\n",
      "7912/7912 [==============================] - 0s 7us/sample - loss: 0.6039 - acc: 0.6574\n",
      "Epoch 16/30\n",
      "7912/7912 [==============================] - 0s 7us/sample - loss: 0.5945 - acc: 0.6670\n",
      "Epoch 17/30\n",
      "7912/7912 [==============================] - 0s 8us/sample - loss: 0.5919 - acc: 0.6678\n",
      "Epoch 18/30\n",
      "7912/7912 [==============================] - 0s 9us/sample - loss: 0.5976 - acc: 0.6644\n",
      "Epoch 19/30\n",
      "7912/7912 [==============================] - 0s 11us/sample - loss: 0.5967 - acc: 0.6624\n",
      "Epoch 20/30\n",
      "7912/7912 [==============================] - 0s 8us/sample - loss: 0.5972 - acc: 0.6708\n",
      "Epoch 21/30\n",
      "7912/7912 [==============================] - 0s 7us/sample - loss: 0.5948 - acc: 0.6658\n",
      "Epoch 22/30\n",
      "7912/7912 [==============================] - 0s 7us/sample - loss: 0.5925 - acc: 0.6714\n",
      "Epoch 23/30\n",
      "7912/7912 [==============================] - 0s 8us/sample - loss: 0.5914 - acc: 0.6704\n",
      "Epoch 24/30\n",
      "7912/7912 [==============================] - 0s 8us/sample - loss: 0.5922 - acc: 0.6715\n",
      "Epoch 25/30\n",
      "7912/7912 [==============================] - 0s 8us/sample - loss: 0.5912 - acc: 0.6704\n",
      "Epoch 26/30\n",
      "7912/7912 [==============================] - 0s 8us/sample - loss: 0.5887 - acc: 0.6726\n",
      "Epoch 27/30\n",
      "7912/7912 [==============================] - 0s 8us/sample - loss: 0.5900 - acc: 0.6695\n",
      "Epoch 28/30\n",
      "7912/7912 [==============================] - 0s 7us/sample - loss: 0.5900 - acc: 0.6708\n",
      "Epoch 29/30\n",
      "7912/7912 [==============================] - 0s 7us/sample - loss: 0.5947 - acc: 0.6704\n",
      "Epoch 30/30\n",
      "7912/7912 [==============================] - 0s 8us/sample - loss: 0.5905 - acc: 0.6682\n",
      "attack model no. 1\n",
      "Train on 8196 samples\n",
      "Epoch 1/30\n",
      "8196/8196 [==============================] - 1s 98us/sample - loss: 0.6717 - acc: 0.6058\n",
      "Epoch 2/30\n",
      "8196/8196 [==============================] - 0s 8us/sample - loss: 0.6456 - acc: 0.6725\n",
      "Epoch 3/30\n",
      "8196/8196 [==============================] - 0s 9us/sample - loss: 0.6260 - acc: 0.6902\n",
      "Epoch 4/30\n",
      "8196/8196 [==============================] - 0s 9us/sample - loss: 0.6120 - acc: 0.6953\n",
      "Epoch 5/30\n",
      "8196/8196 [==============================] - 0s 9us/sample - loss: 0.6087 - acc: 0.6898\n",
      "Epoch 6/30\n",
      "8196/8196 [==============================] - 0s 8us/sample - loss: 0.5928 - acc: 0.6983\n",
      "Epoch 7/30\n",
      "8196/8196 [==============================] - 0s 9us/sample - loss: 0.5897 - acc: 0.6942\n",
      "Epoch 8/30\n",
      "8196/8196 [==============================] - 0s 8us/sample - loss: 0.5773 - acc: 0.7040\n",
      "Epoch 9/30\n",
      "8196/8196 [==============================] - 0s 8us/sample - loss: 0.5806 - acc: 0.6844\n",
      "Epoch 10/30\n",
      "8196/8196 [==============================] - 0s 10us/sample - loss: 0.5717 - acc: 0.6944\n",
      "Epoch 11/30\n",
      "8196/8196 [==============================] - 0s 10us/sample - loss: 0.5592 - acc: 0.7029\n",
      "Epoch 12/30\n",
      "8196/8196 [==============================] - 0s 9us/sample - loss: 0.5597 - acc: 0.6955\n",
      "Epoch 13/30\n",
      "8196/8196 [==============================] - 0s 10us/sample - loss: 0.5557 - acc: 0.7010\n",
      "Epoch 14/30\n",
      "8196/8196 [==============================] - 0s 9us/sample - loss: 0.5523 - acc: 0.7047\n",
      "Epoch 15/30\n",
      "8196/8196 [==============================] - 0s 9us/sample - loss: 0.5517 - acc: 0.7045\n",
      "Epoch 16/30\n",
      "8196/8196 [==============================] - 0s 8us/sample - loss: 0.5505 - acc: 0.7030\n",
      "Epoch 17/30\n",
      "8196/8196 [==============================] - 0s 8us/sample - loss: 0.5509 - acc: 0.7083\n",
      "Epoch 18/30\n",
      "8196/8196 [==============================] - 0s 8us/sample - loss: 0.5436 - acc: 0.7139\n",
      "Epoch 19/30\n",
      "8196/8196 [==============================] - 0s 8us/sample - loss: 0.5478 - acc: 0.7101\n",
      "Epoch 20/30\n",
      "8196/8196 [==============================] - 0s 9us/sample - loss: 0.5503 - acc: 0.7083\n",
      "Epoch 21/30\n",
      "8196/8196 [==============================] - 0s 8us/sample - loss: 0.5466 - acc: 0.7112\n",
      "Epoch 22/30\n",
      "8196/8196 [==============================] - 0s 8us/sample - loss: 0.5442 - acc: 0.7150\n",
      "Epoch 23/30\n",
      "8196/8196 [==============================] - 0s 8us/sample - loss: 0.5440 - acc: 0.7169\n",
      "Epoch 24/30\n",
      "8196/8196 [==============================] - 0s 9us/sample - loss: 0.5434 - acc: 0.7145\n",
      "Epoch 25/30\n",
      "8196/8196 [==============================] - 0s 8us/sample - loss: 0.5454 - acc: 0.7149\n",
      "Epoch 26/30\n",
      "8196/8196 [==============================] - 0s 8us/sample - loss: 0.5440 - acc: 0.7156\n",
      "Epoch 27/30\n",
      "8196/8196 [==============================] - 0s 8us/sample - loss: 0.5432 - acc: 0.7153\n",
      "Epoch 28/30\n",
      "8196/8196 [==============================] - 0s 7us/sample - loss: 0.5493 - acc: 0.7083\n",
      "Epoch 29/30\n",
      "8196/8196 [==============================] - 0s 8us/sample - loss: 0.5451 - acc: 0.7125\n",
      "Epoch 30/30\n",
      "8196/8196 [==============================] - 0s 8us/sample - loss: 0.5413 - acc: 0.7136\n",
      "attack model no. 2\n",
      "Train on 7900 samples\n",
      "Epoch 1/30\n",
      "7900/7900 [==============================] - 1s 94us/sample - loss: 0.6847 - acc: 0.5487\n",
      "Epoch 2/30\n",
      "7900/7900 [==============================] - 0s 7us/sample - loss: 0.6577 - acc: 0.6647\n",
      "Epoch 3/30\n",
      "7900/7900 [==============================] - 0s 7us/sample - loss: 0.6316 - acc: 0.6928\n",
      "Epoch 4/30\n",
      "7900/7900 [==============================] - 0s 8us/sample - loss: 0.6108 - acc: 0.7001\n",
      "Epoch 5/30\n",
      "7900/7900 [==============================] - 0s 8us/sample - loss: 0.5929 - acc: 0.7024\n",
      "Epoch 6/30\n",
      "7900/7900 [==============================] - 0s 8us/sample - loss: 0.5758 - acc: 0.7120\n",
      "Epoch 7/30\n",
      "7900/7900 [==============================] - 0s 8us/sample - loss: 0.5724 - acc: 0.7089\n",
      "Epoch 8/30\n",
      "7900/7900 [==============================] - 0s 8us/sample - loss: 0.5719 - acc: 0.7108\n",
      "Epoch 9/30\n",
      "7900/7900 [==============================] - 0s 7us/sample - loss: 0.5597 - acc: 0.7152\n",
      "Epoch 10/30\n",
      "7900/7900 [==============================] - 0s 8us/sample - loss: 0.5555 - acc: 0.7156\n",
      "Epoch 11/30\n",
      "7900/7900 [==============================] - 0s 8us/sample - loss: 0.5570 - acc: 0.7124\n",
      "Epoch 12/30\n",
      "7900/7900 [==============================] - 0s 7us/sample - loss: 0.5556 - acc: 0.7077\n",
      "Epoch 13/30\n",
      "7900/7900 [==============================] - 0s 7us/sample - loss: 0.5567 - acc: 0.7061\n",
      "Epoch 14/30\n",
      "7900/7900 [==============================] - 0s 8us/sample - loss: 0.5540 - acc: 0.7081\n",
      "Epoch 15/30\n",
      "7900/7900 [==============================] - 0s 8us/sample - loss: 0.5485 - acc: 0.7156\n",
      "Epoch 16/30\n",
      "7900/7900 [==============================] - 0s 8us/sample - loss: 0.5459 - acc: 0.7159\n",
      "Epoch 17/30\n",
      "7900/7900 [==============================] - 0s 8us/sample - loss: 0.5475 - acc: 0.7132\n",
      "Epoch 18/30\n",
      "7900/7900 [==============================] - 0s 8us/sample - loss: 0.5478 - acc: 0.7159\n",
      "Epoch 19/30\n",
      "7900/7900 [==============================] - 0s 9us/sample - loss: 0.5447 - acc: 0.7114\n",
      "Epoch 20/30\n",
      "7900/7900 [==============================] - 0s 7us/sample - loss: 0.5546 - acc: 0.7127\n",
      "Epoch 21/30\n",
      "7900/7900 [==============================] - 0s 8us/sample - loss: 0.5445 - acc: 0.7165\n",
      "Epoch 22/30\n",
      "7900/7900 [==============================] - 0s 8us/sample - loss: 0.5432 - acc: 0.7191\n",
      "Epoch 23/30\n",
      "7900/7900 [==============================] - 0s 11us/sample - loss: 0.5392 - acc: 0.7230\n",
      "Epoch 24/30\n",
      "7900/7900 [==============================] - 0s 8us/sample - loss: 0.5406 - acc: 0.7218\n",
      "Epoch 25/30\n",
      "7900/7900 [==============================] - 0s 7us/sample - loss: 0.5424 - acc: 0.7197\n",
      "Epoch 26/30\n",
      "7900/7900 [==============================] - 0s 7us/sample - loss: 0.5393 - acc: 0.7241\n",
      "Epoch 27/30\n",
      "7900/7900 [==============================] - 0s 8us/sample - loss: 0.5394 - acc: 0.7200\n",
      "Epoch 28/30\n",
      "7900/7900 [==============================] - 0s 8us/sample - loss: 0.5408 - acc: 0.7216\n",
      "Epoch 29/30\n",
      "7900/7900 [==============================] - 0s 7us/sample - loss: 0.5421 - acc: 0.7189\n",
      "Epoch 30/30\n",
      "7900/7900 [==============================] - 0s 7us/sample - loss: 0.5453 - acc: 0.7162\n",
      "attack model no. 3\n",
      "Train on 7968 samples\n",
      "Epoch 1/30\n",
      "7968/7968 [==============================] - 1s 99us/sample - loss: 0.6869 - acc: 0.5481\n",
      "Epoch 2/30\n",
      "7968/7968 [==============================] - 0s 7us/sample - loss: 0.6586 - acc: 0.6749\n",
      "Epoch 3/30\n",
      "7968/7968 [==============================] - 0s 8us/sample - loss: 0.6336 - acc: 0.7062\n",
      "Epoch 4/30\n",
      "7968/7968 [==============================] - 0s 8us/sample - loss: 0.6081 - acc: 0.7171\n",
      "Epoch 5/30\n",
      "7968/7968 [==============================] - 0s 7us/sample - loss: 0.5847 - acc: 0.7184\n",
      "Epoch 6/30\n",
      "7968/7968 [==============================] - 0s 8us/sample - loss: 0.5667 - acc: 0.7200\n",
      "Epoch 7/30\n",
      "7968/7968 [==============================] - 0s 9us/sample - loss: 0.5602 - acc: 0.7150\n",
      "Epoch 8/30\n",
      "7968/7968 [==============================] - 0s 8us/sample - loss: 0.5430 - acc: 0.7233\n",
      "Epoch 9/30\n",
      "7968/7968 [==============================] - 0s 9us/sample - loss: 0.5454 - acc: 0.7263\n",
      "Epoch 10/30\n",
      "7968/7968 [==============================] - 0s 11us/sample - loss: 0.5402 - acc: 0.7252\n",
      "Epoch 11/30\n",
      "7968/7968 [==============================] - 0s 8us/sample - loss: 0.5393 - acc: 0.7211\n",
      "Epoch 12/30\n",
      "7968/7968 [==============================] - 0s 8us/sample - loss: 0.5373 - acc: 0.7249\n",
      "Epoch 13/30\n",
      "7968/7968 [==============================] - 0s 7us/sample - loss: 0.5364 - acc: 0.7236\n",
      "Epoch 14/30\n",
      "7968/7968 [==============================] - 0s 10us/sample - loss: 0.5369 - acc: 0.7206\n",
      "Epoch 15/30\n",
      "7968/7968 [==============================] - 0s 7us/sample - loss: 0.5270 - acc: 0.7302\n",
      "Epoch 16/30\n",
      "7968/7968 [==============================] - 0s 7us/sample - loss: 0.5287 - acc: 0.7289\n",
      "Epoch 17/30\n",
      "7968/7968 [==============================] - 0s 8us/sample - loss: 0.5328 - acc: 0.7236\n",
      "Epoch 18/30\n",
      "7968/7968 [==============================] - 0s 8us/sample - loss: 0.5309 - acc: 0.7245\n",
      "Epoch 19/30\n",
      "7968/7968 [==============================] - 0s 9us/sample - loss: 0.5316 - acc: 0.7258\n",
      "Epoch 20/30\n",
      "7968/7968 [==============================] - 0s 10us/sample - loss: 0.5288 - acc: 0.7307\n",
      "Epoch 21/30\n",
      "7968/7968 [==============================] - 0s 9us/sample - loss: 0.5266 - acc: 0.7289\n",
      "Epoch 22/30\n",
      "7968/7968 [==============================] - 0s 8us/sample - loss: 0.5229 - acc: 0.7362\n",
      "Epoch 23/30\n",
      "7968/7968 [==============================] - 0s 8us/sample - loss: 0.5273 - acc: 0.7326\n",
      "Epoch 24/30\n",
      "7968/7968 [==============================] - 0s 8us/sample - loss: 0.5225 - acc: 0.7279\n",
      "Epoch 25/30\n",
      "7968/7968 [==============================] - 0s 8us/sample - loss: 0.5289 - acc: 0.7326\n",
      "Epoch 26/30\n",
      "7968/7968 [==============================] - 0s 10us/sample - loss: 0.5228 - acc: 0.7344\n",
      "Epoch 27/30\n",
      "7968/7968 [==============================] - 0s 9us/sample - loss: 0.5186 - acc: 0.7316\n",
      "Epoch 28/30\n",
      "7968/7968 [==============================] - 0s 8us/sample - loss: 0.5304 - acc: 0.7270\n",
      "Epoch 29/30\n",
      "7968/7968 [==============================] - 0s 8us/sample - loss: 0.5173 - acc: 0.7356\n",
      "Epoch 30/30\n",
      "7968/7968 [==============================] - 0s 8us/sample - loss: 0.5199 - acc: 0.7374\n",
      "attack model no. 4\n",
      "Train on 8000 samples\n",
      "Epoch 1/30\n",
      "8000/8000 [==============================] - 1s 97us/sample - loss: 0.6907 - acc: 0.5264\n",
      "Epoch 2/30\n",
      "8000/8000 [==============================] - 0s 7us/sample - loss: 0.6629 - acc: 0.6314\n",
      "Epoch 3/30\n",
      "8000/8000 [==============================] - 0s 7us/sample - loss: 0.6413 - acc: 0.6809\n",
      "Epoch 4/30\n",
      "8000/8000 [==============================] - 0s 7us/sample - loss: 0.6176 - acc: 0.6944\n",
      "Epoch 5/30\n",
      "8000/8000 [==============================] - 0s 8us/sample - loss: 0.6027 - acc: 0.6944\n",
      "Epoch 6/30\n",
      "8000/8000 [==============================] - 0s 8us/sample - loss: 0.5963 - acc: 0.6934\n",
      "Epoch 7/30\n",
      "8000/8000 [==============================] - 0s 7us/sample - loss: 0.5863 - acc: 0.6951\n",
      "Epoch 8/30\n",
      "8000/8000 [==============================] - 0s 7us/sample - loss: 0.5855 - acc: 0.6894\n",
      "Epoch 9/30\n",
      "8000/8000 [==============================] - 0s 7us/sample - loss: 0.5796 - acc: 0.6895\n",
      "Epoch 10/30\n",
      "8000/8000 [==============================] - 0s 7us/sample - loss: 0.5689 - acc: 0.6989\n",
      "Epoch 11/30\n",
      "8000/8000 [==============================] - 0s 8us/sample - loss: 0.5657 - acc: 0.7024\n",
      "Epoch 12/30\n",
      "8000/8000 [==============================] - 0s 8us/sample - loss: 0.5655 - acc: 0.7030\n",
      "Epoch 13/30\n",
      "8000/8000 [==============================] - 0s 8us/sample - loss: 0.5671 - acc: 0.7004\n",
      "Epoch 14/30\n",
      "8000/8000 [==============================] - 0s 8us/sample - loss: 0.5588 - acc: 0.7063\n",
      "Epoch 15/30\n",
      "8000/8000 [==============================] - 0s 7us/sample - loss: 0.5577 - acc: 0.7081\n",
      "Epoch 16/30\n",
      "8000/8000 [==============================] - 0s 7us/sample - loss: 0.5608 - acc: 0.7086\n",
      "Epoch 17/30\n",
      "8000/8000 [==============================] - 0s 7us/sample - loss: 0.5548 - acc: 0.7063\n",
      "Epoch 18/30\n",
      "8000/8000 [==============================] - 0s 8us/sample - loss: 0.5583 - acc: 0.7051\n",
      "Epoch 19/30\n",
      "8000/8000 [==============================] - 0s 8us/sample - loss: 0.5545 - acc: 0.7125\n",
      "Epoch 20/30\n",
      "8000/8000 [==============================] - 0s 8us/sample - loss: 0.5539 - acc: 0.7115\n",
      "Epoch 21/30\n",
      "8000/8000 [==============================] - 0s 9us/sample - loss: 0.5568 - acc: 0.7032\n",
      "Epoch 22/30\n",
      "8000/8000 [==============================] - 0s 7us/sample - loss: 0.5539 - acc: 0.7110\n",
      "Epoch 23/30\n",
      "8000/8000 [==============================] - 0s 7us/sample - loss: 0.5574 - acc: 0.7079\n",
      "Epoch 24/30\n",
      "8000/8000 [==============================] - 0s 7us/sample - loss: 0.5505 - acc: 0.7168\n",
      "Epoch 25/30\n",
      "8000/8000 [==============================] - 0s 7us/sample - loss: 0.5584 - acc: 0.7060\n",
      "Epoch 26/30\n",
      "8000/8000 [==============================] - 0s 8us/sample - loss: 0.5520 - acc: 0.7124\n",
      "Epoch 27/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 7us/sample - loss: 0.5551 - acc: 0.7130\n",
      "Epoch 28/30\n",
      "8000/8000 [==============================] - 0s 7us/sample - loss: 0.5563 - acc: 0.7089\n",
      "Epoch 29/30\n",
      "8000/8000 [==============================] - 0s 7us/sample - loss: 0.5532 - acc: 0.7135\n",
      "Epoch 30/30\n",
      "8000/8000 [==============================] - 0s 7us/sample - loss: 0.5518 - acc: 0.7163\n",
      "attack model no. 5\n",
      "Train on 8068 samples\n",
      "Epoch 1/30\n",
      "8068/8068 [==============================] - 1s 101us/sample - loss: 0.6772 - acc: 0.5713\n",
      "Epoch 2/30\n",
      "8068/8068 [==============================] - 0s 7us/sample - loss: 0.6530 - acc: 0.6605\n",
      "Epoch 3/30\n",
      "8068/8068 [==============================] - 0s 8us/sample - loss: 0.6330 - acc: 0.6962\n",
      "Epoch 4/30\n",
      "8068/8068 [==============================] - 0s 9us/sample - loss: 0.6189 - acc: 0.6930\n",
      "Epoch 5/30\n",
      "8068/8068 [==============================] - 0s 9us/sample - loss: 0.6005 - acc: 0.6994\n",
      "Epoch 6/30\n",
      "8068/8068 [==============================] - 0s 8us/sample - loss: 0.5935 - acc: 0.6956\n",
      "Epoch 7/30\n",
      "8068/8068 [==============================] - 0s 8us/sample - loss: 0.5894 - acc: 0.6957\n",
      "Epoch 8/30\n",
      "8068/8068 [==============================] - 0s 9us/sample - loss: 0.5830 - acc: 0.6970\n",
      "Epoch 9/30\n",
      "8068/8068 [==============================] - 0s 8us/sample - loss: 0.5759 - acc: 0.7007\n",
      "Epoch 10/30\n",
      "8068/8068 [==============================] - 0s 9us/sample - loss: 0.5739 - acc: 0.6962\n",
      "Epoch 11/30\n",
      "8068/8068 [==============================] - 0s 10us/sample - loss: 0.5696 - acc: 0.6992\n",
      "Epoch 12/30\n",
      "8068/8068 [==============================] - 0s 9us/sample - loss: 0.5702 - acc: 0.6955\n",
      "Epoch 13/30\n",
      "8068/8068 [==============================] - 0s 8us/sample - loss: 0.5639 - acc: 0.7055\n",
      "Epoch 14/30\n",
      "8068/8068 [==============================] - 0s 9us/sample - loss: 0.5612 - acc: 0.7022\n",
      "Epoch 15/30\n",
      "8068/8068 [==============================] - 0s 8us/sample - loss: 0.5576 - acc: 0.7018\n",
      "Epoch 16/30\n",
      "8068/8068 [==============================] - 0s 10us/sample - loss: 0.5559 - acc: 0.7100\n",
      "Epoch 17/30\n",
      "8068/8068 [==============================] - 0s 12us/sample - loss: 0.5576 - acc: 0.7049\n",
      "Epoch 18/30\n",
      "8068/8068 [==============================] - 0s 8us/sample - loss: 0.5541 - acc: 0.7061\n",
      "Epoch 19/30\n",
      "8068/8068 [==============================] - 0s 8us/sample - loss: 0.5560 - acc: 0.7084\n",
      "Epoch 20/30\n",
      "8068/8068 [==============================] - 0s 8us/sample - loss: 0.5536 - acc: 0.7082\n",
      "Epoch 21/30\n",
      "8068/8068 [==============================] - 0s 9us/sample - loss: 0.5552 - acc: 0.7036\n",
      "Epoch 22/30\n",
      "8068/8068 [==============================] - 0s 9us/sample - loss: 0.5570 - acc: 0.7085\n",
      "Epoch 23/30\n",
      "8068/8068 [==============================] - 0s 9us/sample - loss: 0.5543 - acc: 0.7088\n",
      "Epoch 24/30\n",
      "8068/8068 [==============================] - 0s 10us/sample - loss: 0.5529 - acc: 0.7112\n",
      "Epoch 25/30\n",
      "8068/8068 [==============================] - 0s 8us/sample - loss: 0.5468 - acc: 0.7194\n",
      "Epoch 26/30\n",
      "8068/8068 [==============================] - 0s 8us/sample - loss: 0.5529 - acc: 0.7085\n",
      "Epoch 27/30\n",
      "8068/8068 [==============================] - 0s 9us/sample - loss: 0.5532 - acc: 0.7060\n",
      "Epoch 28/30\n",
      "8068/8068 [==============================] - 0s 8us/sample - loss: 0.5507 - acc: 0.7170\n",
      "Epoch 29/30\n",
      "8068/8068 [==============================] - 0s 9us/sample - loss: 0.5443 - acc: 0.7133\n",
      "Epoch 30/30\n",
      "8068/8068 [==============================] - 0s 9us/sample - loss: 0.5508 - acc: 0.7144\n",
      "attack model no. 6\n",
      "Train on 7944 samples\n",
      "Epoch 1/30\n",
      "7944/7944 [==============================] - 1s 122us/sample - loss: 0.6763 - acc: 0.6033\n",
      "Epoch 2/30\n",
      "7944/7944 [==============================] - 0s 9us/sample - loss: 0.6581 - acc: 0.6568\n",
      "Epoch 3/30\n",
      "7944/7944 [==============================] - 0s 8us/sample - loss: 0.6428 - acc: 0.6688\n",
      "Epoch 4/30\n",
      "7944/7944 [==============================] - 0s 9us/sample - loss: 0.6252 - acc: 0.6718\n",
      "Epoch 5/30\n",
      "7944/7944 [==============================] - 0s 8us/sample - loss: 0.6147 - acc: 0.6754\n",
      "Epoch 6/30\n",
      "7944/7944 [==============================] - 0s 8us/sample - loss: 0.6076 - acc: 0.6730\n",
      "Epoch 7/30\n",
      "7944/7944 [==============================] - 0s 8us/sample - loss: 0.6106 - acc: 0.6686\n",
      "Epoch 8/30\n",
      "7944/7944 [==============================] - 0s 7us/sample - loss: 0.5995 - acc: 0.6751\n",
      "Epoch 9/30\n",
      "7944/7944 [==============================] - 0s 10us/sample - loss: 0.5934 - acc: 0.6827\n",
      "Epoch 10/30\n",
      "7944/7944 [==============================] - 0s 8us/sample - loss: 0.5897 - acc: 0.6789\n",
      "Epoch 11/30\n",
      "7944/7944 [==============================] - 0s 8us/sample - loss: 0.5911 - acc: 0.6775\n",
      "Epoch 12/30\n",
      "7944/7944 [==============================] - 0s 8us/sample - loss: 0.5809 - acc: 0.6854\n",
      "Epoch 13/30\n",
      "7944/7944 [==============================] - 0s 8us/sample - loss: 0.5874 - acc: 0.6755\n",
      "Epoch 14/30\n",
      "7944/7944 [==============================] - 0s 7us/sample - loss: 0.5886 - acc: 0.6806\n",
      "Epoch 15/30\n",
      "7944/7944 [==============================] - 0s 7us/sample - loss: 0.5842 - acc: 0.6823\n",
      "Epoch 16/30\n",
      "7944/7944 [==============================] - 0s 9us/sample - loss: 0.5831 - acc: 0.6848\n",
      "Epoch 17/30\n",
      "7944/7944 [==============================] - 0s 8us/sample - loss: 0.5813 - acc: 0.6845\n",
      "Epoch 18/30\n",
      "7944/7944 [==============================] - 0s 7us/sample - loss: 0.5840 - acc: 0.6847\n",
      "Epoch 19/30\n",
      "7944/7944 [==============================] - 0s 7us/sample - loss: 0.5770 - acc: 0.6917\n",
      "Epoch 20/30\n",
      "7944/7944 [==============================] - 0s 7us/sample - loss: 0.5778 - acc: 0.6866\n",
      "Epoch 21/30\n",
      "7944/7944 [==============================] - 0s 7us/sample - loss: 0.5731 - acc: 0.6931\n",
      "Epoch 22/30\n",
      "7944/7944 [==============================] - 0s 7us/sample - loss: 0.5790 - acc: 0.6888\n",
      "Epoch 23/30\n",
      "7944/7944 [==============================] - 0s 8us/sample - loss: 0.5772 - acc: 0.6907\n",
      "Epoch 24/30\n",
      "7944/7944 [==============================] - 0s 8us/sample - loss: 0.5802 - acc: 0.6864\n",
      "Epoch 25/30\n",
      "7944/7944 [==============================] - 0s 8us/sample - loss: 0.5774 - acc: 0.6893\n",
      "Epoch 26/30\n",
      "7944/7944 [==============================] - 0s 9us/sample - loss: 0.5783 - acc: 0.6828\n",
      "Epoch 27/30\n",
      "7944/7944 [==============================] - 0s 8us/sample - loss: 0.5773 - acc: 0.6829\n",
      "Epoch 28/30\n",
      "7944/7944 [==============================] - 0s 8us/sample - loss: 0.5766 - acc: 0.6866\n",
      "Epoch 29/30\n",
      "7944/7944 [==============================] - 0s 8us/sample - loss: 0.5804 - acc: 0.6845\n",
      "Epoch 30/30\n",
      "7944/7944 [==============================] - 0s 8us/sample - loss: 0.5767 - acc: 0.6844\n",
      "attack model no. 7\n",
      "Train on 8164 samples\n",
      "Epoch 1/30\n",
      "8164/8164 [==============================] - 1s 104us/sample - loss: 0.6884 - acc: 0.5420\n",
      "Epoch 2/30\n",
      "8164/8164 [==============================] - 0s 10us/sample - loss: 0.6637 - acc: 0.6521\n",
      "Epoch 3/30\n",
      "8164/8164 [==============================] - 0s 9us/sample - loss: 0.6361 - acc: 0.6894\n",
      "Epoch 4/30\n",
      "8164/8164 [==============================] - 0s 9us/sample - loss: 0.6142 - acc: 0.6924\n",
      "Epoch 5/30\n",
      "8164/8164 [==============================] - 0s 9us/sample - loss: 0.5950 - acc: 0.6955\n",
      "Epoch 6/30\n",
      "8164/8164 [==============================] - 0s 9us/sample - loss: 0.5854 - acc: 0.6940\n",
      "Epoch 7/30\n",
      "8164/8164 [==============================] - 0s 10us/sample - loss: 0.5786 - acc: 0.6963\n",
      "Epoch 8/30\n",
      "8164/8164 [==============================] - 0s 8us/sample - loss: 0.5717 - acc: 0.6929\n",
      "Epoch 9/30\n",
      "8164/8164 [==============================] - 0s 8us/sample - loss: 0.5705 - acc: 0.6926\n",
      "Epoch 10/30\n",
      "8164/8164 [==============================] - 0s 8us/sample - loss: 0.5632 - acc: 0.7026\n",
      "Epoch 11/30\n",
      "8164/8164 [==============================] - 0s 10us/sample - loss: 0.5629 - acc: 0.7003\n",
      "Epoch 12/30\n",
      "8164/8164 [==============================] - 0s 8us/sample - loss: 0.5591 - acc: 0.7054\n",
      "Epoch 13/30\n",
      "8164/8164 [==============================] - 0s 8us/sample - loss: 0.5554 - acc: 0.7086\n",
      "Epoch 14/30\n",
      "8164/8164 [==============================] - 0s 9us/sample - loss: 0.5631 - acc: 0.7010\n",
      "Epoch 15/30\n",
      "8164/8164 [==============================] - 0s 10us/sample - loss: 0.5614 - acc: 0.7049\n",
      "Epoch 16/30\n",
      "8164/8164 [==============================] - 0s 9us/sample - loss: 0.5534 - acc: 0.7079\n",
      "Epoch 17/30\n",
      "8164/8164 [==============================] - 0s 8us/sample - loss: 0.5529 - acc: 0.7137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/30\n",
      "8164/8164 [==============================] - 0s 9us/sample - loss: 0.5538 - acc: 0.7134\n",
      "Epoch 19/30\n",
      "8164/8164 [==============================] - 0s 9us/sample - loss: 0.5530 - acc: 0.7129\n",
      "Epoch 20/30\n",
      "8164/8164 [==============================] - 0s 10us/sample - loss: 0.5525 - acc: 0.7126\n",
      "Epoch 21/30\n",
      "8164/8164 [==============================] - 0s 9us/sample - loss: 0.5532 - acc: 0.7139\n",
      "Epoch 22/30\n",
      "8164/8164 [==============================] - 0s 8us/sample - loss: 0.5510 - acc: 0.7119\n",
      "Epoch 23/30\n",
      "8164/8164 [==============================] - 0s 8us/sample - loss: 0.5450 - acc: 0.7171\n",
      "Epoch 24/30\n",
      "8164/8164 [==============================] - 0s 10us/sample - loss: 0.5504 - acc: 0.7161\n",
      "Epoch 25/30\n",
      "8164/8164 [==============================] - 0s 9us/sample - loss: 0.5520 - acc: 0.7146\n",
      "Epoch 26/30\n",
      "8164/8164 [==============================] - 0s 8us/sample - loss: 0.5512 - acc: 0.7146\n",
      "Epoch 27/30\n",
      "8164/8164 [==============================] - 0s 8us/sample - loss: 0.5461 - acc: 0.7189\n",
      "Epoch 28/30\n",
      "8164/8164 [==============================] - 0s 8us/sample - loss: 0.5498 - acc: 0.7175\n",
      "Epoch 29/30\n",
      "8164/8164 [==============================] - 0s 8us/sample - loss: 0.5477 - acc: 0.7207\n",
      "Epoch 30/30\n",
      "8164/8164 [==============================] - 0s 10us/sample - loss: 0.5535 - acc: 0.7152\n",
      "attack model no. 8\n",
      "Train on 7792 samples\n",
      "Epoch 1/30\n",
      "7792/7792 [==============================] - 1s 106us/sample - loss: 0.6985 - acc: 0.4938\n",
      "Epoch 2/30\n",
      "7792/7792 [==============================] - 0s 8us/sample - loss: 0.6810 - acc: 0.5815\n",
      "Epoch 3/30\n",
      "7792/7792 [==============================] - 0s 8us/sample - loss: 0.6706 - acc: 0.6115\n",
      "Epoch 4/30\n",
      "7792/7792 [==============================] - 0s 7us/sample - loss: 0.6594 - acc: 0.6172\n",
      "Epoch 5/30\n",
      "7792/7792 [==============================] - 0s 8us/sample - loss: 0.6519 - acc: 0.6222\n",
      "Epoch 6/30\n",
      "7792/7792 [==============================] - 0s 8us/sample - loss: 0.6460 - acc: 0.6281\n",
      "Epoch 7/30\n",
      "7792/7792 [==============================] - 0s 8us/sample - loss: 0.6377 - acc: 0.6328\n",
      "Epoch 8/30\n",
      "7792/7792 [==============================] - 0s 7us/sample - loss: 0.6371 - acc: 0.6339\n",
      "Epoch 9/30\n",
      "7792/7792 [==============================] - 0s 8us/sample - loss: 0.6303 - acc: 0.6321\n",
      "Epoch 10/30\n",
      "7792/7792 [==============================] - 0s 8us/sample - loss: 0.6293 - acc: 0.6281\n",
      "Epoch 11/30\n",
      "7792/7792 [==============================] - 0s 11us/sample - loss: 0.6246 - acc: 0.6407\n",
      "Epoch 12/30\n",
      "7792/7792 [==============================] - 0s 10us/sample - loss: 0.6241 - acc: 0.6382\n",
      "Epoch 13/30\n",
      "7792/7792 [==============================] - 0s 10us/sample - loss: 0.6245 - acc: 0.6300\n",
      "Epoch 14/30\n",
      "7792/7792 [==============================] - 0s 9us/sample - loss: 0.6185 - acc: 0.6428\n",
      "Epoch 15/30\n",
      "7792/7792 [==============================] - 0s 8us/sample - loss: 0.6170 - acc: 0.6460\n",
      "Epoch 16/30\n",
      "7792/7792 [==============================] - 0s 8us/sample - loss: 0.6181 - acc: 0.6358\n",
      "Epoch 17/30\n",
      "7792/7792 [==============================] - 0s 9us/sample - loss: 0.6133 - acc: 0.6414\n",
      "Epoch 18/30\n",
      "7792/7792 [==============================] - 0s 11us/sample - loss: 0.6156 - acc: 0.6410\n",
      "Epoch 19/30\n",
      "7792/7792 [==============================] - 0s 13us/sample - loss: 0.6189 - acc: 0.6382\n",
      "Epoch 20/30\n",
      "7792/7792 [==============================] - 0s 9us/sample - loss: 0.6155 - acc: 0.6471\n",
      "Epoch 21/30\n",
      "7792/7792 [==============================] - 0s 8us/sample - loss: 0.6166 - acc: 0.6458\n",
      "Epoch 22/30\n",
      "7792/7792 [==============================] - 0s 8us/sample - loss: 0.6117 - acc: 0.6505\n",
      "Epoch 23/30\n",
      "7792/7792 [==============================] - 0s 8us/sample - loss: 0.6156 - acc: 0.6448\n",
      "Epoch 24/30\n",
      "7792/7792 [==============================] - 0s 10us/sample - loss: 0.6125 - acc: 0.6477\n",
      "Epoch 25/30\n",
      "7792/7792 [==============================] - 0s 8us/sample - loss: 0.6098 - acc: 0.6473\n",
      "Epoch 26/30\n",
      "7792/7792 [==============================] - 0s 9us/sample - loss: 0.6116 - acc: 0.6462\n",
      "Epoch 27/30\n",
      "7792/7792 [==============================] - 0s 8us/sample - loss: 0.6099 - acc: 0.6505\n",
      "Epoch 28/30\n",
      "7792/7792 [==============================] - 0s 8us/sample - loss: 0.6135 - acc: 0.6468\n",
      "Epoch 29/30\n",
      "7792/7792 [==============================] - 0s 9us/sample - loss: 0.6106 - acc: 0.6455\n",
      "Epoch 30/30\n",
      "7792/7792 [==============================] - 0s 8us/sample - loss: 0.6091 - acc: 0.6507\n",
      "attack model no. 9\n",
      "Train on 8056 samples\n",
      "Epoch 1/30\n",
      "8056/8056 [==============================] - 1s 106us/sample - loss: 0.6819 - acc: 0.5720\n",
      "Epoch 2/30\n",
      "8056/8056 [==============================] - 0s 9us/sample - loss: 0.6536 - acc: 0.6745\n",
      "Epoch 3/30\n",
      "8056/8056 [==============================] - 0s 9us/sample - loss: 0.6272 - acc: 0.7012\n",
      "Epoch 4/30\n",
      "8056/8056 [==============================] - 0s 9us/sample - loss: 0.6062 - acc: 0.6985\n",
      "Epoch 5/30\n",
      "8056/8056 [==============================] - 0s 9us/sample - loss: 0.5827 - acc: 0.7073\n",
      "Epoch 6/30\n",
      "8056/8056 [==============================] - 0s 9us/sample - loss: 0.5744 - acc: 0.7047\n",
      "Epoch 7/30\n",
      "8056/8056 [==============================] - 0s 9us/sample - loss: 0.5703 - acc: 0.7011\n",
      "Epoch 8/30\n",
      "8056/8056 [==============================] - 0s 9us/sample - loss: 0.5559 - acc: 0.7138\n",
      "Epoch 9/30\n",
      "8056/8056 [==============================] - 0s 10us/sample - loss: 0.5587 - acc: 0.7049\n",
      "Epoch 10/30\n",
      "8056/8056 [==============================] - 0s 9us/sample - loss: 0.5516 - acc: 0.7121\n",
      "Epoch 11/30\n",
      "8056/8056 [==============================] - 0s 10us/sample - loss: 0.5506 - acc: 0.7087\n",
      "Epoch 12/30\n",
      "8056/8056 [==============================] - 0s 10us/sample - loss: 0.5430 - acc: 0.7128\n",
      "Epoch 13/30\n",
      "8056/8056 [==============================] - 0s 9us/sample - loss: 0.5424 - acc: 0.7174\n",
      "Epoch 14/30\n",
      "8056/8056 [==============================] - 0s 11us/sample - loss: 0.5415 - acc: 0.7169\n",
      "Epoch 15/30\n",
      "8056/8056 [==============================] - 0s 9us/sample - loss: 0.5396 - acc: 0.7193\n",
      "Epoch 16/30\n",
      "8056/8056 [==============================] - 0s 9us/sample - loss: 0.5408 - acc: 0.7196\n",
      "Epoch 17/30\n",
      "8056/8056 [==============================] - 0s 9us/sample - loss: 0.5394 - acc: 0.7200\n",
      "Epoch 18/30\n",
      "8056/8056 [==============================] - 0s 9us/sample - loss: 0.5429 - acc: 0.7226\n",
      "Epoch 19/30\n",
      "8056/8056 [==============================] - 0s 9us/sample - loss: 0.5375 - acc: 0.7262\n",
      "Epoch 20/30\n",
      "8056/8056 [==============================] - 0s 9us/sample - loss: 0.5456 - acc: 0.7155\n",
      "Epoch 21/30\n",
      "8056/8056 [==============================] - 0s 10us/sample - loss: 0.5396 - acc: 0.7231\n",
      "Epoch 22/30\n",
      "8056/8056 [==============================] - 0s 9us/sample - loss: 0.5384 - acc: 0.7221\n",
      "Epoch 23/30\n",
      "8056/8056 [==============================] - 0s 9us/sample - loss: 0.5408 - acc: 0.7212\n",
      "Epoch 24/30\n",
      "8056/8056 [==============================] - 0s 10us/sample - loss: 0.5394 - acc: 0.7229\n",
      "Epoch 25/30\n",
      "8056/8056 [==============================] - 0s 9us/sample - loss: 0.5348 - acc: 0.7217\n",
      "Epoch 26/30\n",
      "8056/8056 [==============================] - 0s 9us/sample - loss: 0.5366 - acc: 0.7223\n",
      "Epoch 27/30\n",
      "8056/8056 [==============================] - 0s 8us/sample - loss: 0.5368 - acc: 0.7197\n",
      "Epoch 28/30\n",
      "8056/8056 [==============================] - 0s 10us/sample - loss: 0.5399 - acc: 0.7221\n",
      "Epoch 29/30\n",
      "8056/8056 [==============================] - 0s 9us/sample - loss: 0.5360 - acc: 0.7253\n",
      "Epoch 30/30\n",
      "8056/8056 [==============================] - 0s 9us/sample - loss: 0.5351 - acc: 0.7286\n",
      "attack model no. 0\n",
      "Train on 7912 samples\n",
      "Epoch 1/30\n",
      "7912/7912 [==============================] - 1s 145us/sample - loss: 0.6997 - acc: 0.4961\n",
      "Epoch 2/30\n",
      "7912/7912 [==============================] - 0s 8us/sample - loss: 0.6706 - acc: 0.6119\n",
      "Epoch 3/30\n",
      "7912/7912 [==============================] - 0s 8us/sample - loss: 0.6462 - acc: 0.6917\n",
      "Epoch 4/30\n",
      "7912/7912 [==============================] - 0s 9us/sample - loss: 0.6289 - acc: 0.6910\n",
      "Epoch 5/30\n",
      "7912/7912 [==============================] - 0s 12us/sample - loss: 0.6090 - acc: 0.6977\n",
      "Epoch 6/30\n",
      "7912/7912 [==============================] - 0s 10us/sample - loss: 0.5969 - acc: 0.6954\n",
      "Epoch 7/30\n",
      "7912/7912 [==============================] - 0s 10us/sample - loss: 0.5828 - acc: 0.6958\n",
      "Epoch 8/30\n",
      "7912/7912 [==============================] - 0s 10us/sample - loss: 0.5766 - acc: 0.6996\n",
      "Epoch 9/30\n",
      "7912/7912 [==============================] - 0s 10us/sample - loss: 0.5729 - acc: 0.6960\n",
      "Epoch 10/30\n",
      "7912/7912 [==============================] - 0s 11us/sample - loss: 0.5623 - acc: 0.6986\n",
      "Epoch 11/30\n",
      "7912/7912 [==============================] - 0s 10us/sample - loss: 0.5566 - acc: 0.7006\n",
      "Epoch 12/30\n",
      "7912/7912 [==============================] - 0s 12us/sample - loss: 0.5517 - acc: 0.7027\n",
      "Epoch 13/30\n",
      "7912/7912 [==============================] - 0s 11us/sample - loss: 0.5472 - acc: 0.7046\n",
      "Epoch 14/30\n",
      "7912/7912 [==============================] - 0s 10us/sample - loss: 0.5443 - acc: 0.7097\n",
      "Epoch 15/30\n",
      "7912/7912 [==============================] - 0s 12us/sample - loss: 0.5466 - acc: 0.7092\n",
      "Epoch 16/30\n",
      "7912/7912 [==============================] - 0s 12us/sample - loss: 0.5449 - acc: 0.7101\n",
      "Epoch 17/30\n",
      "7912/7912 [==============================] - 0s 10us/sample - loss: 0.5437 - acc: 0.7082\n",
      "Epoch 18/30\n",
      "7912/7912 [==============================] - 0s 9us/sample - loss: 0.5403 - acc: 0.7120\n",
      "Epoch 19/30\n",
      "7912/7912 [==============================] - 0s 9us/sample - loss: 0.5428 - acc: 0.7096\n",
      "Epoch 20/30\n",
      "7912/7912 [==============================] - 0s 9us/sample - loss: 0.5382 - acc: 0.7184\n",
      "Epoch 21/30\n",
      "7912/7912 [==============================] - 0s 9us/sample - loss: 0.5371 - acc: 0.7137\n",
      "Epoch 22/30\n",
      "7912/7912 [==============================] - 0s 9us/sample - loss: 0.5408 - acc: 0.7112\n",
      "Epoch 23/30\n",
      "7912/7912 [==============================] - 0s 10us/sample - loss: 0.5338 - acc: 0.7200\n",
      "Epoch 24/30\n",
      "7912/7912 [==============================] - 0s 10us/sample - loss: 0.5420 - acc: 0.7111\n",
      "Epoch 25/30\n",
      "7912/7912 [==============================] - 0s 9us/sample - loss: 0.5374 - acc: 0.7144\n",
      "Epoch 26/30\n",
      "7912/7912 [==============================] - 0s 9us/sample - loss: 0.5372 - acc: 0.7179\n",
      "Epoch 27/30\n",
      "7912/7912 [==============================] - 0s 10us/sample - loss: 0.5366 - acc: 0.7163\n",
      "Epoch 28/30\n",
      "7912/7912 [==============================] - 0s 9us/sample - loss: 0.5362 - acc: 0.7175\n",
      "Epoch 29/30\n",
      "7912/7912 [==============================] - 0s 8us/sample - loss: 0.5402 - acc: 0.7144\n",
      "Epoch 30/30\n",
      "7912/7912 [==============================] - 0s 8us/sample - loss: 0.5430 - acc: 0.7099\n",
      "attack model no. 1\n",
      "Train on 8196 samples\n",
      "Epoch 1/30\n",
      "8196/8196 [==============================] - 1s 108us/sample - loss: 0.6803 - acc: 0.5677\n",
      "Epoch 2/30\n",
      "8196/8196 [==============================] - 0s 9us/sample - loss: 0.6519 - acc: 0.6642\n",
      "Epoch 3/30\n",
      "8196/8196 [==============================] - 0s 10us/sample - loss: 0.6270 - acc: 0.6898\n",
      "Epoch 4/30\n",
      "8196/8196 [==============================] - 0s 9us/sample - loss: 0.6047 - acc: 0.7022\n",
      "Epoch 5/30\n",
      "8196/8196 [==============================] - 0s 10us/sample - loss: 0.5923 - acc: 0.7083\n",
      "Epoch 6/30\n",
      "8196/8196 [==============================] - 0s 11us/sample - loss: 0.5799 - acc: 0.7108\n",
      "Epoch 7/30\n",
      "8196/8196 [==============================] - 0s 10us/sample - loss: 0.5681 - acc: 0.7116\n",
      "Epoch 8/30\n",
      "8196/8196 [==============================] - 0s 9us/sample - loss: 0.5634 - acc: 0.7086\n",
      "Epoch 9/30\n",
      "8196/8196 [==============================] - 0s 11us/sample - loss: 0.5545 - acc: 0.7062\n",
      "Epoch 10/30\n",
      "8196/8196 [==============================] - 0s 9us/sample - loss: 0.5442 - acc: 0.7102\n",
      "Epoch 11/30\n",
      "8196/8196 [==============================] - 0s 10us/sample - loss: 0.5355 - acc: 0.7121\n",
      "Epoch 12/30\n",
      "8196/8196 [==============================] - 0s 9us/sample - loss: 0.5329 - acc: 0.7174\n",
      "Epoch 13/30\n",
      "8196/8196 [==============================] - 0s 9us/sample - loss: 0.5320 - acc: 0.7193\n",
      "Epoch 14/30\n",
      "8196/8196 [==============================] - 0s 10us/sample - loss: 0.5360 - acc: 0.7127\n",
      "Epoch 15/30\n",
      "8196/8196 [==============================] - 0s 10us/sample - loss: 0.5254 - acc: 0.7213\n",
      "Epoch 16/30\n",
      "8196/8196 [==============================] - 0s 9us/sample - loss: 0.5267 - acc: 0.7213\n",
      "Epoch 17/30\n",
      "8196/8196 [==============================] - 0s 11us/sample - loss: 0.5230 - acc: 0.7254\n",
      "Epoch 18/30\n",
      "8196/8196 [==============================] - 0s 11us/sample - loss: 0.5220 - acc: 0.7269\n",
      "Epoch 19/30\n",
      "8196/8196 [==============================] - 0s 11us/sample - loss: 0.5263 - acc: 0.7272\n",
      "Epoch 20/30\n",
      "8196/8196 [==============================] - 0s 9us/sample - loss: 0.5216 - acc: 0.7329\n",
      "Epoch 21/30\n",
      "8196/8196 [==============================] - 0s 10us/sample - loss: 0.5176 - acc: 0.7341\n",
      "Epoch 22/30\n",
      "8196/8196 [==============================] - 0s 12us/sample - loss: 0.5201 - acc: 0.7297\n",
      "Epoch 23/30\n",
      "8196/8196 [==============================] - 0s 10us/sample - loss: 0.5195 - acc: 0.7293\n",
      "Epoch 24/30\n",
      "8196/8196 [==============================] - 0s 10us/sample - loss: 0.5207 - acc: 0.7301\n",
      "Epoch 25/30\n",
      "8196/8196 [==============================] - 0s 10us/sample - loss: 0.5170 - acc: 0.7352\n",
      "Epoch 26/30\n",
      "8196/8196 [==============================] - 0s 11us/sample - loss: 0.5205 - acc: 0.7293\n",
      "Epoch 27/30\n",
      "8196/8196 [==============================] - 0s 9us/sample - loss: 0.5187 - acc: 0.7340\n",
      "Epoch 28/30\n",
      "8196/8196 [==============================] - 0s 9us/sample - loss: 0.5150 - acc: 0.7323\n",
      "Epoch 29/30\n",
      "8196/8196 [==============================] - 0s 10us/sample - loss: 0.5202 - acc: 0.7276\n",
      "Epoch 30/30\n",
      "8196/8196 [==============================] - 0s 10us/sample - loss: 0.5175 - acc: 0.7330\n",
      "attack model no. 2\n",
      "Train on 7900 samples\n",
      "Epoch 1/30\n",
      "7900/7900 [==============================] - 1s 119us/sample - loss: 0.6926 - acc: 0.5154\n",
      "Epoch 2/30\n",
      "7900/7900 [==============================] - 0s 8us/sample - loss: 0.6685 - acc: 0.6724\n",
      "Epoch 3/30\n",
      "7900/7900 [==============================] - 0s 8us/sample - loss: 0.6433 - acc: 0.7280\n",
      "Epoch 4/30\n",
      "7900/7900 [==============================] - 0s 9us/sample - loss: 0.6144 - acc: 0.7490\n",
      "Epoch 5/30\n",
      "7900/7900 [==============================] - 0s 8us/sample - loss: 0.5855 - acc: 0.7466\n",
      "Epoch 6/30\n",
      "7900/7900 [==============================] - 0s 9us/sample - loss: 0.5550 - acc: 0.7513\n",
      "Epoch 7/30\n",
      "7900/7900 [==============================] - 0s 8us/sample - loss: 0.5335 - acc: 0.7427\n",
      "Epoch 8/30\n",
      "7900/7900 [==============================] - 0s 8us/sample - loss: 0.5165 - acc: 0.7478\n",
      "Epoch 9/30\n",
      "7900/7900 [==============================] - 0s 8us/sample - loss: 0.5035 - acc: 0.7505\n",
      "Epoch 10/30\n",
      "7900/7900 [==============================] - 0s 9us/sample - loss: 0.4962 - acc: 0.7494\n",
      "Epoch 11/30\n",
      "7900/7900 [==============================] - 0s 8us/sample - loss: 0.4967 - acc: 0.7462\n",
      "Epoch 12/30\n",
      "7900/7900 [==============================] - 0s 8us/sample - loss: 0.4897 - acc: 0.7528\n",
      "Epoch 13/30\n",
      "7900/7900 [==============================] - 0s 8us/sample - loss: 0.4867 - acc: 0.7576\n",
      "Epoch 14/30\n",
      "7900/7900 [==============================] - 0s 8us/sample - loss: 0.4834 - acc: 0.7587\n",
      "Epoch 15/30\n",
      "7900/7900 [==============================] - 0s 9us/sample - loss: 0.4817 - acc: 0.7570\n",
      "Epoch 16/30\n",
      "7900/7900 [==============================] - 0s 9us/sample - loss: 0.4695 - acc: 0.7658\n",
      "Epoch 17/30\n",
      "7900/7900 [==============================] - 0s 8us/sample - loss: 0.4754 - acc: 0.7646\n",
      "Epoch 18/30\n",
      "7900/7900 [==============================] - 0s 8us/sample - loss: 0.4745 - acc: 0.7670\n",
      "Epoch 19/30\n",
      "7900/7900 [==============================] - 0s 8us/sample - loss: 0.4689 - acc: 0.7742\n",
      "Epoch 20/30\n",
      "7900/7900 [==============================] - 0s 7us/sample - loss: 0.4679 - acc: 0.7705\n",
      "Epoch 21/30\n",
      "7900/7900 [==============================] - 0s 11us/sample - loss: 0.4683 - acc: 0.7756\n",
      "Epoch 22/30\n",
      "7900/7900 [==============================] - 0s 9us/sample - loss: 0.4703 - acc: 0.7713\n",
      "Epoch 23/30\n",
      "7900/7900 [==============================] - 0s 9us/sample - loss: 0.4694 - acc: 0.7747\n",
      "Epoch 24/30\n",
      "7900/7900 [==============================] - 0s 8us/sample - loss: 0.4680 - acc: 0.7716\n",
      "Epoch 25/30\n",
      "7900/7900 [==============================] - 0s 10us/sample - loss: 0.4729 - acc: 0.7701\n",
      "Epoch 26/30\n",
      "7900/7900 [==============================] - 0s 8us/sample - loss: 0.4759 - acc: 0.7630\n",
      "Epoch 27/30\n",
      "7900/7900 [==============================] - 0s 10us/sample - loss: 0.4705 - acc: 0.7710\n",
      "Epoch 28/30\n",
      "7900/7900 [==============================] - 0s 9us/sample - loss: 0.4680 - acc: 0.7701\n",
      "Epoch 29/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7900/7900 [==============================] - 0s 8us/sample - loss: 0.4666 - acc: 0.7771\n",
      "Epoch 30/30\n",
      "7900/7900 [==============================] - 0s 8us/sample - loss: 0.4627 - acc: 0.7754\n",
      "attack model no. 3\n",
      "Train on 7968 samples\n",
      "Epoch 1/30\n",
      "7968/7968 [==============================] - 1s 135us/sample - loss: 0.6830 - acc: 0.5427\n",
      "Epoch 2/30\n",
      "7968/7968 [==============================] - 0s 9us/sample - loss: 0.6476 - acc: 0.6802\n",
      "Epoch 3/30\n",
      "7968/7968 [==============================] - 0s 7us/sample - loss: 0.6137 - acc: 0.7392\n",
      "Epoch 4/30\n",
      "7968/7968 [==============================] - 0s 8us/sample - loss: 0.5798 - acc: 0.7519\n",
      "Epoch 5/30\n",
      "7968/7968 [==============================] - 0s 7us/sample - loss: 0.5459 - acc: 0.7587\n",
      "Epoch 6/30\n",
      "7968/7968 [==============================] - 0s 9us/sample - loss: 0.5255 - acc: 0.7569\n",
      "Epoch 7/30\n",
      "7968/7968 [==============================] - 0s 8us/sample - loss: 0.5064 - acc: 0.7646\n",
      "Epoch 8/30\n",
      "7968/7968 [==============================] - 0s 7us/sample - loss: 0.4993 - acc: 0.7588\n",
      "Epoch 9/30\n",
      "7968/7968 [==============================] - 0s 8us/sample - loss: 0.4886 - acc: 0.7626\n",
      "Epoch 10/30\n",
      "7968/7968 [==============================] - 0s 8us/sample - loss: 0.4861 - acc: 0.7589\n",
      "Epoch 11/30\n",
      "7968/7968 [==============================] - 0s 7us/sample - loss: 0.4786 - acc: 0.7592\n",
      "Epoch 12/30\n",
      "7968/7968 [==============================] - 0s 7us/sample - loss: 0.4773 - acc: 0.7597\n",
      "Epoch 13/30\n",
      "7968/7968 [==============================] - 0s 7us/sample - loss: 0.4771 - acc: 0.7636\n",
      "Epoch 14/30\n",
      "7968/7968 [==============================] - 0s 7us/sample - loss: 0.4746 - acc: 0.7647\n",
      "Epoch 15/30\n",
      "7968/7968 [==============================] - 0s 8us/sample - loss: 0.4675 - acc: 0.7698\n",
      "Epoch 16/30\n",
      "7968/7968 [==============================] - 0s 7us/sample - loss: 0.4707 - acc: 0.7651\n",
      "Epoch 17/30\n",
      "7968/7968 [==============================] - 0s 7us/sample - loss: 0.4688 - acc: 0.7643\n",
      "Epoch 18/30\n",
      "7968/7968 [==============================] - 0s 7us/sample - loss: 0.4600 - acc: 0.7731\n",
      "Epoch 19/30\n",
      "7968/7968 [==============================] - 0s 7us/sample - loss: 0.4690 - acc: 0.7672\n",
      "Epoch 20/30\n",
      "7968/7968 [==============================] - 0s 7us/sample - loss: 0.4628 - acc: 0.7697\n",
      "Epoch 21/30\n",
      "7968/7968 [==============================] - 0s 7us/sample - loss: 0.4638 - acc: 0.7760\n",
      "Epoch 22/30\n",
      "7968/7968 [==============================] - 0s 7us/sample - loss: 0.4670 - acc: 0.7708\n",
      "Epoch 23/30\n",
      "7968/7968 [==============================] - 0s 7us/sample - loss: 0.4590 - acc: 0.7756\n",
      "Epoch 24/30\n",
      "7968/7968 [==============================] - 0s 7us/sample - loss: 0.4594 - acc: 0.7765\n",
      "Epoch 25/30\n",
      "7968/7968 [==============================] - 0s 7us/sample - loss: 0.4484 - acc: 0.7801\n",
      "Epoch 26/30\n",
      "7968/7968 [==============================] - 0s 8us/sample - loss: 0.4571 - acc: 0.7781\n",
      "Epoch 27/30\n",
      "7968/7968 [==============================] - 0s 7us/sample - loss: 0.4562 - acc: 0.7776\n",
      "Epoch 28/30\n",
      "7968/7968 [==============================] - 0s 7us/sample - loss: 0.4504 - acc: 0.7809\n",
      "Epoch 29/30\n",
      "7968/7968 [==============================] - 0s 7us/sample - loss: 0.4568 - acc: 0.7777\n",
      "Epoch 30/30\n",
      "7968/7968 [==============================] - 0s 7us/sample - loss: 0.4518 - acc: 0.7775\n",
      "attack model no. 4\n",
      "Train on 8000 samples\n",
      "Epoch 1/30\n",
      "8000/8000 [==============================] - 1s 110us/sample - loss: 0.6767 - acc: 0.5930\n",
      "Epoch 2/30\n",
      "8000/8000 [==============================] - 0s 7us/sample - loss: 0.6496 - acc: 0.6649\n",
      "Epoch 3/30\n",
      "8000/8000 [==============================] - 0s 7us/sample - loss: 0.6230 - acc: 0.6988\n",
      "Epoch 4/30\n",
      "8000/8000 [==============================] - 0s 7us/sample - loss: 0.5996 - acc: 0.7194\n",
      "Epoch 5/30\n",
      "8000/8000 [==============================] - 0s 8us/sample - loss: 0.5769 - acc: 0.7290\n",
      "Epoch 6/30\n",
      "8000/8000 [==============================] - 0s 7us/sample - loss: 0.5598 - acc: 0.7286\n",
      "Epoch 7/30\n",
      "8000/8000 [==============================] - 0s 8us/sample - loss: 0.5485 - acc: 0.7291\n",
      "Epoch 8/30\n",
      "8000/8000 [==============================] - 0s 7us/sample - loss: 0.5377 - acc: 0.7352\n",
      "Epoch 9/30\n",
      "8000/8000 [==============================] - 0s 7us/sample - loss: 0.5322 - acc: 0.7404\n",
      "Epoch 10/30\n",
      "8000/8000 [==============================] - 0s 8us/sample - loss: 0.5254 - acc: 0.7359\n",
      "Epoch 11/30\n",
      "8000/8000 [==============================] - 0s 8us/sample - loss: 0.5264 - acc: 0.7309\n",
      "Epoch 12/30\n",
      "8000/8000 [==============================] - 0s 7us/sample - loss: 0.5225 - acc: 0.7329\n",
      "Epoch 13/30\n",
      "8000/8000 [==============================] - 0s 9us/sample - loss: 0.5166 - acc: 0.7370\n",
      "Epoch 14/30\n",
      "8000/8000 [==============================] - 0s 7us/sample - loss: 0.5161 - acc: 0.7380\n",
      "Epoch 15/30\n",
      "8000/8000 [==============================] - 0s 7us/sample - loss: 0.5086 - acc: 0.7460\n",
      "Epoch 16/30\n",
      "8000/8000 [==============================] - 0s 7us/sample - loss: 0.5079 - acc: 0.7418\n",
      "Epoch 17/30\n",
      "8000/8000 [==============================] - 0s 8us/sample - loss: 0.5095 - acc: 0.7430\n",
      "Epoch 18/30\n",
      "8000/8000 [==============================] - 0s 8us/sample - loss: 0.5031 - acc: 0.7450\n",
      "Epoch 19/30\n",
      "8000/8000 [==============================] - 0s 7us/sample - loss: 0.5056 - acc: 0.7467\n",
      "Epoch 20/30\n",
      "8000/8000 [==============================] - 0s 8us/sample - loss: 0.5114 - acc: 0.7455\n",
      "Epoch 21/30\n",
      "8000/8000 [==============================] - 0s 8us/sample - loss: 0.5096 - acc: 0.7400\n",
      "Epoch 22/30\n",
      "8000/8000 [==============================] - 0s 7us/sample - loss: 0.4978 - acc: 0.7505\n",
      "Epoch 23/30\n",
      "8000/8000 [==============================] - 0s 7us/sample - loss: 0.5042 - acc: 0.7499\n",
      "Epoch 24/30\n",
      "8000/8000 [==============================] - 0s 7us/sample - loss: 0.4998 - acc: 0.7533\n",
      "Epoch 25/30\n",
      "8000/8000 [==============================] - 0s 7us/sample - loss: 0.4983 - acc: 0.7527\n",
      "Epoch 26/30\n",
      "8000/8000 [==============================] - 0s 8us/sample - loss: 0.5006 - acc: 0.7508\n",
      "Epoch 27/30\n",
      "8000/8000 [==============================] - 0s 8us/sample - loss: 0.5010 - acc: 0.7517\n",
      "Epoch 28/30\n",
      "8000/8000 [==============================] - 0s 8us/sample - loss: 0.4992 - acc: 0.7530\n",
      "Epoch 29/30\n",
      "8000/8000 [==============================] - 0s 7us/sample - loss: 0.4976 - acc: 0.7535\n",
      "Epoch 30/30\n",
      "8000/8000 [==============================] - 0s 7us/sample - loss: 0.4983 - acc: 0.7508\n",
      "attack model no. 5\n",
      "Train on 8068 samples\n",
      "Epoch 1/30\n",
      "8068/8068 [==============================] - 1s 108us/sample - loss: 0.6886 - acc: 0.5405\n",
      "Epoch 2/30\n",
      "8068/8068 [==============================] - 0s 7us/sample - loss: 0.6564 - acc: 0.6877\n",
      "Epoch 3/30\n",
      "8068/8068 [==============================] - 0s 7us/sample - loss: 0.6245 - acc: 0.7142\n",
      "Epoch 4/30\n",
      "8068/8068 [==============================] - 0s 7us/sample - loss: 0.5851 - acc: 0.7413\n",
      "Epoch 5/30\n",
      "8068/8068 [==============================] - 0s 7us/sample - loss: 0.5631 - acc: 0.7312\n",
      "Epoch 6/30\n",
      "8068/8068 [==============================] - 0s 8us/sample - loss: 0.5360 - acc: 0.7423\n",
      "Epoch 7/30\n",
      "8068/8068 [==============================] - 0s 8us/sample - loss: 0.5277 - acc: 0.7390\n",
      "Epoch 8/30\n",
      "8068/8068 [==============================] - 0s 7us/sample - loss: 0.5245 - acc: 0.7396\n",
      "Epoch 9/30\n",
      "8068/8068 [==============================] - 0s 8us/sample - loss: 0.5192 - acc: 0.7395\n",
      "Epoch 10/30\n",
      "8068/8068 [==============================] - 0s 8us/sample - loss: 0.5157 - acc: 0.7408\n",
      "Epoch 11/30\n",
      "8068/8068 [==============================] - 0s 8us/sample - loss: 0.5110 - acc: 0.7424\n",
      "Epoch 12/30\n",
      "8068/8068 [==============================] - 0s 7us/sample - loss: 0.5107 - acc: 0.7434\n",
      "Epoch 13/30\n",
      "8068/8068 [==============================] - 0s 8us/sample - loss: 0.5030 - acc: 0.7488\n",
      "Epoch 14/30\n",
      "8068/8068 [==============================] - 0s 8us/sample - loss: 0.5020 - acc: 0.7405\n",
      "Epoch 15/30\n",
      "8068/8068 [==============================] - 0s 8us/sample - loss: 0.4972 - acc: 0.7475\n",
      "Epoch 16/30\n",
      "8068/8068 [==============================] - 0s 7us/sample - loss: 0.5028 - acc: 0.7439\n",
      "Epoch 17/30\n",
      "8068/8068 [==============================] - 0s 8us/sample - loss: 0.4955 - acc: 0.7527\n",
      "Epoch 18/30\n",
      "8068/8068 [==============================] - 0s 8us/sample - loss: 0.4998 - acc: 0.7479\n",
      "Epoch 19/30\n",
      "8068/8068 [==============================] - 0s 7us/sample - loss: 0.4926 - acc: 0.7564\n",
      "Epoch 20/30\n",
      "8068/8068 [==============================] - 0s 8us/sample - loss: 0.4923 - acc: 0.7555\n",
      "Epoch 21/30\n",
      "8068/8068 [==============================] - 0s 8us/sample - loss: 0.4945 - acc: 0.7535\n",
      "Epoch 22/30\n",
      "8068/8068 [==============================] - 0s 8us/sample - loss: 0.4909 - acc: 0.7550\n",
      "Epoch 23/30\n",
      "8068/8068 [==============================] - 0s 8us/sample - loss: 0.4927 - acc: 0.7579\n",
      "Epoch 24/30\n",
      "8068/8068 [==============================] - 0s 8us/sample - loss: 0.4952 - acc: 0.7552\n",
      "Epoch 25/30\n",
      "8068/8068 [==============================] - 0s 10us/sample - loss: 0.4917 - acc: 0.7526\n",
      "Epoch 26/30\n",
      "8068/8068 [==============================] - 0s 10us/sample - loss: 0.4925 - acc: 0.7555\n",
      "Epoch 27/30\n",
      "8068/8068 [==============================] - 0s 9us/sample - loss: 0.4896 - acc: 0.7582\n",
      "Epoch 28/30\n",
      "8068/8068 [==============================] - 0s 9us/sample - loss: 0.4896 - acc: 0.7602\n",
      "Epoch 29/30\n",
      "8068/8068 [==============================] - 0s 10us/sample - loss: 0.4876 - acc: 0.7608\n",
      "Epoch 30/30\n",
      "8068/8068 [==============================] - 0s 10us/sample - loss: 0.4893 - acc: 0.7584\n",
      "attack model no. 6\n",
      "Train on 7944 samples\n",
      "Epoch 1/30\n",
      "7944/7944 [==============================] - 1s 120us/sample - loss: 0.6858 - acc: 0.5546\n",
      "Epoch 2/30\n",
      "7944/7944 [==============================] - 0s 8us/sample - loss: 0.6557 - acc: 0.6570\n",
      "Epoch 3/30\n",
      "7944/7944 [==============================] - 0s 8us/sample - loss: 0.6235 - acc: 0.7049\n",
      "Epoch 4/30\n",
      "7944/7944 [==============================] - 0s 8us/sample - loss: 0.6009 - acc: 0.7069\n",
      "Epoch 5/30\n",
      "7944/7944 [==============================] - 0s 8us/sample - loss: 0.5788 - acc: 0.7127\n",
      "Epoch 6/30\n",
      "7944/7944 [==============================] - 0s 8us/sample - loss: 0.5695 - acc: 0.7149\n",
      "Epoch 7/30\n",
      "7944/7944 [==============================] - 0s 10us/sample - loss: 0.5641 - acc: 0.7115\n",
      "Epoch 8/30\n",
      "7944/7944 [==============================] - 0s 9us/sample - loss: 0.5588 - acc: 0.7137\n",
      "Epoch 9/30\n",
      "7944/7944 [==============================] - 0s 8us/sample - loss: 0.5512 - acc: 0.7125\n",
      "Epoch 10/30\n",
      "7944/7944 [==============================] - 0s 8us/sample - loss: 0.5436 - acc: 0.7103\n",
      "Epoch 11/30\n",
      "7944/7944 [==============================] - 0s 8us/sample - loss: 0.5388 - acc: 0.7169\n",
      "Epoch 12/30\n",
      "7944/7944 [==============================] - 0s 8us/sample - loss: 0.5346 - acc: 0.7155\n",
      "Epoch 13/30\n",
      "7944/7944 [==============================] - 0s 9us/sample - loss: 0.5287 - acc: 0.7282\n",
      "Epoch 14/30\n",
      "7944/7944 [==============================] - 0s 9us/sample - loss: 0.5276 - acc: 0.7286\n",
      "Epoch 15/30\n",
      "7944/7944 [==============================] - 0s 9us/sample - loss: 0.5283 - acc: 0.7290\n",
      "Epoch 16/30\n",
      "7944/7944 [==============================] - 0s 9us/sample - loss: 0.5247 - acc: 0.7281\n",
      "Epoch 17/30\n",
      "7944/7944 [==============================] - 0s 10us/sample - loss: 0.5240 - acc: 0.7329\n",
      "Epoch 18/30\n",
      "7944/7944 [==============================] - 0s 9us/sample - loss: 0.5224 - acc: 0.7339\n",
      "Epoch 19/30\n",
      "7944/7944 [==============================] - 0s 9us/sample - loss: 0.5207 - acc: 0.7344\n",
      "Epoch 20/30\n",
      "7944/7944 [==============================] - 0s 9us/sample - loss: 0.5228 - acc: 0.7374\n",
      "Epoch 21/30\n",
      "7944/7944 [==============================] - 0s 9us/sample - loss: 0.5212 - acc: 0.7365\n",
      "Epoch 22/30\n",
      "7944/7944 [==============================] - 0s 8us/sample - loss: 0.5173 - acc: 0.7383\n",
      "Epoch 23/30\n",
      "7944/7944 [==============================] - 0s 8us/sample - loss: 0.5191 - acc: 0.7377\n",
      "Epoch 24/30\n",
      "7944/7944 [==============================] - 0s 8us/sample - loss: 0.5248 - acc: 0.7363\n",
      "Epoch 25/30\n",
      "7944/7944 [==============================] - 0s 8us/sample - loss: 0.5211 - acc: 0.7362\n",
      "Epoch 26/30\n",
      "7944/7944 [==============================] - 0s 11us/sample - loss: 0.5210 - acc: 0.7356\n",
      "Epoch 27/30\n",
      "7944/7944 [==============================] - 0s 9us/sample - loss: 0.5216 - acc: 0.7387\n",
      "Epoch 28/30\n",
      "7944/7944 [==============================] - 0s 8us/sample - loss: 0.5127 - acc: 0.7461\n",
      "Epoch 29/30\n",
      "7944/7944 [==============================] - 0s 8us/sample - loss: 0.5201 - acc: 0.7374\n",
      "Epoch 30/30\n",
      "7944/7944 [==============================] - 0s 9us/sample - loss: 0.5182 - acc: 0.7403\n",
      "attack model no. 7\n",
      "Train on 8164 samples\n",
      "Epoch 1/30\n",
      "8164/8164 [==============================] - 1s 107us/sample - loss: 0.6884 - acc: 0.5561\n",
      "Epoch 2/30\n",
      "8164/8164 [==============================] - 0s 8us/sample - loss: 0.6576 - acc: 0.6687\n",
      "Epoch 3/30\n",
      "8164/8164 [==============================] - 0s 8us/sample - loss: 0.6338 - acc: 0.7021\n",
      "Epoch 4/30\n",
      "8164/8164 [==============================] - 0s 8us/sample - loss: 0.6084 - acc: 0.7133\n",
      "Epoch 5/30\n",
      "8164/8164 [==============================] - 0s 9us/sample - loss: 0.5911 - acc: 0.7135\n",
      "Epoch 6/30\n",
      "8164/8164 [==============================] - 0s 9us/sample - loss: 0.5822 - acc: 0.7096\n",
      "Epoch 7/30\n",
      "8164/8164 [==============================] - 0s 10us/sample - loss: 0.5648 - acc: 0.7163\n",
      "Epoch 8/30\n",
      "8164/8164 [==============================] - 0s 10us/sample - loss: 0.5567 - acc: 0.7163\n",
      "Epoch 9/30\n",
      "8164/8164 [==============================] - 0s 9us/sample - loss: 0.5567 - acc: 0.7082\n",
      "Epoch 10/30\n",
      "8164/8164 [==============================] - 0s 9us/sample - loss: 0.5471 - acc: 0.7164\n",
      "Epoch 11/30\n",
      "8164/8164 [==============================] - 0s 10us/sample - loss: 0.5389 - acc: 0.7177\n",
      "Epoch 12/30\n",
      "8164/8164 [==============================] - 0s 9us/sample - loss: 0.5381 - acc: 0.7220\n",
      "Epoch 13/30\n",
      "8164/8164 [==============================] - 0s 9us/sample - loss: 0.5337 - acc: 0.7242\n",
      "Epoch 14/30\n",
      "8164/8164 [==============================] - 0s 9us/sample - loss: 0.5321 - acc: 0.7197\n",
      "Epoch 15/30\n",
      "8164/8164 [==============================] - 0s 10us/sample - loss: 0.5308 - acc: 0.7268\n",
      "Epoch 16/30\n",
      "8164/8164 [==============================] - 0s 10us/sample - loss: 0.5256 - acc: 0.7305\n",
      "Epoch 17/30\n",
      "8164/8164 [==============================] - 0s 10us/sample - loss: 0.5262 - acc: 0.7344\n",
      "Epoch 18/30\n",
      "8164/8164 [==============================] - 0s 9us/sample - loss: 0.5283 - acc: 0.7303\n",
      "Epoch 19/30\n",
      "8164/8164 [==============================] - 0s 9us/sample - loss: 0.5291 - acc: 0.7270\n",
      "Epoch 20/30\n",
      "8164/8164 [==============================] - 0s 8us/sample - loss: 0.5261 - acc: 0.7298\n",
      "Epoch 21/30\n",
      "8164/8164 [==============================] - 0s 9us/sample - loss: 0.5275 - acc: 0.7317\n",
      "Epoch 22/30\n",
      "8164/8164 [==============================] - 0s 10us/sample - loss: 0.5221 - acc: 0.7360\n",
      "Epoch 23/30\n",
      "8164/8164 [==============================] - 0s 9us/sample - loss: 0.5255 - acc: 0.7327\n",
      "Epoch 24/30\n",
      "8164/8164 [==============================] - 0s 10us/sample - loss: 0.5217 - acc: 0.7347\n",
      "Epoch 25/30\n",
      "8164/8164 [==============================] - 0s 10us/sample - loss: 0.5251 - acc: 0.7333\n",
      "Epoch 26/30\n",
      "8164/8164 [==============================] - 0s 9us/sample - loss: 0.5212 - acc: 0.7346\n",
      "Epoch 27/30\n",
      "8164/8164 [==============================] - 0s 10us/sample - loss: 0.5229 - acc: 0.7344\n",
      "Epoch 28/30\n",
      "8164/8164 [==============================] - 0s 9us/sample - loss: 0.5218 - acc: 0.7335\n",
      "Epoch 29/30\n",
      "8164/8164 [==============================] - 0s 9us/sample - loss: 0.5254 - acc: 0.7347\n",
      "Epoch 30/30\n",
      "8164/8164 [==============================] - 0s 8us/sample - loss: 0.5219 - acc: 0.7370\n",
      "attack model no. 8\n",
      "Train on 7792 samples\n",
      "Epoch 1/30\n",
      "7792/7792 [==============================] - 1s 126us/sample - loss: 0.6974 - acc: 0.4822\n",
      "Epoch 2/30\n",
      "7792/7792 [==============================] - 0s 10us/sample - loss: 0.6771 - acc: 0.5920\n",
      "Epoch 3/30\n",
      "7792/7792 [==============================] - 0s 10us/sample - loss: 0.6592 - acc: 0.6523\n",
      "Epoch 4/30\n",
      "7792/7792 [==============================] - 0s 9us/sample - loss: 0.6432 - acc: 0.6577\n",
      "Epoch 5/30\n",
      "7792/7792 [==============================] - 0s 9us/sample - loss: 0.6302 - acc: 0.6629\n",
      "Epoch 6/30\n",
      "7792/7792 [==============================] - 0s 9us/sample - loss: 0.6224 - acc: 0.6627\n",
      "Epoch 7/30\n",
      "7792/7792 [==============================] - 0s 8us/sample - loss: 0.6116 - acc: 0.6691\n",
      "Epoch 8/30\n",
      "7792/7792 [==============================] - 0s 8us/sample - loss: 0.6045 - acc: 0.6668\n",
      "Epoch 9/30\n",
      "7792/7792 [==============================] - 0s 8us/sample - loss: 0.5989 - acc: 0.6659\n",
      "Epoch 10/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7792/7792 [==============================] - 0s 9us/sample - loss: 0.5943 - acc: 0.6661\n",
      "Epoch 11/30\n",
      "7792/7792 [==============================] - 0s 9us/sample - loss: 0.5876 - acc: 0.6691\n",
      "Epoch 12/30\n",
      "7792/7792 [==============================] - 0s 9us/sample - loss: 0.5835 - acc: 0.6736\n",
      "Epoch 13/30\n",
      "7792/7792 [==============================] - 0s 9us/sample - loss: 0.5826 - acc: 0.6709\n",
      "Epoch 14/30\n",
      "7792/7792 [==============================] - 0s 10us/sample - loss: 0.5767 - acc: 0.6762\n",
      "Epoch 15/30\n",
      "7792/7792 [==============================] - 0s 9us/sample - loss: 0.5726 - acc: 0.6840\n",
      "Epoch 16/30\n",
      "7792/7792 [==============================] - 0s 9us/sample - loss: 0.5739 - acc: 0.6772\n",
      "Epoch 17/30\n",
      "7792/7792 [==============================] - 0s 9us/sample - loss: 0.5757 - acc: 0.6748\n",
      "Epoch 18/30\n",
      "7792/7792 [==============================] - 0s 9us/sample - loss: 0.5708 - acc: 0.6819\n",
      "Epoch 19/30\n",
      "7792/7792 [==============================] - 0s 9us/sample - loss: 0.5739 - acc: 0.6794\n",
      "Epoch 20/30\n",
      "7792/7792 [==============================] - 0s 9us/sample - loss: 0.5682 - acc: 0.6836\n",
      "Epoch 21/30\n",
      "7792/7792 [==============================] - 0s 8us/sample - loss: 0.5693 - acc: 0.6898\n",
      "Epoch 22/30\n",
      "7792/7792 [==============================] - 0s 8us/sample - loss: 0.5723 - acc: 0.6806\n",
      "Epoch 23/30\n",
      "7792/7792 [==============================] - 0s 9us/sample - loss: 0.5654 - acc: 0.6901\n",
      "Epoch 24/30\n",
      "7792/7792 [==============================] - 0s 9us/sample - loss: 0.5689 - acc: 0.6831\n",
      "Epoch 25/30\n",
      "7792/7792 [==============================] - 0s 10us/sample - loss: 0.5717 - acc: 0.6803\n",
      "Epoch 26/30\n",
      "7792/7792 [==============================] - 0s 11us/sample - loss: 0.5719 - acc: 0.6812\n",
      "Epoch 27/30\n",
      "7792/7792 [==============================] - 0s 9us/sample - loss: 0.5710 - acc: 0.6843\n",
      "Epoch 28/30\n",
      "7792/7792 [==============================] - 0s 9us/sample - loss: 0.5681 - acc: 0.6874\n",
      "Epoch 29/30\n",
      "7792/7792 [==============================] - 0s 9us/sample - loss: 0.5666 - acc: 0.6838\n",
      "Epoch 30/30\n",
      "7792/7792 [==============================] - 0s 8us/sample - loss: 0.5638 - acc: 0.6881\n",
      "attack model no. 9\n",
      "Train on 8056 samples\n",
      "Epoch 1/30\n",
      "8056/8056 [==============================] - 1s 120us/sample - loss: 0.6860 - acc: 0.5498\n",
      "Epoch 2/30\n",
      "8056/8056 [==============================] - 0s 10us/sample - loss: 0.6562 - acc: 0.6809\n",
      "Epoch 3/30\n",
      "8056/8056 [==============================] - 0s 10us/sample - loss: 0.6255 - acc: 0.7123\n",
      "Epoch 4/30\n",
      "8056/8056 [==============================] - 0s 10us/sample - loss: 0.5969 - acc: 0.7290\n",
      "Epoch 5/30\n",
      "8056/8056 [==============================] - 0s 10us/sample - loss: 0.5818 - acc: 0.7169\n",
      "Epoch 6/30\n",
      "8056/8056 [==============================] - 0s 11us/sample - loss: 0.5636 - acc: 0.7216\n",
      "Epoch 7/30\n",
      "8056/8056 [==============================] - 0s 10us/sample - loss: 0.5470 - acc: 0.7250\n",
      "Epoch 8/30\n",
      "8056/8056 [==============================] - 0s 10us/sample - loss: 0.5379 - acc: 0.7250\n",
      "Epoch 9/30\n",
      "8056/8056 [==============================] - 0s 10us/sample - loss: 0.5320 - acc: 0.7237\n",
      "Epoch 10/30\n",
      "8056/8056 [==============================] - 0s 11us/sample - loss: 0.5275 - acc: 0.7252\n",
      "Epoch 11/30\n",
      "8056/8056 [==============================] - 0s 11us/sample - loss: 0.5230 - acc: 0.7196\n",
      "Epoch 12/30\n",
      "8056/8056 [==============================] - 0s 10us/sample - loss: 0.5173 - acc: 0.7270\n",
      "Epoch 13/30\n",
      "8056/8056 [==============================] - 0s 9us/sample - loss: 0.5158 - acc: 0.7254\n",
      "Epoch 14/30\n",
      "8056/8056 [==============================] - 0s 8us/sample - loss: 0.5144 - acc: 0.7289\n",
      "Epoch 15/30\n",
      "8056/8056 [==============================] - 0s 10us/sample - loss: 0.5122 - acc: 0.7260\n",
      "Epoch 16/30\n",
      "8056/8056 [==============================] - 0s 10us/sample - loss: 0.5156 - acc: 0.7285\n",
      "Epoch 17/30\n",
      "8056/8056 [==============================] - 0s 10us/sample - loss: 0.5113 - acc: 0.7387\n",
      "Epoch 18/30\n",
      "8056/8056 [==============================] - 0s 10us/sample - loss: 0.5063 - acc: 0.7370\n",
      "Epoch 19/30\n",
      "8056/8056 [==============================] - 0s 10us/sample - loss: 0.5131 - acc: 0.7341\n",
      "Epoch 20/30\n",
      "8056/8056 [==============================] - 0s 9us/sample - loss: 0.5056 - acc: 0.7402\n",
      "Epoch 21/30\n",
      "8056/8056 [==============================] - 0s 10us/sample - loss: 0.5060 - acc: 0.7354\n",
      "Epoch 22/30\n",
      "8056/8056 [==============================] - 0s 9us/sample - loss: 0.5069 - acc: 0.7409\n",
      "Epoch 23/30\n",
      "8056/8056 [==============================] - 0s 10us/sample - loss: 0.5116 - acc: 0.7390\n",
      "Epoch 24/30\n",
      "8056/8056 [==============================] - 0s 10us/sample - loss: 0.5079 - acc: 0.7397\n",
      "Epoch 25/30\n",
      "8056/8056 [==============================] - 0s 9us/sample - loss: 0.5049 - acc: 0.7442\n",
      "Epoch 26/30\n",
      "8056/8056 [==============================] - 0s 11us/sample - loss: 0.5070 - acc: 0.7360\n",
      "Epoch 27/30\n",
      "8056/8056 [==============================] - 0s 11us/sample - loss: 0.5074 - acc: 0.7385\n",
      "Epoch 28/30\n",
      "8056/8056 [==============================] - 0s 10us/sample - loss: 0.5050 - acc: 0.7407\n",
      "Epoch 29/30\n",
      "8056/8056 [==============================] - 0s 10us/sample - loss: 0.4981 - acc: 0.7481\n",
      "Epoch 30/30\n",
      "8056/8056 [==============================] - 0s 10us/sample - loss: 0.4983 - acc: 0.7457\n"
     ]
    }
   ],
   "source": [
    "attack_models = []\n",
    "\n",
    "for i in range(len(epoch_vals)):\n",
    "    attack_models += [build_attack_models(\n",
    "        models[i],\n",
    "        data.shadow,\n",
    "        data.labels_shadow\n",
    "    )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict each of the labels\n",
    "y_pred_ins = []\n",
    "y_pred_outs= []\n",
    "\n",
    "for i in range(len(epoch_vals)):\n",
    "    target_model = models[i]\n",
    "    y_pred_in = target_model.predict(data.train)\n",
    "    y_pred_out = target_model.predict(data.test)\n",
    "    y_pred_ins += [y_pred_in]\n",
    "    y_pred_outs += [y_pred_out]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate membership of each using the naive and evaluate membership methods\n",
    "\n",
    "model_in_preds_shadow = []\n",
    "model_out_preds_shadow = []\n",
    "\n",
    "model_in_preds_naive = []\n",
    "model_out_preds_naive = []\n",
    "\n",
    "for i in range(len(epoch_vals)):\n",
    "    # Naive membership attack\n",
    "    model_in_preds_naive += [naive_membership(y_pred_ins[i],data.labels_train)]\n",
    "    model_out_preds_naive += [naive_membership(y_pred_outs[i],data.labels_test)]\n",
    "    \n",
    "    \n",
    "    # Shadow membership attack\n",
    "    model_in_preds_shadow += [evaluate_membership(\n",
    "                        attack_models[i],\n",
    "                        y_pred_in,\n",
    "                        data.labels_train\n",
    "                    )]\n",
    "    model_out_preds_shadow += [evaluate_membership(\n",
    "                        attack_models[i],\n",
    "                        y_pred_out,\n",
    "                        data.labels_test\n",
    "                    )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain accuracy of each of the attack methods\n",
    "true_positives_shadow = []\n",
    "true_negatives_shadow = []\n",
    "accuracies_shadow = []\n",
    "\n",
    "true_positives_naive = []\n",
    "true_negatives_naive = []\n",
    "accuracies_naive = []\n",
    "\n",
    "\n",
    "for i in range(len(epoch_vals)):\n",
    "    true_positives_shadow += [(model_in_preds_shadow[i] > 0.5).mean()]\n",
    "    true_negatives_shadow += [(model_out_preds_shadow[i] < 0.5).mean()]\n",
    "    accuracies_shadow += [(true_positives_shadow[i] + true_negatives_shadow[i]) / 2.]\n",
    "    \n",
    "    true_positives_naive += [(model_in_preds_naive[i] > 0.5).mean()]\n",
    "    true_negatives_naive += [(model_out_preds_naive[i] < 0.5).mean()]\n",
    "    accuracies_naive += [(true_positives_naive[i] + true_negatives_naive[i]) / 2.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5. Plot of the overfitting measure vs. the accuracies of each of the approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(overfits, accuracies_shadow, label = \"Shadow Attack\")\n",
    "plt.plot(overfits, accuracies_naive, label = \"Naive Attack\")\n",
    "plt.xlabel('Overfit Metric')\n",
    "plt.ylabel('Membership Attack Proportion')\n",
    "plt.title('Membership Attack Accuracy and Overfitting')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
